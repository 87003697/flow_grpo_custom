"""
Hunyuan3D SDE Step with Log Probability for GRPO Training

Based on complex SDE theory similar to SD3, but adapted for Hunyuan3D's reversed timesteps.

Mathematical Framework:
- Hunyuan3D uses reversed timesteps (1000â†’0, sigmas: 1.0â†’0.0)
- Complex SDE formulation with noise scaling and drift correction
- dt = sigma_next - sigma is negative (sigma_next < sigma)
- SDE theory provides proper log probability computation for GRPO

SDE Formulation:
- std_dev_t = f(sigma) based on theoretical noise schedule
- prev_sample_mean = complex drift term with SDE corrections
- noise_term = std_dev_t * sqrt(|dt|) * noise
- deterministic mode degenerates to simple ODE: sample + dt * model_output
"""
import math
from typing import Optional, Tuple, Union

import torch
from diffusers.utils.torch_utils import randn_tensor
from diffusers.schedulers.scheduling_flow_match_euler_discrete import FlowMatchEulerDiscreteScheduler


def hunyuan3d_sde_step_with_logprob(
    scheduler: FlowMatchEulerDiscreteScheduler,
    model_output: torch.FloatTensor,
    timestep: Union[float, torch.FloatTensor],
    sample: torch.FloatTensor,
    prev_sample: Optional[torch.FloatTensor] = None,
    generator: Optional[torch.Generator] = None,
    deterministic: bool = False,
) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
    """
    Predict the sample from the previous timestep using SDE theory adapted for Hunyuan3D.
    
    å‚è€ƒSD3çš„å®ç°æ¨¡å¼ï¼š
    - deterministic=True: ä½¿ç”¨ç®€å•ç¨³å®šçš„ODEç§¯åˆ†
    - deterministic=False: ä½¿ç”¨SDEç§¯åˆ†ï¼ˆå¸¦éšæœºå™ªå£°ï¼‰
    
    Args:
        scheduler: The FlowMatchEulerDiscreteScheduler instance
        model_output: The direct output from learned flow model (velocity)
        timestep: The current discrete timestep in the diffusion chain
        sample: A current instance of a sample created by the diffusion process
        prev_sample: The previous sample for KL computation (if provided)
        generator: A random number generator
        deterministic: Whether to use deterministic (ODE) or stochastic (SDE) mode

    Returns:
        Tuple containing:
        - prev_sample: The predicted sample for the previous timestep
        - log_prob: Log probability of the step
        - prev_sample_mean: Mean of the predicted sample distribution
        - std_dev: Standard deviation of the noise
    """
    # Error checking: cannot pass both generator and prev_sample
    if prev_sample is not None and generator is not None:
        raise ValueError(
            "Cannot pass both generator and prev_sample. Please make sure that either `generator` or"
            " `prev_sample` stays `None`."
        )
    
    # Initialize step_index if needed
    if scheduler.step_index is None:
        scheduler._init_step_index(timestep)
    
    # Get current step index
    step_index = scheduler.step_index
    
    # Ensure we don't go out of bounds
    if step_index >= len(scheduler.sigmas) - 1:
        step_index = len(scheduler.sigmas) - 2
    
    # Get sigmas - Hunyuan3D uses reversed timesteps (decreasing sigmas)
    device = sample.device
    dtype = sample.dtype
    
    sigma = scheduler.sigmas[step_index].to(device=device, dtype=dtype)
    sigma_next = scheduler.sigmas[step_index + 1].to(device=device, dtype=dtype)
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœsigmaè½¬æ¢åå˜æˆ0ï¼Œä½¿ç”¨float32ç²¾åº¦
    if sigma == 0.0 or sigma_next == 0.0:
        sigma = scheduler.sigmas[step_index].to(device=device, dtype=torch.float32)
        sigma_next = scheduler.sigmas[step_index + 1].to(device=device, dtype=torch.float32)
    
    # ğŸ”§ ç»ˆæä¿®å¤ï¼šç¡®ä¿sigmaæ°¸è¿œä¸ä¸º0ï¼Œé¿å…é™¤é›¶é”™è¯¯
    sigma = torch.clamp(sigma, min=1e-8)
    sigma_next = torch.clamp(sigma_next, min=1e-8)
    
    # Compute dt = sigma_next - sigma (negative since sigma_next < sigma)
    dt = sigma_next - sigma
    
    # ==================== SDE Theory (å‚è€ƒSD3å®ç°) ====================
    
    # 1. Compute theoretical noise scaling (å‚è€ƒSD3çš„ç®€æ´å®ç°)
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ¨ç†æ—¶çš„å®é™…sigma_maxï¼Œè€Œä¸æ˜¯è®­ç»ƒæ—¶çš„scheduler.sigma_max
    sigma_max = scheduler.sigmas.max().item()  # æ¨ç†æ—¶çš„å®é™…æœ€å¤§å€¼ (1.0)
    
    # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨SD3çš„å¤„ç†æ–¹å¼ï¼Œä½†ç”¨æ­£ç¡®çš„sigma_max
    condition = sigma >= sigma_max
    denominator_value = torch.where(condition, sigma_max - 1e-8, sigma / sigma_max)
    final_denominator = 1 - denominator_value
    
    # ğŸ”§ ç¡®ä¿åˆ†æ¯ä¸ä¸ºé›¶æˆ–è´Ÿæ•°
    final_denominator = torch.clamp(final_denominator, min=1e-8)
    
    std_dev_t = torch.sqrt(sigma / final_denominator) * 0.7
    
    # 2. SDE mean computation (å‚è€ƒSD3)
    # prev_sample_mean = sample*(1+std_dev_t**2/(2*sigma)*dt) + model_output*(1+std_dev_t**2*(1-sigma)/(2*sigma))*dt
    sample_coeff = 1 + std_dev_t**2 / (2 * sigma) * dt
    progress_ratio = 1 - sigma / sigma_max
    model_coeff = 1 + std_dev_t**2 * progress_ratio / (2 * sigma) * dt
    
    prev_sample_mean = sample * sample_coeff + model_output * model_coeff
    
    # ğŸ”§ æ·»åŠ ï¼šç®€åŒ–çš„NaNæ£€æŸ¥
    if torch.isnan(prev_sample_mean).any():
        print(f"    ğŸ”§ DEBUG: prev_sample_meanæœ‰NaN - sigma={sigma:.6f}, std_dev_t={std_dev_t:.6f}")
    
    # 3. Noise scaling
    noise_std = std_dev_t * torch.sqrt(torch.abs(dt))
    
    # ==================== Sample Generation ====================
    
    # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨ç®€å•çš„ODEä½œä¸ºå‡å€¼
    # ç†è®ºåŸºç¡€ï¼šSDE = ODE + noiseï¼Œæ‰€ä»¥å‡å€¼åº”è¯¥æ˜¯ODEç»“æœ
    sample_float = sample.to(torch.float32)
    model_output_float = model_output.to(torch.float32)
    ode_result = sample_float + dt * model_output_float
    ode_result = ode_result.to(dtype)
    
    # Deterministic mode: use simple ODE
    if deterministic:
        prev_sample = ode_result
        # ğŸ”§ ä¿®å¤ï¼šdeterministicæ¨¡å¼ä¸‹ï¼Œprev_sample_meanåº”è¯¥ä¹Ÿæ˜¯ODEç»“æœ
        prev_sample_mean = ode_result
    else:
        # Stochastic SDE mode: ODE + noise
        if prev_sample is None:
            # Generate noise
            variance_noise = randn_tensor(
                model_output.shape,
                generator=generator,
                device=model_output.device,
                dtype=model_output.dtype,
            )
            # Apply SDE: ODE + noise
            prev_sample = ode_result + noise_std * variance_noise
            # ğŸ”§ ä¿®å¤ï¼šSDEçš„å‡å€¼åº”è¯¥æ˜¯ODEç»“æœ
            prev_sample_mean = ode_result
        else:
            # Use provided prev_sample (for KL computation)
            # ğŸ”§ ä¿®å¤ï¼šè¿™ç§æƒ…å†µä¸‹prev_sample_meanä¹Ÿåº”è¯¥æ˜¯ODEç»“æœ
            prev_sample_mean = ode_result
    
    # ==================== Log Probability Computation ====================
    
    if deterministic:
        # For ODE: log probability is zero (deterministic process)
        log_prob = torch.zeros(sample.shape[0], device=device, dtype=dtype)
        # ğŸ”§ ä¿®å¤ï¼šdeterministicæ¨¡å¼ä¸‹ï¼Œnoise_stdåº”è¯¥æ˜¯0
        noise_std = torch.zeros_like(std_dev_t)
    else:
        # For SDE: Gaussian log probability
        if prev_sample is not None:
            # Use provided prev_sample for KL computation
            diff = prev_sample.detach() - prev_sample_mean
        else:
            diff = prev_sample.detach() - prev_sample_mean
        
        # Gaussian log probability density
        log_prob = (
            -0.5 * (diff ** 2) / (noise_std ** 2)
            - torch.log(noise_std)
            - 0.5 * math.log(2 * math.pi)
        )
        
        # Mean along all dimensions except batch
        log_prob = log_prob.mean(dim=tuple(range(1, log_prob.ndim)))
    
    # Update step index for next iteration (only if not out of bounds)
    if scheduler._step_index < len(scheduler.sigmas) - 2:
        scheduler._step_index += 1
    
    return prev_sample, log_prob, prev_sample_mean, noise_std


def hunyuan3d_scheduler_step_with_logprob(
    scheduler: FlowMatchEulerDiscreteScheduler,
    model_output: torch.FloatTensor,
    timestep: Union[float, torch.FloatTensor],
    sample: torch.FloatTensor,
    generator: Optional[torch.Generator] = None,
    deterministic: bool = False,
    return_dict: bool = True,
) -> Union[object, Tuple]:
    """
    Compatibility wrapper function that maintains the original scheduler interface
    while adding complex SDE computation with log probability.
    
    This function implements sophisticated SDE theory comparable to SD3, but adapted
    for Hunyuan3D's reversed timestep flow matching framework.
    
    Args:
        scheduler: The FlowMatchEulerDiscreteScheduler instance
        model_output: The direct output from learned flow model
        timestep: The current discrete timestep in the diffusion chain
        sample: A current instance of a sample created by the diffusion process
        generator: A random number generator for stochastic sampling
        deterministic: Whether to use deterministic (ODE) or stochastic (SDE) sampling
        return_dict: Whether to return a dict or tuple
        
    Returns:
        If return_dict is True, returns a dict-like object with prev_sample and additional fields
        Otherwise returns a tuple (prev_sample, log_prob, prev_sample_mean, std_dev)
    """
    prev_sample, log_prob, prev_sample_mean, std_dev = hunyuan3d_sde_step_with_logprob(
        scheduler=scheduler,
        model_output=model_output,
        timestep=timestep,
        sample=sample,
        generator=generator,
        deterministic=deterministic,
    )
    
    if not return_dict:
        return prev_sample, log_prob, prev_sample_mean, std_dev
    
    # Return a simple object with the required attributes
    class SchedulerOutput:
        def __init__(self, prev_sample, log_prob, prev_sample_mean, std_dev):
            self.prev_sample = prev_sample
            self.log_prob = log_prob
            self.prev_sample_mean = prev_sample_mean
            self.std_dev = std_dev
    
    return SchedulerOutput(prev_sample, log_prob, prev_sample_mean, std_dev) 