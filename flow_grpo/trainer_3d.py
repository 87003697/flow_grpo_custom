"""
Hunyuan3D GRPO Trainer

3D adaptation of GRPO training for Hunyuan3D model.
Handles 3D mesh generation, reward computation, and GRPO training steps.
"""
import time
import subprocess
from contextlib import contextmanager
import torch
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict
from concurrent import futures

from generators.hunyuan3d.pipeline import Hunyuan3DPipeline
from reward_models.rewards_mesh import multi_mesh_score
from reward_models.uni3d_scorer.uni3d_scorer import Uni3DScorer
from .diffusers_patch.hunyuan3d_pipeline_with_logprob import hunyuan3d_pipeline_with_logprob
from .diffusers_patch.hunyuan3d_sde_with_logprob import hunyuan3d_sde_step_with_logprob

@contextmanager
def gpu_timer(name):
    """ç»¼åˆç›‘æ§ï¼šè€—æ—¶ + GPUæ˜¾å­˜ + GPUåˆ©ç”¨ç‡"""
    
    # å¼€å§‹å‰çŠ¶æ€
    start_time = time.time()
    start_memory = torch.cuda.memory_allocated() / 1024**3  # GB
    start_reserved = torch.cuda.memory_reserved() / 1024**3  # GB
    
    print(f"ğŸ• å¼€å§‹: {name}")
    print(f"  ğŸ“Š åˆå§‹æ˜¾å­˜: {start_memory:.2f}GB (å·²åˆ†é…) / {start_reserved:.2f}GB (å·²ä¿ç•™)")
    
    # è·å–GPUåˆ©ç”¨ç‡
    def get_gpu_utilization():
        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        return int(result.stdout.strip().split('\n')[0])
    
    start_util = get_gpu_utilization()
    print(f"  âš¡ åˆå§‹GPUåˆ©ç”¨ç‡: {start_util}%")
    
    try:
        yield
    finally:
        # ç»“æŸåçŠ¶æ€
        end_time = time.time()
        end_memory = torch.cuda.memory_allocated() / 1024**3
        end_reserved = torch.cuda.memory_reserved() / 1024**3
        end_util = get_gpu_utilization()
        
        duration = end_time - start_time
        memory_delta = end_memory - start_memory
        reserved_delta = end_reserved - start_reserved
        
        print(f"âœ… å®Œæˆ: {name}")
        print(f"  â±ï¸  è€—æ—¶: {duration:.2f}ç§’")
        print(f"  ğŸ“Š ç»“æŸæ˜¾å­˜: {end_memory:.2f}GB (å·²åˆ†é…) / {end_reserved:.2f}GB (å·²ä¿ç•™)")
        print(f"  ğŸ“ˆ æ˜¾å­˜å˜åŒ–: {memory_delta:+.2f}GB (å·²åˆ†é…) / {reserved_delta:+.2f}GB (å·²ä¿ç•™)")
        print(f"  âš¡ ç»“æŸGPUåˆ©ç”¨ç‡: {end_util}%")
        print(f"  ğŸ”¥ å¹³å‡GPUåˆ©ç”¨ç‡: {(start_util + end_util) / 2:.1f}%")
        print()


class Hunyuan3DGRPOTrainer:
    """
    GRPO Trainer adapted for Hunyuan3D 3D generation.
    """
    
    def __init__(
        self,
        pipeline: Hunyuan3DPipeline,  # æ˜ç¡®ï¼šåªæ¥å— Hunyuan3DPipeline
        reward_config: Optional[Dict[str, float]] = None,
        device: str = "cuda",
    ):
        """
        Initialize the 3D GRPO trainer.
        
        Args:
            pipeline: Hunyuan3DPipeline åŒ…è£…ç±»
            reward_config: 3D reward configuration dict, e.g., {"geometric_quality": 0.3, "uni3d": 0.7}
            device: Device to run training on
        """
        self.pipeline = pipeline
        self.device = device
        
        # Set default reward config if not provided
        if reward_config is None:
            reward_config = {
                "geometric_quality": 0.3,
                "uni3d": 0.7
            }
        
        # Create reward function using new rewards_mesh.py
        self.reward_fn = multi_mesh_score(device, reward_config)
        
        # Move core pipeline to device (æ˜ç¡®çš„è®¿é—®è·¯å¾„)
        self.pipeline.core_pipeline.to(device)
    
    def sample_meshes_with_rewards(
        self,
        images: List[str],
        prompts: List[str],
        batch_size: int = 1,
        num_inference_steps: int = 50,
        guidance_scale: float = 5.0,
        deterministic: bool = False,
        kl_reward: float = 0.0,
        executor: Optional[futures.ThreadPoolExecutor] = None,
    ) -> Dict[str, Any]:
        """Sample 3D meshes and compute rewards."""
        
        with gpu_timer("ğŸ¯ 3Dç½‘æ ¼ç”Ÿæˆ"):
            # æ˜ç¡®ï¼šæ€»æ˜¯ä½¿ç”¨ core_pipeline
            actual_pipeline = self.pipeline.core_pipeline
            
            # Process in batches
            all_meshes = []
            all_latents = []
            all_log_probs = []
            all_kl = []
            # ğŸ”§ æ–°å¢ï¼šä¿å­˜å›¾åƒæ¡ä»¶ç”¨äºè®­ç»ƒ
            all_image_conds = []
            
            for i in range(0, len(images), batch_size):
                batch_images = images[i:i+batch_size]
                batch_prompts = prompts[i:i+batch_size]
                
                # Generate meshes with log probabilities
                meshes, latents, log_probs, kl, image_cond = hunyuan3d_pipeline_with_logprob(
                    actual_pipeline,
                    image=batch_images[0] if len(batch_images) == 1 else batch_images,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    deterministic=True,  # ğŸ”§ æ˜ç¡®è®¾ç½®ä¸ºTrueï¼Œç¡®ä¿ä¸åŸç”Ÿæ–¹æ³•ä¸€è‡´
                    kl_reward=kl_reward,
                    return_image_cond=True,  # ğŸ”§ æ–°å¢ï¼šè¯·æ±‚è¿”å›å›¾åƒæ¡ä»¶
                )
                
                all_meshes.extend(meshes if isinstance(meshes, list) else [meshes])
                all_latents.extend(latents)
                all_log_probs.extend(log_probs)
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨appendè€Œä¸æ˜¯extendï¼Œä¿æŒKLçš„äºŒç»´ç»“æ„
                all_kl.append(kl)  # ä¿æŒ(batch_size,)çš„ç»“æ„ï¼Œè€Œä¸æ˜¯æ‹å¹³
                # ğŸ”§ æ–°å¢ï¼šä¿å­˜å›¾åƒæ¡ä»¶
                all_image_conds.append(image_cond)
        
        with gpu_timer("ğŸ† å¥–åŠ±å‡½æ•°è®¡ç®—"):
            # Compute rewards asynchronously if executor provided
            if executor:
                reward_future = executor.submit(self.reward_fn, all_meshes, images, prompts)
                rewards = reward_future
            else:
                rewards = self.reward_fn(all_meshes, images, prompts)
        
        with gpu_timer("ğŸ“¦ ç»“æœæ‰“åŒ…"):
            # ğŸ” Hunyuan3D Trainer Debug: å¤„ç†pipelineè¿”å›æ•°æ®
            # âš ï¸ é‡è¦ï¼šSD3å’ŒHunyuan3Dçš„latent shapeä¸åŒï¼Œä½†æ•°æ®å¤„ç†æ¨¡å¼ç›¸åŒ
            # SD3: all_latentsæ˜¯list of tensorsï¼Œæ¯ä¸ªshapeä¸º(batch_size, 16, 32, 32)
            # Hunyuan3D: all_latentsæ˜¯list of tensorsï¼Œæ¯ä¸ªshapeä¸º(batch_size, 1024, 64)
            # ç›¸åŒç‚¹ï¼šéƒ½æ˜¯lists â†’ stack â†’ åˆ†å‰²ä¸ºcurrent/next states
            print(f"ğŸ” Hunyuan3D Trainer Debug - åŸå§‹æ•°æ®:")
            print(f"  len(all_latents): {len(all_latents)} (SD3ä¹Ÿæ˜¯: num_steps+1)")
            print(f"  len(all_log_probs): {len(all_log_probs)} (SD3ä¹Ÿæ˜¯: num_steps)")
            print(f"  len(all_kl): {len(all_kl)} (SD3ä¹Ÿæ˜¯: num_steps)")
            print(f"  len(all_image_conds): {len(all_image_conds)} (æ–°å¢ï¼šå›¾åƒæ¡ä»¶)")
            if all_latents:
                print(f"  all_latents[0].shape: {all_latents[0].shape} (Hunyuan3D: (batch, 1024, 64))")
                print(f"  å¯¹æ¯”SD3: all_latents[0].shape = (batch, 16, 32, 32)")
            if all_log_probs:
                print(f"  all_log_probs[0].shape: {all_log_probs[0].shape} (ä¸SD3ç›¸åŒ: (batch,))")
            if all_kl:
                # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨æ£€æŸ¥ all_kl[0] çš„ç±»å‹
                if isinstance(all_kl[0], torch.Tensor):
                    print(f"  all_kl[0].shape: {all_kl[0].shape} (ä¸SD3ç›¸åŒ: (batch,))")
                else:
                    print(f"  all_kl[0] ç±»å‹: {type(all_kl[0])}, é•¿åº¦: {len(all_kl[0]) if hasattr(all_kl[0], '__len__') else 'N/A'}")
            if all_image_conds:
                print(f"  all_image_conds[0] ç±»å‹: {type(all_image_conds[0])} (æ–°å¢ï¼šå›¾åƒæ¡ä»¶)")
                if isinstance(all_image_conds[0], dict):
                    print(f"    å­—å…¸åŒ…å«keys: {list(all_image_conds[0].keys())}")
                    for key, value in all_image_conds[0].items():
                        if isinstance(value, torch.Tensor):
                            print(f"      {key}.shape: {value.shape}")
                        else:
                            print(f"      {key}: {type(value)}")
                elif isinstance(all_image_conds[0], torch.Tensor):
                    print(f"  all_image_conds[0].shape: {all_image_conds[0].shape}")
                else:
                    print(f"  all_image_conds[0]: {type(all_image_conds[0])}")
            
            # Convert to tensors
            # ğŸ”§ ä¿®å¤ï¼šæŒ‰SD3æ–¹å¼stack - (batch, steps+1, ...)
            latents_tensor = torch.stack(all_latents, dim=1) if all_latents else torch.empty(0)
            print(f"  ğŸ”§ ä¿®å¤å latents_tensor.shape: {latents_tensor.shape if latents_tensor.numel() > 0 else 'empty'}")
            print(f"    æœŸæœ›æ ¼å¼: (batch, steps+1, 1024, 64)")
            # ğŸ”§ ä¿®å¤ï¼šæŒ‰SD3æ–¹å¼stack - (batch, steps)
            log_probs_tensor = torch.stack(all_log_probs, dim=1) if all_log_probs else torch.empty(0)
            print(f"  ğŸ”§ ä¿®å¤å log_probs_tensor.shape: {log_probs_tensor.shape if log_probs_tensor.numel() > 0 else 'empty'}")
            print(f"    æœŸæœ›æ ¼å¼: (batch, steps)")
            
            # ğŸ”§ æ–°å¢ï¼šå¤„ç†å›¾åƒæ¡ä»¶
            if all_image_conds:
                # å›¾åƒæ¡ä»¶åœ¨æ‰€æœ‰æ­¥éª¤ä¸­éƒ½ç›¸åŒï¼Œåªéœ€è¦ç¬¬ä¸€ä¸ª
                image_cond_tensor = all_image_conds[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªbatchçš„å›¾åƒæ¡ä»¶
                print(f"  ğŸ”§ æ–°å¢ image_cond_tensor ç±»å‹: {type(image_cond_tensor)}")
                if isinstance(image_cond_tensor, dict):
                    print(f"    å­—å…¸åŒ…å«keys: {list(image_cond_tensor.keys())}")
                    for key, value in image_cond_tensor.items():
                        if isinstance(value, torch.Tensor):
                            print(f"      {key}.shape: {value.shape}")
                elif isinstance(image_cond_tensor, torch.Tensor):
                    print(f"    tensor.shape: {image_cond_tensor.shape}")
                print(f"    ç”¨äºè®­ç»ƒé˜¶æ®µçš„æ¡ä»¶è®¡ç®—")
            else:
                image_cond_tensor = None
            
            # ğŸ” Hunyuan3D Trainer Debug - è½¬æ¢åçš„tensorå½¢çŠ¶:
            # âš ï¸ å½“å‰é—®é¢˜ï¼šæˆ‘ä»¬çš„stackæ–¹å¼ä¸SD3ä¸åŒï¼
            # SD3æ–¹å¼: torch.stack(data, dim=1) â†’ (batch_size, num_steps+1, ...)
            # å½“å‰æ–¹å¼: torch.stack(data, dim=0) â†’ (num_steps+1, batch_size, ...)
            print(f"ğŸ” Hunyuan3D Trainer Debug - è½¬æ¢å:")
            if latents_tensor.numel() > 0:
                print(f"  latents_tensor.shape: {latents_tensor.shape}")
                print(f"  å½“å‰: (steps+1, batch, 1024, 64)")
                print(f"  SD3åº”ä¸º: (batch, steps+1, 16, 32, 32)")
            if log_probs_tensor.numel() > 0:
                print(f"  log_probs_tensor.shape: {log_probs_tensor.shape}")
                print(f"  å½“å‰: (steps, batch)")
                print(f"  SD3åº”ä¸º: (batch, steps)")
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿all_klä¸­éƒ½æ˜¯tensorå¹¶æŒ‰SD3æ–¹å¼stack
            if all_kl:
                # ğŸ” SD3 KLå¤„ç†å‚è€ƒ: 
                # SD3: all_klæ˜¯list of tensorsï¼Œæ¯ä¸ªshapeä¸º(batch_size,)
                # SD3æ–¹å¼: torch.stack(all_kl, dim=1) â†’ (batch_size, num_steps)
                # Hunyuan3D: ç›¸åŒçš„æ•°æ®ç»“æ„ï¼Œä½†éœ€è¦æ­£ç¡®çš„stackæ–¹å¼
                print(f"ğŸ” KL tensorå¤„ç† - å¯¹æ¯”SD3:")
                print(f"  all_klé•¿åº¦: {len(all_kl)} (SD3ä¹Ÿæ˜¯: num_steps)")
                
                # å°†all_klä¸­çš„æ¯ä¸ªå…ƒç´ è½¬æ¢ä¸ºtensorï¼ˆå¦‚æœè¿˜ä¸æ˜¯çš„è¯ï¼‰
                all_kl_tensors = []
                for i, kl in enumerate(all_kl):
                    if isinstance(kl, torch.Tensor):
                        all_kl_tensors.append(kl)
                        if i == 0:
                            print(f"  all_kl[0].shape: {kl.shape} (SD3 ref: (1,))")
                    elif isinstance(kl, (list, tuple)):
                        # ğŸ”§ ä¿®å¤ï¼šå¯¹äºlist/tupleï¼Œå…ˆè½¬æ¢ä¸ºtensorå†stack
                        if len(kl) > 0 and isinstance(kl[0], torch.Tensor):
                            # å¦‚æœæ˜¯tensoråˆ—è¡¨ï¼Œå…ˆstackæˆ2D tensor
                            kl_tensor = torch.stack(kl)  # (num_steps, batch_size)
                            kl_tensor = kl_tensor.transpose(0, 1)  # (batch_size, num_steps)
                        else:
                            # å¦‚æœæ˜¯æ•°å€¼åˆ—è¡¨ï¼Œç›´æ¥è½¬æ¢
                            kl_tensor = torch.tensor(kl)
                        all_kl_tensors.append(kl_tensor)
                    else:
                        all_kl_tensors.append(torch.tensor(kl))
                
                # ğŸ”§ ä¿®å¤ï¼šç°åœ¨all_kl_tensorsä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½åº”è¯¥æ˜¯(batch_size, num_steps)
                # æˆ‘ä»¬éœ€è¦åœ¨batchç»´åº¦ä¸Šæ‹¼æ¥
                kl_tensor = torch.cat(all_kl_tensors, dim=0)  # (total_batch_size, num_steps)
                print(f"  æœ€ç»ˆkl_tensor.shape: {kl_tensor.shape} (SD3åº”ä¸º: (batch_size, num_steps))")
            else:
                kl_tensor = torch.empty(0)
            
            
            # ğŸ” æœ€ç»ˆéªŒè¯ - æ‰€æœ‰tensorå½¢çŠ¶
            print(f"ğŸ” æœ€ç»ˆéªŒè¯ - æ‰€æœ‰tensorå½¢çŠ¶:")
            
            # ğŸ”§ ä¿®å¤ï¼šç”Ÿæˆæ­£ç¡®å½¢çŠ¶çš„timesteps - (batch_size, num_steps)
            num_steps = latents_tensor.shape[1] - 1 if latents_tensor.numel() > 0 else 20  # steps = latents_steps - 1
            timesteps_tensor = torch.randint(0, 1000, (len(images), num_steps), device=self.device)
            print(f"  ğŸ”§ ä¿®å¤å timesteps.shape: {timesteps_tensor.shape}")
            print(f"    æœŸæœ›æ ¼å¼: (batch, steps)")
            print(f"    è®¾å¤‡: {timesteps_tensor.device}")
            
            temp_result = {
                "meshes": all_meshes,
                "images": images,
                "prompts": prompts,
                "latents": latents_tensor,
                "log_probs": log_probs_tensor,
                "kl": kl_tensor,
                "rewards": rewards,
                "timesteps": timesteps_tensor,
                "image_cond": image_cond_tensor,  # ğŸ”§ æ–°å¢ï¼šå›¾åƒæ¡ä»¶
            }
            for key, value in temp_result.items():
                if isinstance(value, torch.Tensor):
                    print(f"  {key}.shape: {value.shape}")
                elif isinstance(value, dict):
                    print(f"  {key}: dict with {len(value)} keys")
                else:
                    print(f"  {key}: {type(value)} with {len(value) if hasattr(value, '__len__') else 'N/A'} items")
            print(f"  ==========================================")
            
            return {
                "meshes": all_meshes,
                "images": images,
                "prompts": prompts,
                "latents": latents_tensor,
                "log_probs": log_probs_tensor,
                "kl": kl_tensor,
                "rewards": rewards,
                "timesteps": timesteps_tensor,
                "image_cond": image_cond_tensor,  # ğŸ”§ æ–°å¢ï¼šå›¾åƒæ¡ä»¶
            }
    
    def _compute_rewards_sync(
        self, 
        meshes: List, 
        images: List[str], 
        prompts: List[str]
    ) -> Dict[str, torch.Tensor]:
        """Compute rewards synchronously and return as tensors on the correct device."""
        # Use the reward function to compute scores
        reward_details, _ = self.reward_fn(meshes, prompts, {})
        
        # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šåˆ›å»ºtensorï¼Œé¿å…è®¾å¤‡è½¬æ¢
        rewards = {}
        for key, scores in reward_details.items():
            if isinstance(scores, (list, tuple)):
                # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šåˆ›å»ºï¼Œé¿å…CPU->CUDAè½¬æ¢
                rewards[key] = torch.tensor(scores, device=self.device, dtype=torch.float32)
                print(f"ğŸ”§ ä¼˜åŒ–ï¼š{key} å¥–åŠ±ç›´æ¥åœ¨ {self.device} ä¸Šåˆ›å»ºï¼Œå½¢çŠ¶ {rewards[key].shape}")
            else:
                rewards[key] = torch.tensor([scores], device=self.device, dtype=torch.float32)
                print(f"ğŸ”§ ä¼˜åŒ–ï¼š{key} å¥–åŠ±(æ ‡é‡)ç›´æ¥åœ¨ {self.device} ä¸Šåˆ›å»º")
        
        return rewards
    
    def _compute_rewards_async(
        self, 
        meshes: List, 
        images: List[str], 
        prompts: List[str]
    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
        """Compute rewards asynchronously (for compatibility with original trainer)."""
        rewards = self._compute_rewards_sync(meshes, images, prompts)
        metadata = {
            "num_meshes": len(meshes),
            "avg_score": rewards["avg"].mean().item() if "avg" in rewards else 0.0,
        }
        return rewards, metadata
    
    def compute_log_prob_3d(
        self,
        pipeline,
        sample: Dict[str, torch.Tensor],
        step_index: int,
        config: Any,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Compute log probability for a specific timestep in 3D generation.
        
        Args:
            pipeline: Hunyuan3D pipeline
            sample: Sample data containing latents, timesteps, etc.
            step_index: Index of the timestep to compute
            config: Training configuration
            
        Returns:
            Tuple of (prev_sample, log_prob, prev_sample_mean, std_dev)
        """
        # Get the latents and timestep for this step
        latents = sample["latents"][:, step_index]  # Current latents
        next_latents = sample["next_latents"][:, step_index]  # Target next latents
        timestep = sample["timesteps"][:, step_index]  # Current timestep
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¿å­˜çš„å›¾åƒæ¡ä»¶
        if "image_cond" in sample and sample["image_cond"] is not None:
            cond = sample["image_cond"]
            print(f"ğŸ”§ ä½¿ç”¨ä¿å­˜çš„å›¾åƒæ¡ä»¶: {cond.shape}")
        else:
            # ğŸ”§ ä¿®å¤ï¼šå®ç°å›¾åƒæ¡ä»¶é‡è®¡ç®—é€»è¾‘
            print(f"ğŸ”§ é‡æ–°è®¡ç®—å›¾åƒæ¡ä»¶...")
            # ä»åŸå§‹å›¾åƒé‡æ–°è®¡ç®—æ¡ä»¶
            if "images" in sample:
                # ä½¿ç”¨pipelineçš„æ¡ä»¶ç¼–ç å™¨é‡æ–°è®¡ç®—
                images = sample["images"]
                # å‡è®¾pipelineæœ‰conditionerå±æ€§
                if hasattr(pipeline, 'conditioner'):
                    # é‡æ–°åŠ è½½å’Œç¼–ç å›¾åƒ
                    from PIL import Image
                    import torch
                    
                    # åŠ è½½å›¾åƒ
                    if isinstance(images[0], str):
                        # å¦‚æœæ˜¯è·¯å¾„ï¼ŒåŠ è½½å›¾åƒ
                        pil_images = [Image.open(img_path).convert('RGB') for img_path in images]
                        # è½¬æ¢ä¸ºtensorï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„é¢„å¤„ç†é€»è¾‘è°ƒæ•´ï¼‰
                        # æš‚æ—¶ä½¿ç”¨ç®€åŒ–çš„å¤„ç†
                        import torchvision.transforms as transforms
                        transform = transforms.Compose([
                            transforms.Resize((256, 256)),
                            transforms.ToTensor(),
                            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
                        ])
                        image_tensors = torch.stack([transform(img) for img in pil_images])
                        image_tensors = image_tensors.to(latents.device)
                        
                        # ä½¿ç”¨æ¡ä»¶ç¼–ç å™¨
                        with torch.no_grad():
                            cond = pipeline.conditioner(image_tensors)
                        print(f"ğŸ”§ é‡æ–°è®¡ç®—çš„å›¾åƒæ¡ä»¶: {cond.shape}")
                    else:
                        raise ValueError("Unsupported image format for condition recomputation")
                else:
                    raise ValueError("Pipeline does not have conditioner for image condition recomputation")
            else:
                raise ValueError("No images available for condition recomputation")
        
        # Prepare model input
        latent_model_input = latents
        if hasattr(config.train, 'cfg') and config.train.cfg:
            # Add negative conditioning for classifier-free guidance
            latent_model_input = torch.cat([latent_model_input, latent_model_input])
            # ğŸ”§ ä¿®å¤ï¼šä¸ºCFGå‡†å¤‡è´Ÿæ¡ä»¶
            if hasattr(sample, 'neg_cond') and sample['neg_cond'] is not None:
                neg_cond = sample['neg_cond']
            else:
                # ä½¿ç”¨é›¶æ¡ä»¶ä½œä¸ºè´Ÿæ¡ä»¶
                neg_cond = torch.zeros_like(cond)
            cond = torch.cat([neg_cond, cond])
        
        # Convert timestep to normalized format
        timestep_normalized = timestep.float() / pipeline.scheduler.config.num_train_timesteps
        timestep_tensor = timestep_normalized.to(latents.dtype)
        
        # Predict noise using the model
        with torch.cuda.amp.autocast():
            noise_pred = pipeline.model(latent_model_input, timestep_tensor, cond)
        
        # Apply classifier-free guidance if enabled
        if hasattr(config.train, 'cfg') and config.train.cfg:
            noise_pred_neg, noise_pred_pos = noise_pred.chunk(2)
            guidance_scale = getattr(config.sample, 'guidance_scale', 5.0)
            noise_pred = noise_pred_neg + guidance_scale * (noise_pred_pos - noise_pred_neg)
        
        # ğŸ”§ ä½¿ç”¨configä¸­çš„deterministicè®¾ç½®
        deterministic = getattr(config, 'deterministic', False)
        
        # Compute log probability using our SDE implementation
        prev_sample, log_prob, prev_sample_mean, std_dev = hunyuan3d_sde_step_with_logprob(
            scheduler=pipeline.scheduler,
            model_output=noise_pred,
            timestep=timestep[0],  # Assume batch has same timestep
            sample=latents,
            prev_sample=next_latents,  # Use target as reference
            deterministic=deterministic,
        )
        
        return prev_sample, log_prob, prev_sample_mean, std_dev
    
    def train_step(
        self,
        samples: Dict[str, torch.Tensor],
        pipeline,
        optimizer: torch.optim.Optimizer,
        config: Any,
        accelerator: Any,
    ) -> Dict[str, float]:
        """
        Perform a single GRPO training step for 3D generation.
        
        Args:
            samples: Batch of samples with latents, rewards, etc.
            pipeline: Hunyuan3D pipeline
            optimizer: Optimizer for training
            config: Training configuration
            accelerator: HuggingFace accelerator
            
        Returns:
            Dictionary of training metrics
        """
        info = defaultdict(list)
        num_timesteps = samples["timesteps"].shape[1]
        
        # Train on each timestep
        for j in range(num_timesteps):
            with accelerator.accumulate(pipeline.model):
                with accelerator.autocast():
                    # Compute log probability for current timestep
                    prev_sample, log_prob, prev_sample_mean, std_dev = self.compute_log_prob_3d(
                        pipeline, samples, j, config
                    )
                    
                    # Compute reference log probability if beta > 0 (KL regularization)
                    if getattr(config.train, 'beta', 0) > 0:
                        with torch.no_grad():
                            # Disable adapter for reference computation
                            with pipeline.model.disable_adapter():
                                _, log_prob_ref, prev_sample_mean_ref, std_dev_ref = self.compute_log_prob_3d(
                                    pipeline, samples, j, config
                                )
                
                # GRPO loss computation
                advantages = torch.clamp(
                    samples["advantages"][:, j],
                    -getattr(config.train, 'adv_clip_max', 5.0),
                    getattr(config.train, 'adv_clip_max', 5.0),
                )
                
                # Compute policy ratio
                ratio = torch.exp(log_prob - samples["log_probs"][:, j])
                
                # PPO clipped loss
                unclipped_loss = -advantages * ratio
                clipped_loss = -advantages * torch.clamp(
                    ratio,
                    1.0 - getattr(config.train, 'clip_range', 0.2),
                    1.0 + getattr(config.train, 'clip_range', 0.2),
                )
                policy_loss = torch.mean(torch.maximum(unclipped_loss, clipped_loss))
                
                # KL divergence loss
                if getattr(config.train, 'beta', 0) > 0:
                    kl_loss = ((prev_sample_mean - prev_sample_mean_ref) ** 2).mean(dim=tuple(range(1, prev_sample_mean.ndim))) / (2 * std_dev ** 2)
                    kl_loss = torch.mean(kl_loss)
                    loss = policy_loss + config.train.beta * kl_loss
                else:
                    kl_loss = torch.tensor(0.0)
                    loss = policy_loss
                
                # Collect metrics
                info["approx_kl"].append(
                    0.5 * torch.mean((log_prob - samples["log_probs"][:, j]) ** 2)
                )
                info["clipfrac"].append(
                    torch.mean(
                        (torch.abs(ratio - 1.0) > getattr(config.train, 'clip_range', 0.2)).float()
                    )
                )
                info["policy_loss"].append(policy_loss)
                if getattr(config.train, 'beta', 0) > 0:
                    info["kl_loss"].append(kl_loss)
                info["loss"].append(loss)
                
                # Backward pass
                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(
                        pipeline.parameters(), 
                        getattr(config.train, 'max_grad_norm', 1.0)
                    )
                optimizer.step()
                optimizer.zero_grad()
        
        # Convert metrics to scalars
        return {k: torch.mean(torch.stack(v)).item() for k, v in info.items()}


def create_3d_reward_function(
    reward_config: Optional[Dict[str, float]] = None,
    device: str = "cuda"
):
    """
    Create a reward function for 3D mesh evaluation.
    
    Args:
        reward_config: 3D reward configuration dict, e.g., {"geometric_quality": 0.3, "uni3d": 0.7}
        device: Device for computation
        
    Returns:
        Reward function compatible with GRPO training
    """
    if reward_config is None:
        reward_config = {
            "geometric_quality": 0.3,
            "uni3d": 0.7
        }
    
    # Create reward function using new rewards_mesh.py
    reward_fn = multi_mesh_score(device, reward_config)
    
    def reward_fn_wrapper(meshes, images, prompts, only_strict=True):
        """
        Compute rewards for generated meshes.
        
        Args:
            meshes: List of generated mesh objects
            images: List of input image paths
            prompts: List of text prompts
            only_strict: Whether to use strict evaluation mode
            
        Returns:
            Tuple of (rewards_dict, metadata_dict)
        """
        # Use the new reward function
        reward_details, _ = reward_fn(meshes, prompts, {})
        
        # Convert to numpy arrays for compatibility
        rewards = {}
        for key, scores in reward_details.items():
            if isinstance(scores, (list, tuple)):
                rewards[key] = np.array(scores, dtype=np.float32)
            else:
                rewards[key] = np.array([scores], dtype=np.float32)
        
        # Create metadata
        metadata = {
            "num_meshes": len(meshes),
        }
        
        # Add mean and std for each score type
        for key, scores in rewards.items():
            if len(scores) > 0:
                metadata[f"{key}_mean"] = scores.mean()
                metadata[f"{key}_std"] = scores.std()
        
        return rewards, metadata
    
    return reward_fn_wrapper
