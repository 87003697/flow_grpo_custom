"""
Hunyuan3D GRPO Trainer

3D adaptation of GRPO training for Hunyuan3D model.
Handles 3D mesh generation, reward computation, and GRPO training steps.
"""
import time
import subprocess
from contextlib import contextmanager
import torch
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict
from concurrent import futures

from generators.hunyuan3d.pipeline import Hunyuan3DPipeline
from reward_models.rewards_mesh import multi_mesh_score
from reward_models.uni3d_scorer.uni3d_scorer import Uni3DScorer
from .diffusers_patch.hunyuan3d_pipeline_with_logprob import hunyuan3d_pipeline_with_logprob
from .diffusers_patch.hunyuan3d_sde_with_logprob import hunyuan3d_sde_step_with_logprob

@contextmanager
def gpu_timer(name):
    """ç»¼åˆç›‘æ§ï¼šè€—æ—¶ + GPUæ˜¾å­˜ + GPUåˆ©ç”¨ç‡"""
    
    # å¼€å§‹å‰çŠ¶æ€
    start_time = time.time()
    start_memory = torch.cuda.memory_allocated() / 1024**3  # GB
    start_reserved = torch.cuda.memory_reserved() / 1024**3  # GB
    
    print(f"ğŸ• å¼€å§‹: {name}")
    print(f"  ğŸ“Š åˆå§‹æ˜¾å­˜: {start_memory:.2f}GB (å·²åˆ†é…) / {start_reserved:.2f}GB (å·²ä¿ç•™)")
    
    # è·å–GPUåˆ©ç”¨ç‡
    def get_gpu_utilization():
        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        return int(result.stdout.strip().split('\n')[0])
    
    start_util = get_gpu_utilization()
    print(f"  âš¡ åˆå§‹GPUåˆ©ç”¨ç‡: {start_util}%")
    
    try:
        yield
    finally:
        # ç»“æŸåçŠ¶æ€
        end_time = time.time()
        end_memory = torch.cuda.memory_allocated() / 1024**3
        end_reserved = torch.cuda.memory_reserved() / 1024**3
        end_util = get_gpu_utilization()
        
        duration = end_time - start_time
        memory_delta = end_memory - start_memory
        reserved_delta = end_reserved - start_reserved
        
        print(f"âœ… å®Œæˆ: {name}")
        print(f"  â±ï¸  è€—æ—¶: {duration:.2f}ç§’")
        print(f"  ğŸ“Š ç»“æŸæ˜¾å­˜: {end_memory:.2f}GB (å·²åˆ†é…) / {end_reserved:.2f}GB (å·²ä¿ç•™)")
        print(f"  ğŸ“ˆ æ˜¾å­˜å˜åŒ–: {memory_delta:+.2f}GB (å·²åˆ†é…) / {reserved_delta:+.2f}GB (å·²ä¿ç•™)")
        print(f"  âš¡ ç»“æŸGPUåˆ©ç”¨ç‡: {end_util}%")
        print(f"  ğŸ”¥ å¹³å‡GPUåˆ©ç”¨ç‡: {(start_util + end_util) / 2:.1f}%")
        print()


class Hunyuan3DGRPOTrainer:
    """
    GRPO Trainer adapted for Hunyuan3D 3D generation.
    """
    
    def __init__(
        self,
        pipeline: Hunyuan3DPipeline,  # æ˜ç¡®ï¼šåªæ¥å— Hunyuan3DPipeline
        reward_config: Optional[Dict[str, float]] = None,
        device: str = "cuda",
        sample_batch_size: int = 1,      # ğŸ”§ æ–°å¢ï¼šé‡‡æ ·batch size
        train_batch_size: int = 2,       # ğŸ”§ æ–°å¢ï¼šè®­ç»ƒbatch size
    ):
        """
        Initialize the 3D GRPO trainer with SD3-style batch handling.
        
        Args:
            pipeline: Hunyuan3DPipeline åŒ…è£…ç±»
            reward_config: 3D reward configuration dict, e.g., {"geometric_quality": 0.3, "uni3d": 0.7}
            device: Device to run training on
            sample_batch_size: Batch size for sampling phase
            train_batch_size: Batch size for training phase
        """
        self.pipeline = pipeline
        self.device = device
        
        # Set default reward config if not provided
        if reward_config is None:
            reward_config = {
                "geometric_quality": 0.3,
                "uni3d": 0.7
            }
        
        # Create reward function using new rewards_mesh.py
        self.reward_fn = multi_mesh_score(device, reward_config)
        
        # Move core pipeline to device (æ˜ç¡®çš„è®¿é—®è·¯å¾„)
        self.pipeline.core_pipeline.to(device)
        
        # ğŸ”§ æ–°å¢ï¼šä»¿ç…§SD3é¢„å…ˆå‡†å¤‡è´Ÿé¢å›¾åƒæ¡ä»¶
        print("ğŸ”§ é¢„å…ˆå‡†å¤‡è´Ÿé¢å›¾åƒæ¡ä»¶...")
        self._prepare_negative_conditions(sample_batch_size, train_batch_size)
    
    def _prepare_negative_conditions(self, sample_batch_size: int, train_batch_size: int):
        """
        é¢„å…ˆå‡†å¤‡ä¸åŒbatch sizeçš„è´Ÿé¢å›¾åƒæ¡ä»¶ï¼Œä»¿ç…§SD3ç­–ç•¥
        
        Args:
            sample_batch_size: é‡‡æ ·é˜¶æ®µçš„batch size
            train_batch_size: è®­ç»ƒé˜¶æ®µçš„batch size
        """
        # è·å–pipelineçš„æ ¸å¿ƒç»„ä»¶
        core_pipeline = self.pipeline.core_pipeline
        
        # ğŸ”§ SD3å¼ï¼šç”Ÿæˆå•ä¸ªè´Ÿé¢æ¡ä»¶ï¼Œç„¶åå¤åˆ¶åˆ°ä¸åŒbatch size
        with torch.no_grad():
            # ä½¿ç”¨conditionerçš„unconditional_embeddingæ–¹æ³•
            if hasattr(core_pipeline, 'conditioner'):
                # ç”Ÿæˆå•ä¸ªæ ·æœ¬çš„è´Ÿé¢æ¡ä»¶
                neg_cond_single = core_pipeline.conditioner.unconditional_embedding(
                    batch_size=1,
                    device=self.device
                )
                print(f"ğŸ”§ ç”Ÿæˆçš„å•ä¸ªè´Ÿé¢æ¡ä»¶ç±»å‹: {type(neg_cond_single)}")
                
                if isinstance(neg_cond_single, dict):
                    print(f"ğŸ”§ è´Ÿé¢æ¡ä»¶å­—å…¸keys: {list(neg_cond_single.keys())}")
                    for key, value in neg_cond_single.items():
                        if isinstance(value, torch.Tensor):
                            print(f"  {key}.shape: {value.shape}")
                
                # ğŸ”§ SD3å¼ï¼šé¢„å…ˆå‡†å¤‡ä¸åŒbatch sizeçš„è´Ÿé¢æ¡ä»¶
                # ä»¿ç…§SD3: sample_neg_prompt_embeds = neg_prompt_embed.repeat(config.sample.train_batch_size, 1, 1)
                self.sample_neg_image_cond = self._expand_condition(neg_cond_single, sample_batch_size)
                self.train_neg_image_cond = self._expand_condition(neg_cond_single, train_batch_size)
                
                print(f"ğŸ”§ SD3å¼ï¼šé‡‡æ ·é˜¶æ®µè´Ÿé¢æ¡ä»¶å‡†å¤‡å®Œæˆ (batch_size={sample_batch_size})")
                print(f"ğŸ”§ SD3å¼ï¼šè®­ç»ƒé˜¶æ®µè´Ÿé¢æ¡ä»¶å‡†å¤‡å®Œæˆ (batch_size={train_batch_size})")
            else:
                raise ValueError("Pipeline does not have conditioner for negative condition preparation")
    
    def _expand_condition(self, cond_single: Union[torch.Tensor, Dict], target_batch_size: int):
        """
        å°†å•ä¸ªæ¡ä»¶æ‰©å±•åˆ°æŒ‡å®šçš„batch size
        
        Args:
            cond_single: å•ä¸ªæ ·æœ¬çš„æ¡ä»¶ (batch_size=1)
            target_batch_size: ç›®æ ‡batch size
            
        Returns:
            æ‰©å±•åçš„æ¡ä»¶ (batch_size=target_batch_size)
        """
        if isinstance(cond_single, torch.Tensor):
            return cond_single.repeat(target_batch_size, 1, 1)
        elif isinstance(cond_single, dict):
            expanded_cond = {}
            for key, value in cond_single.items():
                if isinstance(value, torch.Tensor):
                    expanded_cond[key] = value.repeat(target_batch_size, 1, 1)
                else:
                    expanded_cond[key] = value
            return expanded_cond
        else:
            raise ValueError(f"Unsupported condition type: {type(cond_single)}")
    
    def _get_negative_condition_for_batch(self, batch_size: int, mode: str = "sample"):
        """
        è·å–æŒ‡å®šbatch sizeçš„è´Ÿé¢æ¡ä»¶ï¼Œä»¿ç…§SD3çš„åŠ¨æ€è£å‰ªæ–¹å¼
        
        Args:
            batch_size: éœ€è¦çš„batch size
            mode: "sample" æˆ– "train"
            
        Returns:
            å¯¹åº”batch sizeçš„è´Ÿé¢æ¡ä»¶
        """
        if mode == "sample":
            base_neg_cond = self.sample_neg_image_cond
        else:
            base_neg_cond = self.train_neg_image_cond
        
        # ğŸ”§ SD3å¼ï¼šåŠ¨æ€è£å‰ªåˆ°æ‰€éœ€batch size
        # ä»¿ç…§SD3: train_neg_prompt_embeds[:len(sample["prompt_embeds"])]
        if isinstance(base_neg_cond, dict):
            result = {}
            for key, value in base_neg_cond.items():
                if isinstance(value, torch.Tensor):
                    result[key] = value[:batch_size]
                else:
                    result[key] = value
            return result
        else:
            return base_neg_cond[:batch_size]
    
    def sample_meshes_with_rewards(
        self,
        images: List[str],
        input_batch_size: int = 2,           # ğŸ”§ æ–°å¢ï¼šè¾“å…¥å›¾åƒæ•°é‡
        num_meshes_per_image: int = 2,       # ğŸ”§ æ–°å¢ï¼šæ¯ä¸ªå›¾åƒçš„å€™é€‰æ•°é‡
        num_inference_steps: int = 50,
        guidance_scale: float = 5.0,
        deterministic: bool = False,
        kl_reward: float = 0.0,
        # ğŸ”§ æ–°å¢ï¼šmesh é…ç½®å‚æ•°
        octree_resolution: int = 384,
        mc_level: float = 0.0,
        mc_algo: str = None,
        box_v: float = 1.01,
        num_chunks: int = 8000,
        executor: Optional[futures.ThreadPoolExecutor] = None,
    ) -> Dict[str, Any]:
        """
        Sample meshes with rewards using multi-candidate generation.
        
        Args:
            images: List of image paths
            input_batch_size: Number of different images to process
            num_meshes_per_image: Number of mesh candidates per image
            num_inference_steps: Number of denoising steps
            guidance_scale: Guidance scale for generation
            deterministic: Whether to use deterministic mode
            kl_reward: KL reward coefficient
            octree_resolution: Resolution for octree-based mesh extraction
            mc_level: Marching cubes level (iso-value)
            mc_algo: Marching cubes algorithm ('mc', 'dmc', or None)
            box_v: Bounding box volume for mesh extraction
            num_chunks: Number of chunks for mesh processing
            executor: Thread executor for async reward computation
            
        Returns:
            Dictionary with generated meshes, latents, log_probs, etc.
        """
        
        # ğŸ”§ å¤šå€™é€‰ç”Ÿæˆé€»è¾‘
        all_meshes = []
        all_latents = []
        all_log_probs = []
        all_kl = []
        all_positive_image_conds = []  # ğŸ”§ ä¿®å¤ï¼šåˆ†ç¦»å­˜å‚¨æ­£é¢æ¡ä»¶
        expanded_images = []
        
        total_meshes = input_batch_size * num_meshes_per_image
        
        with gpu_timer(f"ğŸ¯ å¤šå€™é€‰ç”Ÿæˆ - {input_batch_size}å›¾åƒÃ—{num_meshes_per_image}å€™é€‰={total_meshes}mesh"):
            # ä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆå¤šä¸ªå€™é€‰
            for i in range(input_batch_size):
                if i >= len(images):
                    break  # é˜²æ­¢è¶Šç•Œ
                    
                image_path = images[i]
                
                # ğŸ”§ ä¸ºå½“å‰å›¾åƒç”Ÿæˆå¤šä¸ªå€™é€‰
                with gpu_timer(f"å›¾åƒ{i+1}/{input_batch_size} - {num_meshes_per_image}ä¸ªå€™é€‰"):
                    # é‡å¤å½“å‰å›¾åƒï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰
                    candidate_images = [image_path] * num_meshes_per_image
                    
                    # è·å–å®é™…pipeline
                    actual_pipeline = self.pipeline.core_pipeline if hasattr(self.pipeline, 'core_pipeline') else self.pipeline
                    
                    # ğŸ”§ SD3å¼ï¼šå…ˆç¼–ç æ­£é¢å›¾åƒæ¡ä»¶
                    from PIL import Image
                    if isinstance(candidate_images[0], str):
                        pil_images = [Image.open(img_path).convert('RGBA') for img_path in candidate_images]
                    else:
                        pil_images = candidate_images
                    
                    # ä½¿ç”¨pipelineçš„æ–¹æ³•ç¼–ç å›¾åƒæ¡ä»¶
                    cond_inputs = actual_pipeline.prepare_image(pil_images)
                    image_tensor = cond_inputs.pop('image')
                    
                    # ç¼–ç æ­£é¢å›¾åƒæ¡ä»¶ï¼ˆä¸ä½¿ç”¨CFGï¼‰
                    positive_image_cond = actual_pipeline.encode_cond(
                        image=image_tensor,
                        additional_cond_inputs=cond_inputs,
                        do_classifier_free_guidance=False,  # ğŸ”§ SD3å¼ï¼šåˆ†ç¦»ç¼–ç 
                        dual_guidance=False,
                    )
                    
                    # ğŸ”§ SD3å¼ï¼šè·å–å¯¹åº”çš„è´Ÿé¢æ¡ä»¶
                    current_batch_size = len(candidate_images)
                    negative_image_cond = self._get_negative_condition_for_batch(current_batch_size, mode="sample")
                    
                    print(f"ğŸ”§ SD3å¼æ¡ä»¶ç¼–ç å®Œæˆ:")
                    print(f"  æ­£é¢æ¡ä»¶: {positive_image_cond.shape if isinstance(positive_image_cond, torch.Tensor) else 'dict'}")
                    print(f"  è´Ÿé¢æ¡ä»¶: {negative_image_cond.shape if isinstance(negative_image_cond, torch.Tensor) else 'dict'}")
                    
                    # ğŸ”§ SD3å¼ï¼šä¼ é€’åˆ†ç¦»çš„æ¡ä»¶åˆ°pipeline
                    meshes, latents, log_probs, kl, returned_pos_cond = hunyuan3d_pipeline_with_logprob(
                        actual_pipeline,
                        image=candidate_images,
                        num_inference_steps=num_inference_steps,
                        guidance_scale=guidance_scale,
                        deterministic=deterministic,
                        kl_reward=kl_reward,
                        # ğŸ”§ æ–°å¢ï¼šä¼ é€’ mesh é…ç½®å‚æ•°
                        octree_resolution=octree_resolution,
                        mc_level=mc_level,
                        mc_algo=mc_algo,
                        box_v=box_v,
                        num_chunks=num_chunks,
                        return_image_cond=True,
                        positive_image_cond=positive_image_cond,  # ğŸ”§ SD3å¼ï¼šç›´æ¥ä¼ å…¥
                        negative_image_cond=negative_image_cond,  # ğŸ”§ SD3å¼ï¼šç›´æ¥ä¼ å…¥
                    )
                    
                    all_meshes.extend(meshes if isinstance(meshes, list) else [meshes])
                    all_latents.extend(latents)
                    all_log_probs.extend(log_probs)
                    all_kl.append(kl)
                    all_positive_image_conds.append(returned_pos_cond)  # ğŸ”§ ä¿®å¤ï¼šå­˜å‚¨è¿”å›çš„æ­£é¢æ¡ä»¶
                    
                    # æ‰©å±•å›¾åƒåˆ—è¡¨ï¼ˆç”¨äºrewardè®¡ç®—ï¼‰
                    expanded_images.extend(candidate_images)
        
        # ğŸ”§ æ–°å¢ï¼šéªŒè¯æ•°æ®ä¸€è‡´æ€§
        expected_total = min(input_batch_size, len(images)) * num_meshes_per_image
        assert len(all_meshes) == expected_total, f"Expected {expected_total} meshes, got {len(all_meshes)}"
        assert len(expanded_images) == expected_total, f"Expected {expected_total} images, got {len(expanded_images)}"
        
        # è®¡ç®—rewards
        with gpu_timer("ğŸ† å¥–åŠ±å‡½æ•°è®¡ç®—"):
            if executor:
                reward_future = executor.submit(
                    self.reward_fn, 
                    all_meshes, 
                    None,  # prompts - ä¸éœ€è¦
                    {},    # metadata
                    expanded_images  # ğŸ”§ ä¼ é€’å›¾åƒ
                )
                rewards = reward_future
            else:
                rewards = self.reward_fn(all_meshes, None, {}, expanded_images)
        
        # æ•°æ®æ‰“åŒ…
        with gpu_timer("ğŸ“¦ ç»“æœæ‰“åŒ…"):
            # æŒ‰SD3æ–¹å¼å¤„ç†tensor
            latents_tensor = torch.stack(all_latents, dim=1) if all_latents else torch.empty(0)
            log_probs_tensor = torch.stack(all_log_probs, dim=1) if all_log_probs else torch.empty(0)
            
            # å¤„ç†KL tensor
            if all_kl:
                all_kl_tensors = []
                for kl in all_kl:
                    if isinstance(kl, torch.Tensor):
                        all_kl_tensors.append(kl)
                    elif isinstance(kl, (list, tuple)):
                        if len(kl) > 0 and isinstance(kl[0], torch.Tensor):
                            kl_tensor = torch.stack(kl).transpose(0, 1)  # (batch_size, num_steps)
                        else:
                            kl_tensor = torch.tensor(kl)
                        all_kl_tensors.append(kl_tensor)
                    else:
                        all_kl_tensors.append(torch.tensor(kl))
                
                kl_tensor = torch.cat(all_kl_tensors, dim=0)
            else:
                kl_tensor = torch.empty(0)
            
            # ğŸ”§ å®Œå…¨ä»¿ç…§SD3çš„æ—¶é—´æ­¥å¤„ç†æ–¹å¼
            # SD3æ–¹å¼: timesteps = pipeline.scheduler.timesteps.repeat(batch_size, 1)
            # ä¸å†ä½¿ç”¨å¤æ‚çš„retrieve_timestepsï¼Œç›´æ¥ä½¿ç”¨schedulerçš„timesteps
            
            # ç”Ÿæˆtimesteps - ğŸ”§ å®Œå…¨ä»¿ç…§SD3çš„å®ç°
            num_steps = latents_tensor.shape[1] - 1 if latents_tensor.numel() > 0 else 20
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå®Œå…¨æŒ‰ç…§SD3çš„timestepså¤„ç†æ–¹å¼
            # SD3: timesteps = pipeline.scheduler.timesteps.repeat(config.sample.train_batch_size, 1)
            actual_pipeline = self.pipeline.core_pipeline if hasattr(self.pipeline, 'core_pipeline') else self.pipeline
            
            try:
                # ğŸ”§ SD3å¼ï¼šç›´æ¥ä½¿ç”¨schedulerçš„timestepsï¼Œè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆ
                # ç¡®ä¿schedulerçš„timestepså·²ç»æ­£ç¡®è®¾ç½®
                if hasattr(actual_pipeline.scheduler, 'timesteps') and len(actual_pipeline.scheduler.timesteps) > 0:
                    # ä½¿ç”¨schedulerç°æœ‰çš„timesteps
                    scheduler_timesteps = actual_pipeline.scheduler.timesteps
                    
                    # å¦‚æœschedulerçš„timestepsæ•°é‡ä¸å¤Ÿï¼Œé‡æ–°è®¾ç½®
                    if len(scheduler_timesteps) < num_steps:
                        actual_pipeline.scheduler.set_timesteps(num_steps, device=self.device)
                        scheduler_timesteps = actual_pipeline.scheduler.timesteps
                    
                    # å–å‰num_stepsä¸ªæ—¶é—´æ­¥
                    used_timesteps = scheduler_timesteps[:num_steps]
                    
                    # ğŸ”§ SD3å¼ï¼šé‡å¤timestepsåˆ°æ¯ä¸ªæ ·æœ¬
                    # SD3: timesteps = pipeline.scheduler.timesteps.repeat(config.sample.train_batch_size, 1)
                    timesteps_tensor = used_timesteps.unsqueeze(0).repeat(expected_total, 1)
                    
                    print(f"ğŸ”§ SD3å¼æ—¶é—´æ­¥ç”ŸæˆæˆåŠŸ:")
                    print(f"  ä½¿ç”¨scheduler.timesteps: {scheduler_timesteps[:5]}... (å‰5ä¸ª)")
                    print(f"  ç”Ÿæˆçš„æ—¶é—´æ­¥å½¢çŠ¶: {timesteps_tensor.shape}")
                    print(f"  æ—¶é—´æ­¥èŒƒå›´: [{timesteps_tensor.min():.1f}, {timesteps_tensor.max():.1f}]")
                    
                else:
                    # å¦‚æœscheduleræ²¡æœ‰timestepsï¼Œå…ˆè®¾ç½®
                    actual_pipeline.scheduler.set_timesteps(num_steps, device=self.device)
                    scheduler_timesteps = actual_pipeline.scheduler.timesteps
                    timesteps_tensor = scheduler_timesteps.unsqueeze(0).repeat(expected_total, 1)
                    
                    print(f"ğŸ”§ é‡æ–°è®¾ç½®scheduler timesteps:")
                    print(f"  timestepså½¢çŠ¶: {timesteps_tensor.shape}")
                    print(f"  æ—¶é—´æ­¥èŒƒå›´: [{timesteps_tensor.min():.1f}, {timesteps_tensor.max():.1f}]")
                
            except Exception as e:
                print(f"ğŸš¨ SD3å¼æ—¶é—´æ­¥ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å®‰å…¨æ¨¡å¼: {e}")
                # ğŸ”§ å®‰å…¨å›é€€ï¼šç”Ÿæˆæ ‡å‡†çš„é€’å‡æ—¶é—´æ­¥åºåˆ—
                max_timesteps = getattr(actual_pipeline.scheduler.config, 'num_train_timesteps', 1000)
                timesteps_list = torch.linspace(max_timesteps-1, 0, num_steps, device=self.device, dtype=torch.long)
                timesteps_tensor = timesteps_list.unsqueeze(0).repeat(expected_total, 1)
                
                print(f"ğŸ”§ å®‰å…¨å›é€€æ—¶é—´æ­¥:")
                print(f"  ç”Ÿæˆé€’å‡åºåˆ—: [{timesteps_tensor[0, 0]:.0f} -> {timesteps_tensor[0, -1]:.0f}]")
                print(f"  æ—¶é—´æ­¥å½¢çŠ¶: {timesteps_tensor.shape}")
            
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†æ­£é¢å›¾åƒæ¡ä»¶ï¼Œä»¿ç…§SD3çš„æ–¹å¼
            positive_image_cond_tensor = all_positive_image_conds[0] if all_positive_image_conds else None
            
            return {
                "meshes": all_meshes,
                "images": expanded_images,
                "latents": latents_tensor,
                "log_probs": log_probs_tensor,
                "kl": kl_tensor,
                "rewards": rewards,
                "timesteps": timesteps_tensor,
                "positive_image_cond": positive_image_cond_tensor,  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                # ğŸ”§ æ–°å¢ï¼šå…ƒæ•°æ®
                "metadata": {
                    "input_batch_size": input_batch_size,
                    "num_meshes_per_image": num_meshes_per_image,
                    "total_meshes": expected_total,
                }
            }
    
    def _compute_rewards_sync(
        self, 
        meshes: List, 
        images: List[str], 
        prompts: List[str]
    ) -> Dict[str, torch.Tensor]:
        """Compute rewards synchronously and return as tensors on the correct device."""
        # Use the reward function to compute scores
        reward_details, _ = self.reward_fn(meshes, prompts, {})
        
        # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šåˆ›å»ºtensorï¼Œé¿å…è®¾å¤‡è½¬æ¢
        rewards = {}
        for key, scores in reward_details.items():
            if isinstance(scores, (list, tuple)):
                # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šåˆ›å»ºï¼Œé¿å…CPU->CUDAè½¬æ¢
                rewards[key] = torch.tensor(scores, device=self.device, dtype=torch.float32)
                print(f"ğŸ”§ ä¼˜åŒ–ï¼š{key} å¥–åŠ±ç›´æ¥åœ¨ {self.device} ä¸Šåˆ›å»ºï¼Œå½¢çŠ¶ {rewards[key].shape}")
            else:
                rewards[key] = torch.tensor([scores], device=self.device, dtype=torch.float32)
                print(f"ğŸ”§ ä¼˜åŒ–ï¼š{key} å¥–åŠ±(æ ‡é‡)ç›´æ¥åœ¨ {self.device} ä¸Šåˆ›å»º")
        
        return rewards
    
    def _compute_rewards_async(
        self, 
        meshes: List, 
        images: List[str], 
        prompts: List[str]
    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
        """Compute rewards asynchronously (for compatibility with original trainer)."""
        rewards = self._compute_rewards_sync(meshes, images, prompts)
        metadata = {
            "num_meshes": len(meshes),
            "avg_score": rewards["avg"].mean().item() if "avg" in rewards else 0.0,
        }
        return rewards, metadata
    
    def compute_log_prob_3d(
        self,
        pipeline,
        sample: Dict[str, torch.Tensor],
        step_index: int,
        config: Any,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Compute log probability for a specific timestep in 3D generation.
        Uses SD3-style dynamic condition assembly for flexible batch handling.
        
        Args:
            pipeline: Hunyuan3D pipeline
            sample: Sample data containing latents, timesteps, etc.
            step_index: Index of the timestep to compute
            config: Training configuration
            
        Returns:
            Tuple of (prev_sample, log_prob, prev_sample_mean, std_dev)
        """
        # Get the latents and timestep for this step
        latents = sample["latents"][:, step_index]  # Current latents
        next_latents = sample["next_latents"][:, step_index]  # Target next latents
        timestep = sample["timesteps"][:, step_index]  # Current timestep
        
        current_batch_size = latents.shape[0]
        print(f"ğŸ”§ SD3å¼æ¡ä»¶å¤„ç†ï¼šå½“å‰batch_size={current_batch_size}")
        
        # ğŸ”§ SD3å¼åŠ¨æ€æ¡ä»¶ç»„åˆ - ç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…å¤æ‚çš„æ˜ å°„é€»è¾‘
        if "positive_image_cond" in sample and sample["positive_image_cond"] is not None:
            # è·å–æ­£é¢æ¡ä»¶
            pos_cond = sample["positive_image_cond"]
            if isinstance(pos_cond, dict) and 'main' in pos_cond:
                pos_cond = pos_cond['main']
            
            print(f"ğŸ”§ æ­£é¢æ¡ä»¶åŸå§‹å½¢çŠ¶: {pos_cond.shape}")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç®€åŒ–æ¡ä»¶å¤„ç†ï¼Œé¿å…å¤æ‚çš„metadataæ˜ å°„
            # å¦‚æœæ¡ä»¶çš„batch sizeä¸åŒ¹é…ï¼Œç›´æ¥é‡å¤æˆ–åˆ‡ç‰‡
            if pos_cond.shape[0] != current_batch_size:
                if pos_cond.shape[0] == 1:
                    # å¦‚æœåªæœ‰1ä¸ªæ¡ä»¶ï¼Œé‡å¤åˆ°current_batch_size
                    pos_cond = pos_cond.repeat(current_batch_size, 1, 1)
                    print(f"ğŸ”§ é‡å¤æ¡ä»¶åˆ°batch size: {pos_cond.shape}")
                elif pos_cond.shape[0] > current_batch_size:
                    # å¦‚æœæ¡ä»¶å¤ªå¤šï¼Œåˆ‡ç‰‡åˆ°current_batch_size
                    pos_cond = pos_cond[:current_batch_size]
                    print(f"ğŸ”§ åˆ‡ç‰‡æ¡ä»¶åˆ°batch size: {pos_cond.shape}")
                else:
                    # å¦‚æœæ¡ä»¶å¤ªå°‘ï¼Œé‡å¤æœ€åä¸€ä¸ªæ¡ä»¶
                    last_cond = pos_cond[-1:].repeat(current_batch_size - pos_cond.shape[0], 1, 1)
                    pos_cond = torch.cat([pos_cond, last_cond], dim=0)
                    print(f"ğŸ”§ æ‰©å±•æ¡ä»¶åˆ°batch size: {pos_cond.shape}")
            
            print(f"ğŸ”§ æœ€ç»ˆæ­£é¢æ¡ä»¶: {pos_cond.shape}")
            
            # ğŸ”§ å®Œå…¨ä»¿ç…§SD3çš„CFGå¤„ç†é€»è¾‘
            if hasattr(config.train, 'cfg') and config.train.cfg:
                # ğŸ”§ SD3å¼ï¼šåŠ¨æ€ç»„åˆè´Ÿé¢å’Œæ­£é¢æ¡ä»¶
                neg_cond = self._get_negative_condition_for_batch(current_batch_size, mode="train")
                if isinstance(neg_cond, dict) and 'main' in neg_cond:
                    neg_cond = neg_cond['main']
                
                print(f"ğŸ”§ è´Ÿé¢æ¡ä»¶ï¼ˆåŠ¨æ€è£å‰ªï¼‰: {neg_cond.shape}")
                
                # ğŸ”§ SD3å¼ï¼šç»„åˆCFGæ ¼å¼ [negative_batch, positive_batch]
                cond = torch.cat([neg_cond, pos_cond], dim=0)
                print(f"ğŸ”§ SD3å¼ç»„åˆåCFGæ¡ä»¶: {cond.shape}")
            else:
                # ğŸ”§ SD3å¼ï¼šç¦ç”¨CFGæ—¶åªä½¿ç”¨æ­£é¢æ¡ä»¶
                cond = pos_cond
                print(f"ğŸ”§ éCFGæ¨¡å¼ï¼Œä½¿ç”¨æ­£é¢æ¡ä»¶: {cond.shape}")
                
        else:
            # ğŸ”§ å‘åå…¼å®¹ï¼šå¤„ç†æ—§æ ¼å¼çš„image_cond
            if "image_cond" in sample and sample["image_cond"] is not None:
                cond = sample["image_cond"]
                if isinstance(cond, dict) and 'main' in cond:
                    cond = cond['main']
                print(f"ğŸ”§ å‘åå…¼å®¹ï¼šä½¿ç”¨image_cond {cond.shape}")
            else:
                raise ValueError("No image conditions found in sample")
        
        # ğŸ”§ å‡†å¤‡æ¨¡å‹è¾“å…¥ - ä»¿ç…§SD3çš„latent_model_inputå¤„ç†
        latent_model_input = latents
        if hasattr(config.train, 'cfg') and config.train.cfg:
            # ğŸ”§ CFGæ¨¡å¼ï¼šå¤åˆ¶latentsï¼ˆä»¿ç…§SD3ï¼‰
            latent_model_input = torch.cat([latent_model_input, latent_model_input])
            print(f"ğŸ”§ CFGæ¨¡å¼ï¼šlatent_model_input.shape = {latent_model_input.shape}")
            print(f"ğŸ”§ CFGæ¨¡å¼ï¼šcond.shape = {cond.shape}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿è®­ç»ƒé˜¶æ®µä¸é‡‡æ ·é˜¶æ®µçš„æ—¶é—´æ­¥å¤„ç†å®Œå…¨ä¸€è‡´
        # é‡‡æ ·é˜¶æ®µï¼štimestep = timestep / self.scheduler.config.num_train_timesteps
        # è®­ç»ƒé˜¶æ®µï¼šä¹Ÿå¿…é¡»åšç›¸åŒçš„æ ‡å‡†åŒ–
        
        # Convert timestep to normalized format
        # ğŸ”§ é‡è¦ä¿®å¤ï¼šä¿æŒä¸é‡‡æ ·é˜¶æ®µä¸€è‡´çš„æ ‡å‡†åŒ–
        # é‡‡æ ·é˜¶æ®µï¼štimestep / self.scheduler.config.num_train_timesteps
        timestep_float = timestep.float()
        timestep_normalized = timestep_float / pipeline.scheduler.config.num_train_timesteps
        
        # ğŸ”§ ç¡®ä¿æ•°æ®ç±»å‹ä¸latentsä¸€è‡´ï¼ˆä»¿ç…§é‡‡æ ·é˜¶æ®µï¼‰
        timestep_tensor = timestep_normalized.to(device=latents.device, dtype=latents.dtype)
        
        # ğŸ”§ CFGæ¨¡å¼ä¸‹ä¹Ÿéœ€è¦å¤åˆ¶timestepï¼ˆä¿æŒä¸SD3ä¸€è‡´ï¼‰
        if hasattr(config.train, 'cfg') and config.train.cfg:
            timestep_tensor = torch.cat([timestep_tensor, timestep_tensor])
        
        # ğŸ”§ ç§»é™¤clampæ“ä½œï¼šSD3ä¸å¯¹timestepsè¿›è¡Œclamp
        # timestep_tensor = torch.clamp(timestep_tensor, 0.0, 1.0)  # åˆ é™¤è¿™è¡Œ
        
        # ğŸ”§ éªŒè¯æ‰€æœ‰å¼ é‡çš„å½¢çŠ¶å’Œè®¾å¤‡
        print(f"ğŸ” æ¨¡å‹è¾“å…¥éªŒè¯:")
        print(f"  latent_model_input.shape: {latent_model_input.shape}")
        print(f"  timestep_tensor.shape: {timestep_tensor.shape}")
        print(f"  timestep_tensor.dtype: {timestep_tensor.dtype}")
        print(f"  timestep_tensor.device: {timestep_tensor.device}")
        print(f"  timestep_tensorèŒƒå›´: [{timestep_tensor.min():.6f}, {timestep_tensor.max():.6f}]")
        print(f"  cond.shape: {cond.shape}")
        print(f"  æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡: {latent_model_input.device == timestep_tensor.device == cond.device}")
        
        # ğŸ”§ æ·»åŠ æ›´å®‰å…¨çš„NaNæ£€æŸ¥
        try:
            has_nan_latent = torch.isnan(latent_model_input).any().item()
            has_nan_timestep = torch.isnan(timestep_tensor).any().item()
            has_nan_cond = torch.isnan(cond).any().item()
            print(f"  NaNæ£€æŸ¥: latent={has_nan_latent}, timestep={has_nan_timestep}, cond={has_nan_cond}")
        except Exception as e:
            print(f"  NaNæ£€æŸ¥å¤±è´¥: {e}")
        
        # Predict noise using the model
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ¨èçš„torch.amp.autocast('cuda')æ›¿ä»£è¿‡æ—¶çš„torch.cuda.amp.autocast
        with torch.amp.autocast('cuda'):
            # ğŸ”§ ä¿®å¤ï¼šæ¨¡å‹æœŸæœ›çš„æ˜¯contextså‚æ•°ï¼Œä¸æ˜¯cond
            contexts = {'main': cond}
            
            try:
                noise_pred = pipeline.model(latent_model_input, timestep_tensor, contexts)
                print(f"ğŸ‰ æ¨¡å‹è°ƒç”¨æˆåŠŸï¼noise_pred.shape: {noise_pred.shape}")
            except Exception as e:
                print(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
                print(f"ğŸ” é”™è¯¯ç±»å‹: {type(e)}")
                
                # ğŸ”§ æ·»åŠ æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                print(f"ğŸ” è¯¦ç»†è°ƒè¯•ä¿¡æ¯:")
                print(f"  pipeline.model: {type(pipeline.model)}")
                print(f"  latent_model_input: shape={latent_model_input.shape}, dtype={latent_model_input.dtype}, device={latent_model_input.device}")
                print(f"  timestep_tensor: shape={timestep_tensor.shape}, dtype={timestep_tensor.dtype}, device={timestep_tensor.device}")
                print(f"  contexts['main']: shape={contexts['main'].shape}, dtype={contexts['main'].dtype}, device={contexts['main'].device}")
                
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿æŸ¥çœ‹å®Œæ•´å †æ ˆ
                raise
        
        # ğŸ”§ Apply classifier-free guidance if enabledï¼ˆä»¿ç…§SD3ï¼‰
        if hasattr(config.train, 'cfg') and config.train.cfg:
            noise_pred_neg, noise_pred_pos = noise_pred.chunk(2)
            guidance_scale = getattr(config.sample, 'guidance_scale', 5.0)
            noise_pred = noise_pred_neg + guidance_scale * (noise_pred_pos - noise_pred_neg)
        
        # ğŸ”§ ä½¿ç”¨configä¸­çš„deterministicè®¾ç½®
        deterministic = getattr(config, 'deterministic', False)
        
        # Compute log probability using our SDE implementation
        prev_sample, log_prob, prev_sample_mean, std_dev = hunyuan3d_sde_step_with_logprob(
            scheduler=pipeline.scheduler,
            model_output=noise_pred,
            timestep=timestep[0],  # Assume batch has same timestep
            sample=latents,
            prev_sample=next_latents,  # Use target as reference
            deterministic=deterministic,
        )
        
        return prev_sample, log_prob, prev_sample_mean, std_dev
    
    def train_step(
        self,
        samples: Dict[str, torch.Tensor],
        pipeline,
        optimizer: torch.optim.Optimizer,
        config: Any,
        accelerator: Any,
    ) -> Dict[str, float]:
        """
        Perform a single GRPO training step for 3D generation.
        
        Args:
            samples: Batch of samples with latents, rewards, etc.
            pipeline: Hunyuan3D pipeline
            optimizer: Optimizer for training
            config: Training configuration
            accelerator: HuggingFace accelerator
            
        Returns:
            Dictionary of training metrics
        """
        info = defaultdict(list)
        num_timesteps = samples["timesteps"].shape[1]
        
        # Train on each timestep
        for j in range(num_timesteps):
            with accelerator.accumulate(pipeline.model):
                with accelerator.autocast():
                    # Compute log probability for current timestep
                    prev_sample, log_prob, prev_sample_mean, std_dev = self.compute_log_prob_3d(
                        pipeline, samples, j, config
                    )
                    
                    # Compute reference log probability if beta > 0 (KL regularization)
                    if getattr(config.train, 'beta', 0) > 0:
                        with torch.no_grad():
                            # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ disable_adapterï¼ŒPeftModel å¿…é¡»æ”¯æŒæ­¤æ–¹æ³•
                            with pipeline.model.disable_adapter():
                                _, log_prob_ref, prev_sample_mean_ref, std_dev_ref = self.compute_log_prob_3d(
                                    pipeline, samples, j, config
                                )
                
                # GRPO loss computation
                advantages = torch.clamp(
                    samples["advantages"][:, j],
                    -getattr(config.train, 'adv_clip_max', 5.0),
                    getattr(config.train, 'adv_clip_max', 5.0),
                )
                
                # Compute policy ratio
                ratio = torch.exp(log_prob - samples["log_probs"][:, j])
                
                # PPO clipped loss
                unclipped_loss = -advantages * ratio
                clipped_loss = -advantages * torch.clamp(
                    ratio,
                    1.0 - getattr(config.train, 'clip_range', 0.2),
                    1.0 + getattr(config.train, 'clip_range', 0.2),
                )
                policy_loss = torch.mean(torch.maximum(unclipped_loss, clipped_loss))
                
                # KL divergence loss
                if getattr(config.train, 'beta', 0) > 0:
                    kl_loss = ((prev_sample_mean - prev_sample_mean_ref) ** 2).mean(dim=tuple(range(1, prev_sample_mean.ndim))) / (2 * std_dev ** 2)
                    kl_loss = torch.mean(kl_loss)
                    loss = policy_loss + config.train.beta * kl_loss
                else:
                    kl_loss = torch.tensor(0.0)
                    loss = policy_loss
                
                # Collect metrics
                info["approx_kl"].append(
                    0.5 * torch.mean((log_prob - samples["log_probs"][:, j]) ** 2)
                )
                info["clipfrac"].append(
                    torch.mean(
                        (torch.abs(ratio - 1.0) > getattr(config.train, 'clip_range', 0.2)).float()
                    )
                )
                info["policy_loss"].append(policy_loss)
                if getattr(config.train, 'beta', 0) > 0:
                    info["kl_loss"].append(kl_loss)
                info["loss"].append(loss)
                
                # Backward pass
                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(
                        pipeline.model.parameters(),  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ model.parameters() è€Œä¸æ˜¯ pipeline.parameters()
                        getattr(config.train, 'max_grad_norm', 1.0)
                    )
                optimizer.step()
                optimizer.zero_grad()
        
        # Convert metrics to scalars
        return {k: torch.mean(torch.stack(v)).item() for k, v in info.items()}


def create_3d_reward_function(
    reward_config: Optional[Dict[str, float]] = None,
    device: str = "cuda"
):
    """
    Create a reward function for 3D mesh evaluation with image support.
    
    Args:
        reward_config: 3D reward configuration dict, e.g., {"geometric_quality": 0.3, "uni3d": 0.7}
        device: Device for computation
        
    Returns:
        Reward function compatible with GRPO training and image input
    """
    if reward_config is None:
        reward_config = {
            "geometric_quality": 0.3,
            "uni3d": 0.7  # ğŸ”§ ç°åœ¨å¯ä»¥ä½¿ç”¨å›¾åƒäº†ï¼
        }
    
    # Create reward function using new rewards_mesh.py
    reward_fn = multi_mesh_score(device, reward_config)
    
    def reward_fn_wrapper(meshes, prompts, metadata, images=None):  # ğŸ”§ æ–°å¢ images å‚æ•°
        """
        Compute rewards for generated meshes with image support.
        
        Args:
            meshes: List of generated mesh objects
            prompts: List of text prompts (can be None)
            metadata: Metadata dictionary
            images: List of input image paths (for image-based scoring)
            
        Returns:
            Tuple of (rewards_dict, metadata_dict)
        """
        # ğŸ”§ ä¼ é€’ images å‚æ•°
        reward_details, _ = reward_fn(meshes, prompts, metadata, images)
        
        # Convert to numpy arrays for compatibility
        rewards = {}
        for key, scores in reward_details.items():
            if isinstance(scores, (list, tuple)):
                rewards[key] = np.array(scores, dtype=np.float32)
            else:
                rewards[key] = np.array([scores], dtype=np.float32)
        
        # Create metadata
        reward_metadata = {
            "num_meshes": len(meshes),
        }
        
        # Add mean and std for each score type
        for key, scores in rewards.items():
            if len(scores) > 0:
                reward_metadata[f"{key}_mean"] = scores.mean()
                reward_metadata[f"{key}_std"] = scores.std()
        
        return rewards, reward_metadata
    
    return reward_fn_wrapper
