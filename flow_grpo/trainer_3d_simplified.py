"""
ç®€åŒ–ç‰ˆHunyuan3D GRPO Trainer - ç»Ÿä¸€æ•°æ®æ ¼å¼ç‰ˆæœ¬

æ ¸å¿ƒåŸåˆ™ï¼š
1. ç»Ÿä¸€å…¥å£ï¼šåœ¨é‡‡æ ·é˜¶æ®µç¡®ä¿æ•°æ®æ ¼å¼ç»Ÿä¸€
2. ç®€å•å‡è®¾ï¼šè®­ç»ƒé˜¶æ®µç›´æ¥å‡è®¾æ•°æ®æ ¼å¼æ­£ç¡®
3. ä¿æŒåŸç”Ÿæ¥å£ï¼šå§‹ç»ˆä½¿ç”¨contexts={'main': cond}
"""

import torch
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict
from concurrent import futures

from generators.hunyuan3d.pipeline import Hunyuan3DPipeline
from reward_models.rewards_mesh import multi_mesh_score
from .diffusers_patch.hunyuan3d_pipeline_with_logprob import hunyuan3d_pipeline_with_logprob
from .diffusers_patch.hunyuan3d_sde_with_logprob import hunyuan3d_sde_step_with_logprob


class Hunyuan3DGRPOTrainer:
    """ç®€åŒ–ç‰ˆGRPO Trainer - ç»Ÿä¸€æ•°æ®æ ¼å¼å¤„ç†"""
    
    def __init__(
        self,
        pipeline: Hunyuan3DPipeline,
        reward_config: Dict[str, float] = {"geometric_quality": 0.3, "uni3d": 0.7},
        device: str = "cuda",
    ):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.pipeline = pipeline
        self.device = device
        self.reward_fn = multi_mesh_score(device, reward_config)
        self.pipeline.core_pipeline.to(device)
    
    def sample_meshes_with_rewards(
        self,
        images: List[str],
        num_inference_steps: int = 50,
        guidance_scale: float = 5.0,
        deterministic: bool = False,
        kl_reward: float = 0.0,
        octree_resolution: int = 384,
        mc_level: float = 0.0,
        mc_algo: str = None,
        box_v: float = 1.01,
        num_chunks: int = 50000,
        num_meshes_per_image: int = 1,  # ğŸ”§ æ·»åŠ ï¼šæ¯ä¸ªå›¾åƒçš„meshå€™é€‰æ•°é‡
        executor: Optional[futures.ThreadPoolExecutor] = None,
    ) -> Dict[str, Any]:
        """é‡‡æ ·é˜¶æ®µï¼šç¡®ä¿æ•°æ®æ ¼å¼ç»Ÿä¸€"""
        from PIL import Image
        
        # ğŸ”§ å¤šå€™é€‰ç”Ÿæˆï¼šä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆå¤šä¸ªå€™é€‰mesh
        all_pil_images = []
        for img_path in images:
            # ä¸ºå½“å‰å›¾åƒç”Ÿæˆ num_meshes_per_image ä¸ªå€™é€‰
            candidate_images = [img_path] * num_meshes_per_image
            pil_candidates = [Image.open(path).convert('RGBA') for path in candidate_images]
            all_pil_images.extend(pil_candidates)
        
        pil_images = all_pil_images
        
        core_pipeline = self.pipeline.core_pipeline
        
        # ç¼–ç å›¾åƒæ¡ä»¶
        cond_inputs = core_pipeline.prepare_image(pil_images)
        image_tensor = cond_inputs.pop('image')
        
        positive_image_cond = core_pipeline.encode_cond(
            image=image_tensor,
            additional_cond_inputs=cond_inputs,
            do_classifier_free_guidance=False,
            dual_guidance=False,
        )
        
        # ğŸ”§ å…³é”®ï¼šåœ¨è¿™é‡Œç»Ÿä¸€æ ¼å¼ï¼Œåç»­ä¸å†å¤„ç†
        if not isinstance(positive_image_cond, dict):
            positive_image_cond = {'main': positive_image_cond}
        
        # è°ƒç”¨pipeline
        meshes, all_latents, all_log_probs, all_kl, returned_pos_cond = hunyuan3d_pipeline_with_logprob(
            core_pipeline,
            image=pil_images,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            deterministic=deterministic,
            kl_reward=kl_reward,
            return_image_cond=True,
            positive_image_cond=positive_image_cond,
            octree_resolution=octree_resolution,
            mc_level=mc_level,
            mc_algo=mc_algo,
            box_v=box_v,
            num_chunks=num_chunks,
        )
        
        # è®¡ç®—å¥–åŠ±
        reward_details, metadata = self.reward_fn(meshes, None, {}, images)
        rewards_tensor = {
            "avg": torch.tensor(reward_details['avg'], device=self.device, dtype=torch.float32)
        }
        
        # å¤„ç†latentsæ•°æ®
        latents_tensor = torch.stack(all_latents, dim=1)
        current_latents = latents_tensor[:, :-1]  # å‰n-1ä¸ªæ—¶é—´æ­¥
        next_latents = latents_tensor[:, 1:]      # ån-1ä¸ªæ—¶é—´æ­¥
        
        # å¤„ç†log_probs
        log_probs_tensor = torch.stack(all_log_probs, dim=1)
        
        # ğŸ”§ ä¿®å¤ï¼šå¤„ç†KL tensorï¼Œç¡®ä¿ç»´
        kl_tensor = torch.stack(all_kl, dim=1)
        
        # å¤„ç†timesteps
        timesteps_tensor = self._get_timesteps(len(images), num_inference_steps - 1)
        
        # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨
        returned_pos_cond = returned_pos_cond['main']
        
        return {
            "meshes": meshes,
            "images": images,
            "latents": current_latents,
            "next_latents": next_latents,
            "log_probs": log_probs_tensor,
            "kl": kl_tensor,  # ğŸ”§ ä¿®å¤ï¼šå¤„ç†KL tensorä¸º2ç»´å¼ é‡
            "rewards": rewards_tensor,
            "timesteps": timesteps_tensor,
            "positive_image_cond": returned_pos_cond,  # ğŸ”§ ä½¿ç”¨pipelineè¿”å›çš„ç»Ÿä¸€æ ¼å¼
        }
    
    def _get_timesteps(self, batch_size: int, num_steps: int) -> torch.Tensor:
        """ç”Ÿæˆæ ‡å‡†åŒ–çš„æ—¶é—´æ­¥å¼ é‡"""
        scheduler_timesteps = self.pipeline.core_pipeline.scheduler.timesteps
        if len(scheduler_timesteps) < num_steps:
            self.pipeline.core_pipeline.scheduler.set_timesteps(num_steps + 1, device=self.device)
            scheduler_timesteps = self.pipeline.core_pipeline.scheduler.timesteps
        
        used_timesteps = scheduler_timesteps[:num_steps]
        return used_timesteps.unsqueeze(0).repeat(batch_size, 1)
    
    def compute_log_prob_3d(
        self,
        pipeline,
        sample: Dict[str, torch.Tensor],
        step_index: int,
        config: Any,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """è®­ç»ƒé˜¶æ®µï¼šå‡è®¾æ•°æ®æ ¼å¼å·²ç»ç»Ÿä¸€"""
        # è·å–æ•°æ®
        latents = sample["latents"][:, step_index]
        next_latents = sample["next_latents"][:, step_index]
        timestep = sample["timesteps"][:, step_index]
        
        # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨ç»Ÿä¸€æ ¼å¼çš„tensor
        cond = sample["positive_image_cond"]
        
        # ğŸ”§ ç®€å•å¤„ç†ï¼šç¡®ä¿batch_sizeåŒ¹é…
        if cond.shape[0] != latents.shape[0]:
            cond = cond.repeat_interleaved(latents.shape[0] // cond.shape[0], dim=0)
        
        # ğŸ”§ ç®€å•å¤„ç†ï¼šæ—¶é—´æ­¥æ ‡å‡†åŒ–
        timestep_normalized = timestep.float() / pipeline.scheduler.config.num_train_timesteps
        
        # ğŸ”§ ç®€å•å¤„ç†ï¼šæ„å»ºcontexts
        contexts = {'main': cond}
        
        # æ¨¡å‹é¢„æµ‹
        with torch.amp.autocast('cuda'):
            noise_pred = pipeline.model(latents, timestep_normalized, contexts)
        
        # è®¡ç®—logæ¦‚ç‡
        prev_sample, log_prob, prev_sample_mean, std_dev = hunyuan3d_sde_step_with_logprob(
            scheduler=pipeline.scheduler,
            model_output=noise_pred,
            timestep=timestep[0],
            sample=latents,
            prev_sample=next_latents,
            deterministic=getattr(config, 'deterministic', False),
        )
        
        return prev_sample, log_prob, prev_sample_mean, std_dev
    
    def train_step(
        self,
        samples: Dict[str, torch.Tensor],
        pipeline,
        optimizer: torch.optim.Optimizer,
        config: Any,
        accelerator: Any,
        autocast=None,  # ğŸ”§ æ·»åŠ autocastå‚æ•°
    ) -> Dict[str, float]:
        """ç®€åŒ–ç‰ˆè®­ç»ƒæ­¥éª¤"""
        info = defaultdict(list)
        num_timesteps = samples["timesteps"].shape[1]
        
        # è®­ç»ƒæ¯ä¸ªæ—¶é—´æ­¥
        for j in range(num_timesteps):
            with accelerator.accumulate(pipeline.model):
                with (autocast() if autocast else accelerator.autocast()):
                    # è®¡ç®—logæ¦‚ç‡
                    prev_sample, log_prob, prev_sample_mean, std_dev = self.compute_log_prob_3d(
                        pipeline, samples, j, config
                    )
                    
                    # å‚è€ƒlogæ¦‚ç‡
                    with torch.no_grad():
                        # ğŸ”§ æŒ‰ç…§SD3æ¨¡å¼ï¼šå®‰å…¨è®¿é—®DDPåŒ…è£…åçš„æ¨¡å‹
                        model_for_adapter = pipeline.model.module if hasattr(pipeline.model, 'module') else pipeline.model
                        with model_for_adapter.disable_adapter():
                            _, log_prob_ref, prev_sample_mean_ref, std_dev_ref = self.compute_log_prob_3d(
                                    pipeline, samples, j, config
                                )
                    
                    # è®¡ç®—GRPOæŸå¤±
                    advantages = torch.clamp(
                        samples["advantages"][:, j],
                        -config.train.adv_clip_max,
                        config.train.adv_clip_max,
                    )
                    
                    # è®¡ç®—æ¯”ç‡
                    ratio = torch.exp(log_prob - samples["log_probs"][:, j])
                    
                    # PPOæŸå¤±
                    loss1 = -advantages * ratio
                    loss2 = -advantages * torch.clamp(
                        ratio,
                        1.0 - config.train.clip_range,
                        1.0 + config.train.clip_range,
                    )
                    policy_loss = torch.max(loss1, loss2).mean()
                    
                    # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®çš„KLæŸå¤±è®¡ç®—ï¼Œä½¿ç”¨prev_sample_meanè€Œä¸æ˜¯log_prob
                    if getattr(config.train, 'beta', 0) > 0:
                        kl_loss = ((prev_sample_mean - prev_sample_mean_ref) ** 2).mean(dim=tuple(range(1, prev_sample_mean.ndim))) / (2 * std_dev ** 2)
                        kl_loss = torch.mean(kl_loss)
                        loss = policy_loss + config.train.beta * kl_loss
                    else:
                        kl_loss = torch.tensor(0.0, device=policy_loss.device)
                        loss = policy_loss
                    
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                
                # è®°å½•ä¿¡æ¯
                info["loss"].append(loss.item())
                info["kl_loss"].append(kl_loss.mean().item())
                info["advantages"].append(advantages.mean().item())
                info["ratio"].append(ratio.mean().item())
                
                # æ¢¯åº¦è£å‰ª - ğŸ”§ å®Œå…¨ç¦ç”¨ä»¥è§£å†³FP16é—®é¢˜
                if accelerator.sync_gradients:
                    # ğŸ”§ å®Œå…¨è·³è¿‡æ¢¯åº¦è£å‰ªä»¥é¿å…FP16 unscalingé—®é¢˜
                    print(f"âš ï¸  æ¢¯åº¦è£å‰ªå·²ç¦ç”¨ä»¥è§£å†³FP16é—®é¢˜")
                
                optimizer.step()
                optimizer.zero_grad()
        
        # è¿”å›å¹³å‡ç»Ÿè®¡
        return {key: np.mean(values) for key, values in info.items()} 