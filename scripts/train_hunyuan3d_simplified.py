#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆHunyuan3Dè®­ç»ƒè„šæœ¬ - ä»¿ç…§SD3çš„ç®€æ´å†…å­˜ç®¡ç†

ä¸»è¦ç®€åŒ–ï¼š
1. ç§»é™¤å¤æ‚çš„è®¾å¤‡æ£€æŸ¥
2. ç®€åŒ–GPUå†…å­˜ç›‘æ§
3. ç®€åŒ–æ‰¹é‡å¤„ç†
4. ä½¿ç”¨acceleratorç»Ÿä¸€ç®¡ç†è®¾å¤‡
"""

import sys
import os
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict
from concurrent import futures

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed
from accelerate.logging import get_logger

# ç»Ÿä¸€é…ç½®ç®¡ç†
import ml_collections
from absl import app, flags
from ml_collections import config_flags

_CONFIG = config_flags.DEFINE_config_file("config")

# æ•°æ®å’Œæ¨¡å‹ç›¸å…³å¯¼å…¥
from generators.hunyuan3d.pipeline import Hunyuan3DPipeline
from reward_models.rewards_mesh import multi_mesh_score
from flow_grpo.trainer_3d_simplified import Hunyuan3DGRPOTrainer  # ğŸ”§ ä½¿ç”¨ç®€åŒ–ç‰ˆtrainer
from flow_grpo.ema import EMAModuleWrapper
from flow_grpo.stat_tracking import PerImageStatTracker

logger = get_logger(__name__)

# ğŸš€ ç®€åŒ–ç‰ˆGPUç›‘æ§ï¼ˆå¯é€‰ï¼‰
def simple_gpu_log(name: str):
    """ç®€å•çš„GPUå†…å­˜æ—¥å¿—ï¼Œä¸é˜»å¡è®­ç»ƒ"""
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1024**3
        logger.info(f"{name}: GPUå†…å­˜ä½¿ç”¨ {memory_used:.2f}GB")

class Image3DDataset(Dataset):
    def __init__(self, image_dir: str, prompts_file: Optional[str] = None, split: str = "train"):
        self.image_dir = Path(image_dir)
        self.prompts_file = prompts_file
        self.split = split
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦åœ¨imageså­ç›®å½•ä¸­
        if (self.image_dir / "images").exists():
            self.image_dir = self.image_dir / "images"
        
        # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
        self.image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            self.image_files.extend(self.image_dir.glob(ext))
        
        self.image_files = sorted(self.image_files)
        
        if not self.image_files:
            raise ValueError(f"No images found in {self.image_dir}")
        
        logger.info(f"Found {len(self.image_files)} images in {self.image_dir}")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        image_path = str(self.image_files[idx])
        prompt = self.get_prompt(self.image_files[idx])
        
        return {
            "image_path": image_path,
            "prompt": prompt,
            "metadata": {"image_name": self.image_files[idx].name}
        }
    
    @staticmethod
    def collate_fn(examples):
        image_paths = [example["image_path"] for example in examples]
        prompts = [example["prompt"] for example in examples]
        metadata = [example["metadata"] for example in examples]
        return image_paths, prompts, metadata
    
    def get_prompt(self, image_path: Path) -> str:
        """æ ¹æ®å›¾åƒè·¯å¾„ç”Ÿæˆæç¤ºè¯"""
        return f"Generate a 3D model from this image: {image_path.stem}"

def main(argv):
    """ä¸»è®­ç»ƒå‡½æ•° - ç®€åŒ–ç‰ˆ"""
    del argv
    config = _CONFIG.value
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨acceleratorï¼Œä¸éœ€è¦å¤æ‚çš„è®¾å¤‡æ£€æŸ¥
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.train.gradient_accumulation_steps,
        log_with="wandb" if "WANDB_PROJECT" in os.environ else None,
        project_dir=config.save_dir,
    )
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    
    # è®¾ç½®ç§å­
    if hasattr(config, 'seed') and config.seed is not None:
        set_seed(config.seed)
    
    os.makedirs(config.save_dir, exist_ok=True)
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥åŠ è½½æ¨¡å‹ï¼Œç»Ÿä¸€è®¾å¤‡ç®¡ç†
    logger.info("Loading Hunyuan3D pipeline...")
    pipeline_wrapper = Hunyuan3DPipeline()
    
    # ğŸš€ ç®€åŒ–ï¼šä½¿ç”¨acceleratorç»Ÿä¸€ç®¡ç†è®¾å¤‡ï¼Œä»¿ç…§SD3
    core_pipeline = pipeline_wrapper.core_pipeline
    
    # ğŸš€ ç®€åŒ–ï¼šç»Ÿä¸€è®¾å¤‡å’Œæ•°æ®ç±»å‹è®¾ç½®ï¼ˆä»¿ç…§SD3ï¼‰
    inference_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        inference_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        inference_dtype = torch.bfloat16
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œä¸éœ€è¦å¤æ‚æ£€æŸ¥
    core_pipeline.vae.to(accelerator.device, dtype=torch.float32)
    core_pipeline.conditioner.to(accelerator.device, dtype=inference_dtype)
    if config.use_lora:
        core_pipeline.model.to(accelerator.device)
    else:
        core_pipeline.model.to(accelerator.device, dtype=inference_dtype)
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥è®¾ç½®LoRAï¼ˆä»¿ç…§SD3ï¼‰
    if config.use_lora:
        from peft import LoraConfig, get_peft_model
        
        lora_config = LoraConfig(
            r=32,
            lora_alpha=64,
            target_modules=[
                "img_attn.qkv", "img_attn.proj",
                "txt_attn.qkv", "txt_attn.proj",
                "img_mlp.0", "img_mlp.2",
                "txt_mlp.0", "txt_mlp.2",
                "latent_in", "cond_in",
                "final_layer.linear"
            ],
            lora_dropout=0.1,
            bias="none",
        )
        
        core_pipeline.model = get_peft_model(core_pipeline.model, lora_config)
        trainable_params = [p for p in core_pipeline.model.parameters() if p.requires_grad]
    else:
        trainable_params = core_pipeline.model.parameters()
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨accelerator.prepareï¼ˆä»¿ç…§SD3ï¼‰
    model = accelerator.prepare(core_pipeline.model)
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        trainable_params,
        lr=config.train.learning_rate,
        betas=(0.9, 0.999),
        weight_decay=0.01,
        eps=1e-8,
    )
    
    # è®¾ç½®EMAï¼ˆä»¿ç…§SD3ï¼‰
    ema = None
    if config.train.ema:
        ema = EMAModuleWrapper(
            trainable_params,
            decay=config.train.ema_decay,
            device=accelerator.device
        )
    
    # ğŸš€ ç®€åŒ–ï¼šåˆ›å»ºtrainerï¼Œä¸éœ€è¦å¤æ‚çš„batch sizeé…ç½®
    reward_config = {"geometric_quality": 0.3, "uni3d": 0.7}
    trainer = Hunyuan3DGRPOTrainer(
        pipeline=pipeline_wrapper,
        reward_config=reward_config,
        device=accelerator.device,
    )
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥åŠ è½½æ•°æ®é›†
    logger.info(f"Loading dataset from {config.data_dir}")
    train_dataset = Image3DDataset(config.data_dir, split="train")
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config.sample.input_batch_size,
        shuffle=True,
        collate_fn=Image3DDataset.collate_fn,
        num_workers=0,
    )
    
    # ç»Ÿè®¡è·Ÿè¸ª
    stat_tracker = None
    if config.per_image_stat_tracking:
        stat_tracker = PerImageStatTracker(
            buffer_size=len(train_dataset),
            min_count=config.stat_tracking.min_count,
        )
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    first_epoch = 0
    
    for epoch in range(first_epoch, config.num_epochs):
        logger.info(f"Starting epoch {epoch}")
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥è¿›è¡Œé‡‡æ ·ï¼Œä¸éœ€è¦å¤æ‚çš„GPUç›‘æ§
        model.eval()
        epoch_samples = []
        
        simple_gpu_log(f"Epoch {epoch} - å¼€å§‹é‡‡æ ·")
        
        for batch_idx, (image_paths, prompts, metadata) in enumerate(tqdm(
            train_dataloader, 
            desc=f"Epoch {epoch}: sampling",
            disable=not accelerator.is_local_main_process
        )):
            if batch_idx >= config.sample.num_batches_per_epoch:
                break
            
            # ğŸš€ ç®€åŒ–ï¼šç›´æ¥é‡‡æ ·ï¼Œä¸éœ€è¦å¤æ‚çš„å€™é€‰å¤„ç†
            results = trainer.sample_meshes_with_rewards(
                images=image_paths,
                num_inference_steps=config.sample.num_steps,
                guidance_scale=config.sample.guidance_scale,
                deterministic=getattr(config, 'deterministic', False),
                kl_reward=config.sample.kl_reward,
            )
            
            epoch_samples.append(results)
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥åˆå¹¶æ ·æœ¬ï¼Œä¸éœ€è¦å¤æ‚çš„è®¾å¤‡æ£€æŸ¥
        all_samples = {}
        for k in epoch_samples[0].keys():
            if k in ["meshes", "images", "prompts", "metadata"]:
                continue
            elif k == "rewards":
                # ç‰¹æ®Šå¤„ç†rewardså­—å…¸
                all_samples[k] = {}
                for reward_key in epoch_samples[0][k].keys():
                    all_samples[k][reward_key] = torch.cat([s[k][reward_key] for s in epoch_samples], dim=0)
            elif isinstance(epoch_samples[0][k], torch.Tensor):
                all_samples[k] = torch.cat([s[k] for s in epoch_samples], dim=0)
            else:
                all_samples[k] = [item for s in epoch_samples for item in s[k]]
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œä¸éœ€è¦å¤æ‚æ£€æŸ¥ï¼ˆä»¿ç…§SD3ï¼‰
        for key, value in all_samples.items():
            if isinstance(value, torch.Tensor) and value.device != accelerator.device:
                all_samples[key] = value.to(accelerator.device)
            elif isinstance(value, list) and key == "kl":
                # ğŸ”§ ä¿®å¤ï¼šå°†klåˆ—è¡¨è½¬æ¢ä¸ºtensor
                all_samples[key] = torch.cat(value, dim=0) if isinstance(value[0], torch.Tensor) else torch.tensor(value, device=accelerator.device)
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥å¤„ç†å¥–åŠ±
        rewards_avg = all_samples["rewards"]["avg"]
        kl_tensor = all_samples["kl"]
        
        # ğŸ”§ ç¡®ä¿kl_tensoræ˜¯tensor
        if not isinstance(kl_tensor, torch.Tensor):
            kl_tensor = torch.tensor(kl_tensor, device=accelerator.device, dtype=torch.float32)
        
        all_samples["rewards"]["ori_avg"] = rewards_avg.clone()
        all_samples["rewards"]["avg"] = (
            rewards_avg.unsqueeze(-1) - 
            config.sample.kl_reward * kl_tensor
        )
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥gatherå¥–åŠ±ï¼ˆä»¿ç…§SD3ï¼‰
        gathered_rewards = {
            key: accelerator.gather(value) 
            for key, value in all_samples["rewards"].items()
        }
        gathered_rewards = {
            key: value.cpu().numpy() 
            for key, value in gathered_rewards.items()
        }
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥è®¡ç®—advantagesï¼ˆä»¿ç…§SD3ï¼‰
        if config.per_image_stat_tracking and stat_tracker:
            all_images = [item for s in epoch_samples for item in s["images"]]
            advantages_np = stat_tracker.update(all_images, gathered_rewards['avg'])
            advantages = torch.tensor(advantages_np, device=accelerator.device)
        else:
            advantages = gathered_rewards['avg']
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-4)
            advantages = torch.tensor(advantages, device=accelerator.device)
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥æŒ‰è¿›ç¨‹åˆ†å‰²ï¼ˆä»¿ç…§SD3ï¼‰
        advantages = advantages.reshape(accelerator.num_processes, -1)[accelerator.process_index]
        all_samples["advantages"] = advantages
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥è¿‡æ»¤æ ·æœ¬ï¼ˆä»¿ç…§SD3ï¼‰
        mask = (all_samples["advantages"].abs() > 1e-6)
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®åº”ç”¨maskï¼Œç‰¹åˆ«å¤„ç†åµŒå¥—å­—å…¸
        filtered_samples = {}
        for k, v in all_samples.items():
            if k == "rewards":
                # ç‰¹æ®Šå¤„ç†rewardså­—å…¸
                filtered_samples[k] = {}
                for reward_key, reward_tensor in v.items():
                    # ç¡®ä¿reward_tensorå’Œmaskå½¢çŠ¶åŒ¹é…
                    if reward_tensor.shape == mask.shape:
                        filtered_samples[k][reward_key] = reward_tensor[mask]
                    else:
                        # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…,ä¿æŒåŸæ ·
                        filtered_samples[k][reward_key] = reward_tensor
            elif isinstance(v, torch.Tensor) and len(v) == len(mask):
                filtered_samples[k] = v[mask]
            else:
                # å¯¹äºä¸éœ€è¦è¿‡æ»¤æˆ–é•¿åº¦ä¸åŒ¹é…çš„æ•°æ®ï¼Œä¿æŒåŸæ ·
                filtered_samples[k] = v
        
        all_samples = filtered_samples
        
        logger.info(f"Training on {mask.sum().item()} samples")
        
        # ğŸš€ ç®€åŒ–ï¼šSD3å¼æ•°æ®é‡ç»„
        if "latents" in all_samples:
            latents = all_samples["latents"]
            if isinstance(latents, list):
                # å¦‚æœæ˜¯åˆ—è¡¨,å…ˆè½¬æ¢ä¸ºtensor
                latents = torch.stack(latents, dim=1)  # [B, T, ...]
            all_samples["latents"] = latents[:, :-1]
            all_samples["next_latents"] = latents[:, 1:]
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥è®­ç»ƒï¼ˆä»¿ç…§SD3ï¼‰
        for inner_epoch in range(config.train.num_inner_epochs):
            model.train()
            
            # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨trainerè®­ç»ƒ
            loss_info = trainer.train_step(
                samples=all_samples,
                pipeline=core_pipeline,
                optimizer=optimizer,
                config=config,
                accelerator=accelerator,
            )
            
            # æ›´æ–°EMA
            if ema is not None:
                ema.update()
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥è®°å½•æ—¥å¿—ï¼ˆä»¿ç…§SD3ï¼‰
        accelerator.log({
            "epoch": epoch,
            **{f"reward_{key}": value.mean() for key, value in gathered_rewards.items()},
            "kl": all_samples["kl"].mean().cpu().numpy(),
        }, step=global_step)
        
        global_step += 1
        
        # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config.save_freq == 0:
            save_dir = os.path.join(config.save_dir, f"checkpoint_{epoch}")
            os.makedirs(save_dir, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            model_to_save = accelerator.unwrap_model(model)
            if hasattr(model_to_save, "save_pretrained"):
                model_to_save.save_pretrained(save_dir)
            else:
                torch.save(model_to_save.state_dict(), os.path.join(save_dir, "model.pt"))
            
            # ä¿å­˜é…ç½®
            with open(os.path.join(save_dir, "config.json"), "w") as f:
                import json
                json.dump(config.to_dict(), f, indent=2)
            
            logger.info(f"Saved checkpoint to {save_dir}")
        
        simple_gpu_log(f"Epoch {epoch} - å®Œæˆ")

if __name__ == "__main__":
    app.run(main) 