#!/usr/bin/env python3
"""
Hunyuan3D GRPOè®­ç»ƒè„šæœ¬ - å†…è”æ¶æ„ï¼ˆç±»ä¼¼SD3ï¼‰

æ¶æ„æ”¹è¿›ï¼š
1. ç§»é™¤ç‹¬ç«‹trainerç±»ï¼Œæ‰€æœ‰é€»è¾‘å†…è”åˆ°mainå‡½æ•°
2. ç›´æ¥ä½¿ç”¨core_pipelineï¼Œæ— åŒ…è£…å™¨
3. é‡‡æ ·ã€è®­ç»ƒã€è¯„ä¼°åœ¨ç»Ÿä¸€è„šæœ¬ä¸­å¤„ç†
4. ä¸SD3ä¿æŒä¸€è‡´çš„ä»£ç ç»„ç»‡ç»“æ„
"""

import sys
import os
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict
from concurrent import futures

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed
from accelerate.logging import get_logger

# ç»Ÿä¸€é…ç½®ç®¡ç†
import ml_collections
from absl import app, flags
from ml_collections import config_flags

_CONFIG = config_flags.DEFINE_config_file("config")

# æ•°æ®å’Œæ¨¡å‹ç›¸å…³å¯¼å…¥
from generators.hunyuan3d.pipeline import Hunyuan3DPipeline
from reward_models.rewards_mesh import multi_mesh_score
from flow_grpo.diffusers_patch.hunyuan3d_pipeline_with_logprob import hunyuan3d_pipeline_with_logprob
from flow_grpo.diffusers_patch.hunyuan3d_sde_with_logprob import hunyuan3d_sde_step_with_logprob
from flow_grpo.ema import EMAModuleWrapper
from flow_grpo.stat_tracking import PerImageStatTracker

logger = get_logger(__name__)

# ğŸš€ ç®€åŒ–ç‰ˆGPUç›‘æ§ï¼ˆå¯é€‰ï¼‰
def simple_gpu_log(name: str):
    """ç®€å•çš„GPUå†…å­˜æ—¥å¿—ï¼Œä¸é˜»å¡è®­ç»ƒ"""
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1024**3
        logger.info(f"{name}: GPUå†…å­˜ä½¿ç”¨ {memory_used:.2f}GB")


class Image3DDataset(Dataset):
    def __init__(self, image_dir: str, prompts_file: Optional[str] = None, split: str = "train"):
        self.image_dir = Path(image_dir)
        self.prompts_file = prompts_file
        self.split = split
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦åœ¨imageså­ç›®å½•ä¸­
        if (self.image_dir / "images").exists():
            self.image_dir = self.image_dir / "images"
        
        # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
        self.image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            self.image_files.extend(self.image_dir.glob(ext))
        
        self.image_files = sorted(self.image_files)
        
        if not self.image_files:
            raise ValueError(f"No images found in {self.image_dir}")
        
        logger.info(f"Found {len(self.image_files)} images in {self.image_dir}")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        image_path = str(self.image_files[idx])
        prompt = self.get_prompt(self.image_files[idx])
        
        return {
            "image_path": image_path,
            "prompt": prompt,
            "metadata": {"image_name": self.image_files[idx].name}
        }
    
    @staticmethod
    def collate_fn(examples):
        image_paths = [example["image_path"] for example in examples]
        prompts = [example["prompt"] for example in examples]
        metadata = [example["metadata"] for example in examples]
        return image_paths, prompts, metadata
    
    def get_prompt(self, image_path: Path) -> str:
        """æ ¹æ®å›¾åƒè·¯å¾„ç”Ÿæˆæç¤ºè¯"""
        return f"Generate a 3D model from this image: {image_path.stem}"

def get_timesteps(pipeline, batch_size: int, num_steps: int, device: str) -> torch.Tensor:
    """ç”Ÿæˆæ ‡å‡†åŒ–çš„æ—¶é—´æ­¥å¼ é‡"""
    if hasattr(pipeline.scheduler, 'timesteps'):
        # æ‰©æ•£è°ƒåº¦å™¨æœ‰timestepså±æ€§
        timesteps = pipeline.scheduler.timesteps[:num_steps]
    else:
        # æ‰‹åŠ¨ç”Ÿæˆæ—¶é—´æ­¥
        timesteps = torch.linspace(
            pipeline.scheduler.config.num_train_timesteps - 1, 
            0, 
            num_steps, 
            dtype=torch.long
        )
    
    # æ‰©å±•åˆ°batchç»´åº¦
    timesteps = timesteps.unsqueeze(0).repeat(batch_size, 1)
    return timesteps.to(device)

def compute_log_prob_3d(pipeline, sample: Dict[str, torch.Tensor], step_index: int, config: Any):
    """è®¡ç®—3Dæ‰©æ•£æ¨¡å‹çš„logæ¦‚ç‡ - ç±»ä¼¼SD3çš„compute_log_prob"""
    # è·å–æ•°æ®
    latents = sample["latents"][:, step_index]
    next_latents = sample["next_latents"][:, step_index]
    timestep = sample["timesteps"][:, step_index]
    
    # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨ç»Ÿä¸€æ ¼å¼çš„tensor
    cond = sample["positive_image_cond"]
    
    # ğŸ”§ ç®€å•å¤„ç†ï¼šç¡®ä¿batch_sizeåŒ¹é…
    if cond.shape[0] != latents.shape[0]:
        cond = cond.repeat_interleaved(latents.shape[0] // cond.shape[0], dim=0)
    
    # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šæ—¶é—´æ­¥æ ‡å‡†åŒ–ä¸è£å‰ª
    timestep_normalized = torch.clamp(
        timestep.float() / pipeline.scheduler.config.num_train_timesteps, 
        min=1e-6, max=1.0 - 1e-6
    )
    
    # ğŸ”§ ç®€å•å¤„ç†ï¼šæ„å»ºcontexts
    contexts = {'main': cond}
    
    # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šæ£€æŸ¥è¾“å…¥
    if torch.isnan(latents).any() or torch.isinf(latents).any():
        logger.warning(f"âš ï¸  è¾“å…¥latentsåŒ…å«NaNæˆ–Infå€¼")
        latents = torch.nan_to_num(latents, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # æ¨¡å‹é¢„æµ‹
    with torch.amp.autocast('cuda'):
        noise_pred = pipeline.model(latents, timestep_normalized, contexts)
    
    # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šæ£€æŸ¥æ¨¡å‹è¾“å‡º
    if torch.isnan(noise_pred).any() or torch.isinf(noise_pred).any():
        logger.warning(f"âš ï¸  æ¨¡å‹è¾“å‡ºåŒ…å«NaNæˆ–Infå€¼")
        noise_pred = torch.nan_to_num(noise_pred, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # è®¡ç®—logæ¦‚ç‡
    prev_sample, log_prob, prev_sample_mean, std_dev = hunyuan3d_sde_step_with_logprob(
        scheduler=pipeline.scheduler,
        model_output=noise_pred,
        timestep=timestep[0],
        sample=latents,
        prev_sample=next_latents,
    )
    
    # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šæ£€æŸ¥è¾“å‡ºå¹¶è¿›è¡Œè£å‰ª
    if torch.isnan(log_prob).any() or torch.isinf(log_prob).any():
        logger.warning(f"âš ï¸  log_probåŒ…å«NaNæˆ–Infå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        log_prob = torch.zeros_like(log_prob)
    
    if torch.isnan(prev_sample_mean).any() or torch.isinf(prev_sample_mean).any():
        logger.warning(f"âš ï¸  prev_sample_meanåŒ…å«NaNæˆ–Infå€¼ï¼Œä½¿ç”¨è£å‰ªå€¼")
        prev_sample_mean = torch.nan_to_num(prev_sample_mean, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šstd_devè£å‰ªé˜²æ­¢è¿‡å¤§å€¼
    std_dev = torch.clamp(std_dev, min=1e-6, max=100.0)

    return prev_sample, log_prob, prev_sample_mean, std_dev


def save_ckpt_hunyuan3d(model, ema, optimizer, epoch, global_step, save_dir, accelerator):
    """
    SD3é£æ ¼çš„æ£€æŸ¥ç‚¹ä¿å­˜å‡½æ•°
    
    Args:
        model: è®­ç»ƒæ¨¡å‹
        ema: EMAåŒ…è£…å™¨
        optimizer: ä¼˜åŒ–å™¨
        epoch: å½“å‰epoch
        global_step: å…¨å±€æ­¥æ•°
        save_dir: ä¿å­˜ç›®å½•
        accelerator: Acceleratorå¯¹è±¡
    """
    checkpoint_dir = os.path.join(save_dir, f"checkpoints", f"checkpoint-{global_step}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ğŸ”§ SD3å¯¹é½ï¼šä¿å­˜æ¨¡å‹çŠ¶æ€
    unwrapped_model = accelerator.unwrap_model(model)
    model_state = unwrapped_model.state_dict()
    
    # ğŸ”§ SD3å¯¹é½ï¼šä¿å­˜ä¸»æ¨¡å‹
    model_path = os.path.join(checkpoint_dir, "pytorch_model.bin")
    torch.save(model_state, model_path)
    
    # ğŸ”§ SD3å¯¹é½ï¼šä¿å­˜EMAï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if ema is not None:
        ema_state = ema.state_dict()
        ema_path = os.path.join(checkpoint_dir, "pytorch_model_ema.bin")
        torch.save(ema_state, ema_path)
    
    # ğŸ”§ SD3å¯¹é½ï¼šä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
    optimizer_path = os.path.join(checkpoint_dir, "optimizer.bin")
    torch.save(optimizer.state_dict(), optimizer_path)
    
    # ğŸ”§ SD3å¯¹é½ï¼šä¿å­˜è®­ç»ƒå…ƒä¿¡æ¯
    metadata = {
        "epoch": epoch,
        "global_step": global_step,
        "pytorch_version": torch.__version__,
    }
    metadata_path = os.path.join(checkpoint_dir, "training_metadata.json")
    import json
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)
    
    logger.info(f"âœ… SD3é£æ ¼æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_dir}")


def main(argv):
    """ä¸»è®­ç»ƒå‡½æ•° - å†…è”æ¶æ„ï¼ˆç±»ä¼¼SD3ï¼‰+ SD3å†…å­˜ç®¡ç†ç­–ç•¥"""
    del argv
    config = _CONFIG.value
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨acceleratorï¼Œä¸éœ€è¦å¤æ‚çš„è®¾å¤‡æ£€æŸ¥
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.train.gradient_accumulation_steps,
        log_with="wandb" if "WANDB_PROJECT" in os.environ else None,
        project_dir=config.save_dir,
    )
    
    # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šå¯ç”¨PyTorchå†…å­˜ä¼˜åŒ–ç­–ç•¥
    torch.backends.cudnn.benchmark = False  # å‡å°‘å†…å­˜ç¢ç‰‡
    torch.backends.cuda.max_split_size_mb = 128  # é™åˆ¶å†…å­˜åˆ†å‰²å¤§å°
    
    # ğŸ”§ Flash Attentionä¼˜åŒ–ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®
    attention_config = getattr(config, 'attention_optimization', None)
    if attention_config:
        if hasattr(torch.backends.cuda, 'enable_flash_sdp') and attention_config.enable_flash_sdp:
            torch.backends.cuda.enable_flash_sdp(True)
            logger.info("âœ… Flash Attention å·²å¯ç”¨")
        
        if hasattr(torch.backends.cuda, 'enable_mem_efficient_sdp') and attention_config.enable_mem_efficient_sdp:
            torch.backends.cuda.enable_mem_efficient_sdp(True)
            logger.info("âœ… Memory Efficient Attention å·²å¯ç”¨")
        
        if hasattr(torch.backends.cuda, 'enable_math_sdp'):
            torch.backends.cuda.enable_math_sdp(attention_config.enable_math_sdp)
            if not attention_config.enable_math_sdp:
                logger.info("âœ… Math SDPA å·²ç¦ç”¨ï¼ˆä¼˜å…ˆä½¿ç”¨Flash/Memory Efficientï¼‰")
        
        # TF32ä¼˜åŒ–
        if hasattr(torch.backends.cuda, 'allow_tf32') and attention_config.allow_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            logger.info("âœ… TF32åŠ é€Ÿ å·²å¯ç”¨")
    else:
        # ğŸ”§ å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰attention_optimizationé…ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
        if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
            torch.backends.cuda.enable_flash_sdp(True)  # å¯ç”¨Flash Attention
            if hasattr(torch.backends.cuda, 'enable_mem_efficient_sdp'):
                torch.backends.cuda.enable_mem_efficient_sdp(True)  # å¯ç”¨Memory Efficient Attention
            if hasattr(torch.backends.cuda, 'enable_math_sdp'):
                torch.backends.cuda.enable_math_sdp(False)  # ç¦ç”¨æ•°å­¦SDPAï¼Œä¼˜å…ˆä½¿ç”¨Flash/Memory Efficient
            if hasattr(torch.backends.cuda, 'allow_tf32'):
                torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸TF32åŠ é€ŸçŸ©é˜µä¹˜æ³•
                torch.backends.cudnn.allow_tf32 = True
            logger.info("ğŸš€ é»˜è®¤Attentionä¼˜åŒ–å·²å¯ç”¨: Flash Attention + Memory Efficient Attention")
    
    # âœ¨ æ–°å¢ï¼šSD3é£æ ¼çš„TF32ä¼˜åŒ–ç®¡ç†
    if config.allow_tf32:
        torch.backends.cuda.matmul.allow_tf32 = True
        logger.info("âœ… SD3é£æ ¼TF32ä¼˜åŒ–å·²å¯ç”¨")
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    
    # è®¾ç½®ç§å­
    if hasattr(config, 'seed') and config.seed is not None:
        set_seed(config.seed)
    
    # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šæ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        logger.info(f"ğŸ§¹ GPUå†…å­˜æ¸…ç†å®Œæˆï¼Œå¯ç”¨å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    os.makedirs(config.save_dir, exist_ok=True)
    
    # ğŸš€ ç›´æ¥åŠ è½½pipelineï¼Œæ— åŒ…è£…å™¨ï¼ˆç±»ä¼¼SD3ï¼‰
    logger.info("Loading Hunyuan3D pipeline...")
    pipeline_wrapper = Hunyuan3DPipeline()
    
    # ğŸš€ è·å–æ ¸å¿ƒpipelineï¼Œç›´æ¥æ“ä½œï¼ˆç±»ä¼¼SD3ç›´æ¥ä½¿ç”¨StableDiffusion3Pipelineï¼‰
    pipeline = pipeline_wrapper.core_pipeline
    
    # âœ¨ æ–°å¢ï¼šSD3é£æ ¼çš„ç²¾åº¦ç®¡ç† - æ›´æ™ºèƒ½çš„inference_dtypeé€‰æ‹©
    inference_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        inference_dtype = torch.float16
        logger.info("âœ… ä½¿ç”¨FP16æ¨ç†ç²¾åº¦")
    elif accelerator.mixed_precision == "bf16":
        inference_dtype = torch.bfloat16
        logger.info("âœ… ä½¿ç”¨BF16æ¨ç†ç²¾åº¦")
    else:
        logger.info("âœ… ä½¿ç”¨FP32æ¨ç†ç²¾åº¦")
    
    # âœ¨ æ–°å¢ï¼šSD3é£æ ¼çš„æ¨¡å‹å‚æ•°å†»ç»“ç­–ç•¥ - æ˜ç¡®ç®¡ç†å“ªäº›å‚æ•°éœ€è¦æ¢¯åº¦
    logger.info("ğŸ”§ SD3é£æ ¼å‚æ•°å†»ç»“ç­–ç•¥...")
    pipeline.vae.requires_grad_(False)
    pipeline.conditioner.requires_grad_(False)
    pipeline.model.requires_grad_(not config.use_lora)
    logger.info("âœ… æ¨¡å‹å‚æ•°æ¢¯åº¦è®¾ç½®å®Œæˆ")
    
    # âœ¨ æ–°å¢ï¼šSD3é£æ ¼çš„åˆ†å±‚è®¾å¤‡ç§»åŠ¨ - ä¸åŒç»„ä»¶ä½¿ç”¨ä¸åŒç²¾åº¦ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    logger.info("ğŸ”§ SD3é£æ ¼åˆ†å±‚è®¾å¤‡ç§»åŠ¨...")
    
    # VAEä¿æŒFP32ï¼ˆSD3ç­–ç•¥ï¼‰- ç”¨äºé«˜ç²¾åº¦è§£ç 
    pipeline.vae.to(accelerator.device, dtype=torch.float32)
    
    # Conditionerä½¿ç”¨æ¨ç†ç²¾åº¦ï¼ˆSD3ç­–ç•¥ï¼‰- èŠ‚çœå†…å­˜
    pipeline.conditioner.to(accelerator.device, dtype=inference_dtype)
    
    # Modelçš„ç²¾åº¦ç­–ç•¥ï¼šLoRAæ—¶ä¸å¼ºåˆ¶ç²¾åº¦è½¬æ¢ï¼ˆSD3ç­–ç•¥ï¼‰
    if config.use_lora:
        pipeline.model.to(accelerator.device)  # LoRAæ—¶è®©ç³»ç»Ÿè‡ªåŠ¨ç®¡ç†ç²¾åº¦
        logger.info("âœ… LoRAæ¨¡å¼ï¼šæ¨¡å‹ç²¾åº¦ç”±ç³»ç»Ÿè‡ªåŠ¨ç®¡ç†")
    else:
        pipeline.model.to(accelerator.device, dtype=inference_dtype)
        logger.info(f"âœ… å…¨å‚æ•°è®­ç»ƒï¼šæ¨¡å‹ä½¿ç”¨{inference_dtype}ç²¾åº¦")
    
    # ğŸš€ å…³é”®ä¿®å¤ï¼šæ˜¾å¼ç¦ç”¨VAEå’Œconditionerçš„æ¢¯åº¦ï¼Œè®¾ç½®evalæ¨¡å¼ï¼ˆç±»ä¼¼SD3ï¼‰
    logger.info("ğŸ”§ è®¾ç½®VAEå’Œconditionerä¸ºæ¨ç†æ¨¡å¼...")
    pipeline.vae.eval()
    pipeline.conditioner.eval()
    pipeline.vae.requires_grad_(False)
    pipeline.conditioner.requires_grad_(False)
    logger.info("âœ… VAEå’Œconditioneræ¢¯åº¦å·²ç¦ç”¨ï¼Œå·²è®¾ç½®ä¸ºevalæ¨¡å¼")
    
    # âœ¨ æ–°å¢ï¼šSD3é£æ ¼çš„å†…å­˜ä¼˜åŒ–ç­–ç•¥é€‰æ‹©
    memory_optimization_level = getattr(config, 'memory_optimization_level', 'aggressive')
    
    if memory_optimization_level == 'aggressive':
        # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šè®­ç»ƒæ—¶å°†VAEç§»åŠ¨åˆ°CPUä»¥èŠ‚çœæ˜¾å­˜ï¼ˆHunyuan3Dç‰¹æœ‰ï¼‰
        logger.info("ğŸš€ æ¿€è¿›å†…å­˜ä¼˜åŒ–ï¼šå°†VAEç§»åŠ¨åˆ°CPUä»¥èŠ‚çœè®­ç»ƒæ˜¾å­˜...")
        pipeline.vae.to('cpu')
        logger.info("âœ… VAEå·²ç§»åŠ¨åˆ°CPUï¼Œæ˜¾å­˜èŠ‚çœçº¦8-12GB")
    elif memory_optimization_level == 'moderate':
        # SD3é£æ ¼ï¼šVAEä¿ç•™åœ¨GPUä½†ä½¿ç”¨FP16
        if inference_dtype != torch.float32:
            pipeline.vae.to(accelerator.device, dtype=inference_dtype)
            logger.info(f"âœ… ä¸­ç­‰å†…å­˜ä¼˜åŒ–ï¼šVAEä½¿ç”¨{inference_dtype}ç²¾åº¦")
    else:
        # conservative: ä¿æŒVAEåœ¨GPU FP32ï¼ˆSD3é»˜è®¤ï¼‰
        logger.info("âœ… ä¿å®ˆå†…å­˜ç­–ç•¥ï¼šVAEä¿æŒGPU FP32ç²¾åº¦")
    
    # ğŸš€ LoRAè®¾ç½®ï¼ˆç±»ä¼¼SD3ï¼‰
    if config.use_lora:
        from peft import LoraConfig, get_peft_model
        
        lora_config = LoraConfig(
            r=32,
            lora_alpha=64,
            target_modules=[
                # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®çœŸå®æ¨¡å‹ç»“æ„è®¾ç½®target_modules
                "to_q", "to_k", "to_v", "out_proj",      # æ³¨æ„åŠ›å±‚
                "fc1", "fc2",                             # MLPå±‚
                "final_layer.linear",                     # è¾“å‡ºå±‚
            ],
            lora_dropout=0.1,
            bias="none",
        )
        
        pipeline.model = get_peft_model(pipeline.model, lora_config)
    
    # ğŸ”§ å…³é”®ï¼šæŒ‰ç…§SD3æ¨¡å¼ï¼Œå…ˆè·å–æ¨¡å‹å¼•ç”¨
    model = pipeline.model
    
    # ğŸ”§ å…³é”®ï¼šè·å–trainableå‚æ•°ï¼ˆSD3æ–¹å¼ï¼‰
    trainable_params = list(filter(lambda p: p.requires_grad, model.parameters()))
    
    # âœ¨ æ–°å¢ï¼šSD3é£æ ¼çš„ä¼˜åŒ–å™¨åˆå§‹åŒ–
    if config.train.use_8bit_adam:
        try:
            import bitsandbytes as bnb
            logger.info("âœ… ä½¿ç”¨8bit Adamä¼˜åŒ–å™¨")
        except ImportError:
            raise ImportError(
                "Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`"
            )
        optimizer_cls = bnb.optim.AdamW8bit
    else:
        optimizer_cls = torch.optim.AdamW
        logger.info("âœ… ä½¿ç”¨æ ‡å‡†AdamWä¼˜åŒ–å™¨")
    
    # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆSD3é£æ ¼çš„å‚æ•°è®¾ç½®ï¼‰
    optimizer = optimizer_cls(
        trainable_params,
        lr=config.train.learning_rate,
        betas=(config.train.adam_beta1, config.train.adam_beta2),
        weight_decay=config.train.adam_weight_decay,
        eps=config.train.adam_epsilon,
    )
    
    # ğŸ”§ å…³é”®ï¼šæœ€åprepareï¼ˆSD3æ–¹å¼ï¼‰
    model, optimizer = accelerator.prepare(model, optimizer)
    
    # ğŸ”§ å…³é”®ï¼šè®©pipelineä½¿ç”¨preparedçš„æ¨¡å‹
    pipeline.model = model
    
    # âœ¨ æ–°å¢ï¼šSD3é£æ ¼çš„autocastç­–ç•¥ - æ ¹æ®LoRAä½¿ç”¨æƒ…å†µæ™ºèƒ½é€‰æ‹©
    import contextlib
    if config.use_lora:
        autocast = contextlib.nullcontext  # LoRAè®­ç»ƒæ—¶ä¸ä½¿ç”¨autocastèŠ‚çœå†…å­˜
        logger.info("âœ… LoRAæ¨¡å¼ï¼šç¦ç”¨autocastä»¥èŠ‚çœå†…å­˜")
    else:
        autocast = accelerator.autocast  # å…¨å‚æ•°è®­ç»ƒæ—¶ä½¿ç”¨autocastæå‡æ€§èƒ½
        logger.info("âœ… å…¨å‚æ•°æ¨¡å¼ï¼šå¯ç”¨autocastæå‡æ€§èƒ½")
    
    # è®¾ç½®EMAï¼ˆä»¿ç…§SD3ï¼‰
    ema = None
    if config.train.ema:
        ema = EMAModuleWrapper(
            trainable_params,
            decay=config.train.ema_decay,
            device=accelerator.device
        )
        logger.info("âœ… EMAå·²å¯ç”¨")
    
    # ğŸš€ åˆå§‹åŒ–å¥–åŠ±å‡½æ•°ï¼ˆå†…è”ï¼Œæ— trainerï¼‰
    reward_config = {"geometric_quality": 1.0, "uni3d": 0.0}  # ğŸš€ æ˜¾å­˜ä¼˜åŒ–ï¼šç¦ç”¨Uni3DèŠ‚çœå¤§é‡æ˜¾å­˜
    reward_fn = multi_mesh_score(accelerator.device, reward_config)
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥åŠ è½½æ•°æ®é›†
    logger.info(f"Loading dataset from {config.data_dir}")
    train_dataset = Image3DDataset(config.data_dir, split="train")
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config.sample.input_batch_size,
        shuffle=True,
        collate_fn=Image3DDataset.collate_fn,
        num_workers=0,
    )
    
    # ç»Ÿè®¡è·Ÿè¸ª
    stat_tracker = None
    if config.per_image_stat_tracking:
        stat_tracker = PerImageStatTracker(
            global_std=getattr(config.sample, 'global_std', False)
        )
    
    # Prepare dataloader
    train_dataloader = accelerator.prepare(train_dataloader)
    
    # executor to perform callbacks asynchronously
    executor = futures.ThreadPoolExecutor(max_workers=8)
    
    # è®­ç»ƒå¾ªç¯ï¼ˆç±»ä¼¼SD3æ¶æ„ï¼‰
    global_step = 0
    first_epoch = 0
    
    # ğŸ”§ SD3å¯¹é½ï¼šåˆ›å»ºæ•°æ®è¿­ä»£å™¨
    train_iter = iter(train_dataloader)
    
    # number of timesteps within each trajectory to train on
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæˆ‘ä»¬æœ‰20å¯¹latentsï¼Œæ‰€ä»¥å¯ä»¥è®­ç»ƒ20ä¸ªæ—¶é—´æ­¥
    num_latent_pairs = config.sample.num_steps  # 20å¯¹latents
    num_train_timesteps = min(
        int(num_latent_pairs * config.train.timestep_fraction),
        num_latent_pairs
    )
    
    for epoch in range(first_epoch, config.num_epochs):
        logger.info(f"Starting epoch {epoch}")
        
        #################### SAMPLING ####################
        model.eval()
        samples = []  # ğŸ”§ SD3å¯¹é½ï¼šç›´æ¥ä½¿ç”¨samplesåˆ—è¡¨ï¼Œä¸ç”¨epoch_samples
        
        simple_gpu_log(f"Epoch {epoch} - å¼€å§‹é‡‡æ ·")
        
        for i in tqdm(
            range(config.sample.num_batches_per_epoch),  # ğŸ”§ SD3å¯¹é½ï¼šç›´æ¥éå†batchæ•°é‡
            desc=f"Epoch {epoch}: sampling",
            disable=not accelerator.is_local_main_process,
            position=0,
        ):
            # ğŸ”§ SD3å¯¹é½ï¼šä»train_iterè·å–æ•°æ®ï¼Œå¤„ç†StopIteration
            try:
                image_paths, prompts, metadata = next(train_iter)
            except StopIteration:
                # å¦‚æœæ•°æ®ä¸å¤Ÿï¼Œé‡æ–°å¼€å§‹è¿­ä»£å™¨
                train_iter = iter(train_dataloader)
                image_paths, prompts, metadata = next(train_iter)
            
            # ğŸš€ å†…è”é‡‡æ ·é€»è¾‘ï¼ˆåŸtrainer.sample_meshes_with_rewardsï¼‰
            from PIL import Image
            
            # ğŸ”§ å¤šå€™é€‰ç”Ÿæˆï¼šä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆå¤šä¸ªå€™é€‰mesh
            # all_pil_images = []
            # for img_path in image_paths:
            #     # ä¸ºå½“å‰å›¾åƒç”Ÿæˆ num_meshes_per_image ä¸ªå€™é€‰
            #     candidate_images = [img_path] * config.sample.num_meshes_per_image
            #     pil_candidates = [Image.open(path).convert('RGBA') for path in candidate_images]
            #     all_pil_images.extend(pil_candidates)
            pil_images = [Image.open(path).convert('RGBA') for path in image_paths]
            
            # ç¼–ç å›¾åƒæ¡ä»¶
            cond_inputs = pipeline.prepare_image(pil_images)
            image_cond = pipeline.encode_cond(
                image=cond_inputs.pop('image'),
                additional_cond_inputs=cond_inputs,
                do_classifier_free_guidance=True,
                dual_guidance=False,
            )
            # image_cond.keys() = ['main']
            # image_cond['main'].shape = torch.Size([2, 1370, 1024])
            positive_image_cond = {}
            negative_image_cond = {}
            for key in image_cond.keys():
                batch_size = image_cond[key].shape[0]
                positive_image_cond[key] = image_cond[key][:batch_size//2].repeat_interleave(config.sample.num_meshes_per_image, dim=0)
                negative_image_cond[key] = image_cond[key][batch_size//2:].repeat_interleave(config.sample.num_meshes_per_image, dim=0)

            # è°ƒç”¨pipeline
            with torch.no_grad():
                meshes, all_latents, all_log_probs, all_kl = hunyuan3d_pipeline_with_logprob(
                    pipeline,
                    positive_image_cond=positive_image_cond,
                    negative_image_cond=negative_image_cond,
                    num_inference_steps=config.sample.num_steps,
                    guidance_scale=config.sample.guidance_scale,
                    kl_reward=config.sample.kl_reward,
                    octree_resolution=384,
                    mc_level=0.0,
                    mc_algo=None,
                    box_v=1.01,
                    num_chunks=50000,
                )
            
            # ğŸ”§ SD3å¯¹é½ï¼šå¤„ç†latentsæ•°æ®
            latents = torch.stack(all_latents, dim=1)  # (batch_size, num_steps + 1, ...)
            log_probs = torch.stack(all_log_probs, dim=1)  # (batch_size, num_steps)
            kl = torch.stack(all_kl, dim=1)  # (batch_size, num_steps)
            
            # ğŸ”§ SD3å¯¹é½ï¼štimestepså¤„ç†
            timesteps = pipeline.scheduler.timesteps.repeat(
                len(pil_images), 1
            )  # (batch_size, num_steps)
            
            # è®¡ç®—å¥–åŠ±ï¼ˆå¼‚æ­¥ï¼‰
            rewards = executor.submit(reward_fn, meshes, None, {}, image_paths)
            time.sleep(0)  # yield to make sure reward computation starts
            
            # ğŸ”§ SD3å¯¹é½ï¼šå¤„ç†latentsåˆ‡ç‰‡
            current_latents = latents[:, :-1]  # å‰n-1ä¸ªæ—¶é—´æ­¥
            next_latents = latents[:, 1:]      # ån-1ä¸ªæ—¶é—´æ­¥
            
            # ğŸ”§ SD3å¯¹é½ï¼šç®€åŒ–positive_image_condå¤„ç†
            if isinstance(returned_pos_cond, dict):
                positive_image_cond_tensor = returned_pos_cond['main']
            else:
                positive_image_cond_tensor = returned_pos_cond
            
            samples.append({
                "latents": current_latents,
                "next_latents": next_latents,
                "log_probs": log_probs,
                "kl": kl,
                "rewards": rewards,  # å¼‚æ­¥ç»“æœ
                "timesteps": timesteps,
                "positive_image_cond": positive_image_cond_tensor,
            })
            
        # ğŸ”§ é‡‡æ ·å®Œæˆï¼Œè®°å½•å†…å­˜çŠ¶æ€
        simple_gpu_log(f"Epoch {epoch} - é‡‡æ ·å®Œæˆ")
        
        # # ğŸ”§ SD3å¯¹é½ï¼šæ—©æœŸepochè·³è¿‡æ£€æŸ¥ï¼ˆé‡æ–°å¯ç”¨ä»¥é¿å…é—®é¢˜ï¼‰
        # if epoch < 2:
        #     continue
        # NOTE: æ²¡ä»€ä¹ˆç”¨ï¼Œæ³¨é‡Šæ‰äº†
            
        # ğŸ”§ æ£€æŸ¥samplesæ˜¯å¦ä¸ºç©ºï¼Œé¿å…IndexError
        if not samples:
            logger.warning(f"âš ï¸  Epoch {epoch}: No samples collected, skipping training")
            continue
            
        # ğŸ”§ SD3å¯¹é½ï¼šç­‰å¾…æ‰€æœ‰å¥–åŠ±è®¡ç®—å®Œæˆ
        for sample in tqdm(
            samples,
            desc="Waiting for rewards",
            disable=not accelerator.is_local_main_process,
        ):
            reward_details, reward_metadata = sample["rewards"].result()
            sample["rewards"] = torch.tensor(reward_details['avg'], device=accelerator.device, dtype=torch.float32)
        
        # ğŸ”§ SD3å¯¹é½ï¼šcollate samples into dictï¼ˆå®Œå…¨æŒ‰ç…§SD3æ–¹å¼ï¼‰
        samples = {k: torch.cat([s[k] for s in samples], dim=0) for k in samples[0].keys()}
        
        # ğŸš€ å¤„ç†å¥–åŠ±å’Œadvantagesï¼ˆç±»ä¼¼SD3ï¼‰
        rewards_avg = samples["rewards"]  # ç°åœ¨ç›´æ¥æ˜¯tensor
        kl_tensor = samples["kl"]
        
        # ğŸ”§ SD3å¯¹é½ï¼šKLè°ƒæ•´åçš„å¥–åŠ±ï¼Œä¿æŒSD3çš„ç»“æ„
        samples["rewards"] = {"avg": rewards_avg}  # é‡æ–°åŒ…è£…ä¸ºdictç»“æ„
        samples["rewards"]["ori_avg"] = rewards_avg
        samples["rewards"]["avg"] = rewards_avg.unsqueeze(-1) - config.sample.kl_reward * kl_tensor
        
        # gather rewards across processes
        gathered_rewards = {key: accelerator.gather(value) for key, value in samples["rewards"].items()}
        gathered_rewards_np = {key: value.cpu().numpy() for key, value in gathered_rewards.items()}
        
        # è®¡ç®—advantagesï¼ˆç±»ä¼¼SD3ï¼‰
        if config.per_image_stat_tracking and stat_tracker:
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç®€åŒ–çš„å›¾åƒè·¯å¾„å¤„ç†
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸å†æœ‰imagesä¿¡æ¯ï¼Œæ‰€ä»¥ç®€åŒ–å¤„ç†
            advantages_np = stat_tracker.update(
                list(range(len(gathered_rewards_np["avg"]))),  # ä½¿ç”¨ç´¢å¼•ä»£æ›¿å›¾åƒè·¯å¾„
                gathered_rewards_np["avg"].mean(axis=1)
            )
            advantages = torch.tensor(advantages_np, device=accelerator.device)
        else:
            advantages = gathered_rewards["avg"].mean(axis=1)  # å¹³å‡æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰æ—¶é—´æ­¥
            
            # ğŸ”§ æ ‡å‡†åŒ–advantages
            advantages_std = advantages.std()
            if advantages_std > 1e-8:
                advantages = (advantages - advantages.mean()) / (advantages_std + 1e-4)
            else:
                advantages = advantages - advantages.mean()  # åªåšä¸­å¿ƒåŒ–
                
        # æ‰©å±•advantagesåˆ°æ—¶é—´ç»´åº¦
        num_steps = samples["timesteps"].shape[1]
        advantages = advantages.unsqueeze(1).expand(-1, num_steps)
        samples["advantages"] = advantages
        
        # è¿‡æ»¤æ ·æœ¬ï¼ˆç±»ä¼¼SD3ï¼‰
        valid_mask = (advantages.abs().sum(dim=1) > 1e-6)
        if valid_mask.sum().item() == 0:
            logger.warning("âš ï¸  æ‰€æœ‰æ ·æœ¬éƒ½è¢«è¿‡æ»¤æ‰äº†ï¼ä½¿ç”¨æ‰€æœ‰æ ·æœ¬...")
            valid_mask = torch.ones(len(advantages), dtype=torch.bool, device=advantages.device)
        
        # å®‰å…¨çš„æ ·æœ¬è¿‡æ»¤
        for key in samples.keys():
            if isinstance(samples[key], torch.Tensor):
                if samples[key].shape[0] == valid_mask.shape[0]:
                    samples[key] = samples[key][valid_mask]
            elif isinstance(samples[key], dict):
                for sub_key in samples[key]:
                    if isinstance(samples[key][sub_key], torch.Tensor):
                        if samples[key][sub_key].shape[0] == valid_mask.shape[0]:
                            samples[key][sub_key] = samples[key][sub_key][valid_mask]
                
        logger.info(f"Training on {valid_mask.sum().item()} samples")
                
        # ğŸ”§ æ•°æ®åˆ‡åˆ†ä¸ºè®­ç»ƒbatch size
        total_samples = samples["latents"].shape[0]
        train_batch_size = config.train.batch_size
        
        if total_samples > train_batch_size:
            for key in samples.keys():
                if isinstance(samples[key], torch.Tensor):
                    if samples[key].shape[0] == total_samples:
                        samples[key] = samples[key][:train_batch_size]
                elif isinstance(samples[key], dict):
                    for sub_key in samples[key]:
                        if isinstance(samples[key][sub_key], torch.Tensor):
                            if samples[key][sub_key].shape[0] == total_samples:
                                samples[key][sub_key] = samples[key][sub_key][:train_batch_size]
        
        # log rewards
        accelerator.log(
            {
                "epoch": epoch,
                **{f"reward_{key}": value.mean() for key, value in gathered_rewards_np.items()},
                "kl": samples["kl"].mean().cpu().numpy(),
            },
            step=global_step,
        )
        
        # ğŸ”§ æ•°æ®å¤„ç†å®Œæˆï¼Œè®°å½•å†…å­˜çŠ¶æ€
        simple_gpu_log(f"Epoch {epoch} - æ•°æ®å¤„ç†å®Œæˆ")
        
        # ğŸ”§ SD3å¯¹é½ï¼šæ•°æ®æ¸…ç†å’Œæ ·æœ¬è¿‡æ»¤ï¼ˆå‚è€ƒSD3å®ç°ï¼‰
        if accelerator.is_local_main_process:
            print("advantages: ", samples["advantages"].abs().mean())
            print("kl: ", samples["kl"].mean())
        
        # ğŸ”§ SD3å¯¹é½ï¼šåˆ é™¤è®­ç»ƒä¸éœ€è¦çš„é”®
        del samples["rewards"]
        if "images" in samples:
            del samples["images"]

        # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨å‰é¢è¿‡æ»¤åçš„samples
        num_batches = getattr(config.sample, 'num_batches_per_epoch', 1)
        
        total_batch_size, num_timesteps = samples["timesteps"].shape
        assert num_timesteps == config.sample.num_steps
        
        #################### TRAINING ####################
        # å†…è”è®­ç»ƒé€»è¾‘ - å®Œå…¨å¯¹é½SD3æ¶æ„
        for inner_epoch in range(config.train.num_inner_epochs):
            # ğŸ”§ SD3å¯¹é½ï¼šæ‰¹æ¬¡ç»´åº¦éšæœºåŒ–
            perm = torch.randperm(total_batch_size, device=accelerator.device)
            samples = {k: v[perm] for k, v in samples.items()}
            
            # ğŸ”§ SD3å¯¹é½ï¼šæ—¶é—´ç»´åº¦éšæœºåŒ–ï¼ˆæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹ï¼‰
            if getattr(config.train, 'shuffle_timesteps', False):  # æ³¨æ„é»˜è®¤ä¸ºFalse
                if total_batch_size > 0:  # æ·»åŠ è¾¹ç•Œæ£€æŸ¥
                    perms = torch.stack([
                        torch.randperm(num_timesteps, device=accelerator.device)
                        for _ in range(total_batch_size)
                    ])
                else:
                    perms = torch.empty(0, num_timesteps, device=accelerator.device, dtype=torch.long)
            else:
                # SD3é»˜è®¤ï¼šä½¿ç”¨é¡ºåºæ—¶é—´æ­¥
                if total_batch_size > 0:  # æ·»åŠ è¾¹ç•Œæ£€æŸ¥
                    perms = torch.stack([
                        torch.arange(num_timesteps, device=accelerator.device)
                        for _ in range(total_batch_size)
                    ])
                else:
                    perms = torch.empty(0, num_timesteps, device=accelerator.device, dtype=torch.long)
            
            # å¯¹æ—¶é—´ç›¸å…³çš„é”®è¿›è¡Œé‡æ’
            for key in ["timesteps", "latents", "next_latents", "log_probs", "advantages"]:
                if key in samples:
                    samples[key] = samples[key][
                        torch.arange(total_batch_size, device=accelerator.device)[:, None],
                        perms,
                    ]
            
            # ğŸ”§ SD3å¯¹é½ï¼šé‡æ–°æ‰¹å¤„ç†
            samples_batched = {
                k: v.reshape(-1, total_batch_size // num_batches, *v.shape[1:])
                for k, v in samples.items()
            }
            
            # è½¬æ¢ä¸ºlist of dictsæ ¼å¼ï¼ˆSD3é£æ ¼ï¼‰
            samples_batched = [
                dict(zip(samples_batched, x)) for x in zip(*samples_batched.values())
            ]
            
            # ğŸ”§ SD3å¯¹é½ï¼šåŒé‡å¾ªç¯è®­ç»ƒç»“æ„
            model.train()
            info = defaultdict(list)
            
            for i, sample in tqdm(
                list(enumerate(samples_batched)),
                desc=f"Epoch {epoch}.{inner_epoch}: training",
                position=0,
                disable=not accelerator.is_local_main_process,
            ):
                # è®­ç»ƒæ¯ä¸ªæ—¶é—´æ­¥ï¼ˆSD3é£æ ¼ï¼‰
                train_timesteps = [step_index for step_index in range(num_train_timesteps)]
                for j in tqdm(
                    train_timesteps,
                    desc="Timestep",
                    position=1,
                    leave=False,
                    disable=not accelerator.is_local_main_process,
                ):
                    # ğŸ”§ SD3å¯¹é½ï¼šæ¢¯åº¦ç´¯ç§¯åŒ…è£…å™¨
                    with accelerator.accumulate(model):
                        with autocast():
                            # è®¡ç®—logæ¦‚ç‡
                            prev_sample, log_prob, prev_sample_mean, std_dev = compute_log_prob_3d(
                                pipeline, sample, j, config
                            )
                            
                            # å‚è€ƒlogæ¦‚ç‡ï¼ˆKLæ­£åˆ™åŒ–ï¼‰
                            if getattr(config.train, 'beta', 0) > 0:
                                with torch.no_grad():
                                    # ğŸ”§ SD3é£æ ¼ï¼šå®‰å…¨è®¿é—®DDPåŒ…è£…åçš„æ¨¡å‹
                                    model_for_adapter = model.module if hasattr(model, 'module') else model
                                    with model_for_adapter.disable_adapter():
                                        _, log_prob_ref, prev_sample_mean_ref, std_dev_ref = compute_log_prob_3d(
                                            pipeline, sample, j, config
                                        )
                        
                        # ğŸ”§ SD3å¯¹é½ï¼šGRPOæŸå¤±è®¡ç®—
                        advantages = torch.clamp(
                            sample["advantages"][:, j],
                            -config.train.adv_clip_max,
                            config.train.adv_clip_max,
                        )
                        
                        ratio = torch.exp(log_prob - sample["log_probs"][:, j])
                        unclipped_loss = -advantages * ratio
                        clipped_loss = -advantages * torch.clamp(
                            ratio,
                            1.0 - config.train.clip_range,
                            1.0 + config.train.clip_range,
                        )
                        policy_loss = torch.mean(torch.maximum(unclipped_loss, clipped_loss))
                        
                        # KLæŸå¤±ï¼ˆSD3é£æ ¼ï¼‰
                        if getattr(config.train, 'beta', 0) > 0:
                            kl_loss = ((prev_sample_mean - prev_sample_mean_ref) ** 2).mean(dim=tuple(range(1, prev_sample_mean.ndim))) / (2 * std_dev ** 2)
                            kl_loss = torch.mean(kl_loss)
                            loss = policy_loss + config.train.beta * kl_loss
                        else:
                            loss = policy_loss
                        
                        # ğŸ”§ SD3å¯¹é½ï¼šè®°å½•ç»Ÿè®¡ä¿¡æ¯
                        info["approx_kl"].append(
                            0.5 * torch.mean((log_prob - sample["log_probs"][:, j]) ** 2)
                        )
                        info["clipfrac"].append(
                            torch.mean(
                                (torch.abs(ratio - 1.0) > config.train.clip_range).float()
                            )
                        )
                        info["policy_loss"].append(policy_loss)
                        if getattr(config.train, 'beta', 0) > 0:
                            info["kl_loss"].append(kl_loss)
                        info["loss"].append(loss)
                        
                        # ğŸ”§ SD3å¯¹é½ï¼šåå‘ä¼ æ’­å’Œä¼˜åŒ–
                        accelerator.backward(loss)
                        if accelerator.sync_gradients:
                            accelerator.clip_grad_norm_(
                                model.parameters(), config.train.max_grad_norm
                            )
                        optimizer.step()
                        optimizer.zero_grad()
                    
                    # ğŸ”§ SD3å¯¹é½ï¼šè®°å½•è®­ç»ƒä¿¡æ¯å’Œæ›´æ–°å…¨å±€æ­¥æ•°
                    if accelerator.sync_gradients:
                        # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ï¼ˆSD3é£æ ¼ï¼‰
                        step_info = {k: torch.tensor(v).mean().item() for k, v in info.items()}
                        step_info.update({"epoch": epoch, "inner_epoch": inner_epoch})
                        accelerator.log(step_info, step=global_step)
                        global_step += 1
                        
                        # æ¸…ç©ºç»Ÿè®¡ä¿¡æ¯
                        info = defaultdict(list)
                        
                        # ğŸ”§ SD3å¯¹é½ï¼šEMAæ›´æ–°
                    if ema is not None:
                            ema.step(model.parameters())
            
            # è®°å½•epochç»Ÿè®¡ä¿¡æ¯
            logger.info(f"Epoch {epoch}.{inner_epoch} completed")
        
        # ğŸ”§ è®­ç»ƒå®Œæˆï¼Œè®°å½•å†…å­˜çŠ¶æ€
        simple_gpu_log(f"Epoch {epoch} - è®­ç»ƒå®Œæˆ")
        
        # ğŸ”§ SD3å¯¹é½ï¼šå‘¨æœŸæ€§ä¿å­˜æ£€æŸ¥ç‚¹
        if accelerator.is_main_process and (epoch + 1) % getattr(config, 'save_freq', 10) == 0:
            save_ckpt_hunyuan3d(
                model, 
                ema,
                optimizer, 
                epoch, 
                global_step, 
                config.save_dir,
                accelerator
            )
            logger.info(f"Checkpoint saved at epoch {epoch}")
        
        simple_gpu_log(f"Epoch {epoch} - å®Œæˆ")

if __name__ == "__main__":
    app.run(main) 