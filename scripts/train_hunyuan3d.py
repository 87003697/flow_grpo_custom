#!/usr/bin/env python3
"""
Train Hunyuan3D with GRPO - ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®ç®¡ç†ç³»ç»Ÿ

ä½¿ç”¨ä¸SD3ç›¸åŒçš„é…ç½®ç®¡ç†æ–¹å¼ï¼š
- absl.flags ç”¨äºå‘½ä»¤è¡Œå‚æ•°
- ml_collections ç”¨äºå¤æ‚é…ç½®ç»“æ„
- ä¸ train_sd3.py å®Œå…¨ä¸€è‡´çš„æ¥å£
"""

# ğŸ”§ åˆ é™¤RMSNormè¡¥ä¸ï¼šPyTorch 2.6.0+ åŸç”Ÿæ”¯æŒRMSNormï¼Œæ— éœ€è¡¥ä¸
# ğŸ”§ åº”ç”¨RMSNormå…¼å®¹æ€§è¡¥ä¸ï¼ˆä»¿ç…§å®˜æ–¹ä»£ç ï¼‰
import sys
import os
# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# from pytorch_rmsnorm_patch import apply_rmsnorm_patch
# apply_rmsnorm_patch()

import math
import random
import time
import logging
from functools import partial
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict
from concurrent import futures

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration, set_seed
from accelerate.logging import get_logger

# ğŸ”§ ç»Ÿä¸€é…ç½®ç®¡ç† - ä¸SD3ä¿æŒä¸€è‡´
import ml_collections
from absl import app
from absl import flags
from ml_collections import config_flags

# ğŸ”§ å¯¼å…¥ç»Ÿä¸€çš„é…ç½®æ–‡ä»¶
_CONFIG = config_flags.DEFINE_config_file("config")

# ğŸ”§ ä¸SD3ä¿æŒä¸€è‡´çš„è¿›åº¦æ¡é…ç½®
tqdm = partial(tqdm, dynamic_ncols=True)

# æ•°æ®å’Œæ¨¡å‹ç›¸å…³å¯¼å…¥
# from datasets.image_datasets import ImageDataset  # ğŸ”§ æš‚æ—¶ç§»é™¤
from generators.hunyuan3d.pipeline import Hunyuan3DPipeline
from reward_models.rewards_mesh import multi_mesh_score
from reward_models.uni3d_scorer.uni3d_scorer import Uni3DScorer
from flow_grpo.trainer_3d import Hunyuan3DGRPOTrainer
from flow_grpo.ema import EMAModuleWrapper
from flow_grpo.stat_tracking import PerImageStatTracker  # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨ PerImageStatTracker

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
logger = get_logger(__name__)

# ğŸ”§ æ·»åŠ GPUè®¡æ—¶å’Œç›‘æ§åŠŸèƒ½
import subprocess
from contextlib import contextmanager
@contextmanager
def gpu_timer(name):
    """ç»¼åˆç›‘æ§ï¼šè€—æ—¶ + GPUæ˜¾å­˜ + GPUåˆ©ç”¨ç‡"""
    
    # å¼€å§‹å‰çŠ¶æ€
    start_time = time.time()
    start_memory = torch.cuda.memory_allocated() / 1024**3  # GB
    start_reserved = torch.cuda.memory_reserved() / 1024**3  # GB
    
    print(f"ğŸ• å¼€å§‹: {name}")
    print(f"  ğŸ“Š åˆå§‹æ˜¾å­˜: {start_memory:.2f}GB (å·²åˆ†é…) / {start_reserved:.2f}GB (å·²ä¿ç•™)")
    
    # è·å–GPUåˆ©ç”¨ç‡
    def get_gpu_utilization():
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            return int(result.stdout.strip().split('\n')[0])
        except:
            return 0
    
    start_util = get_gpu_utilization()
    print(f"  âš¡ åˆå§‹GPUåˆ©ç”¨ç‡: {start_util}%")
    
    try:
        yield
    finally:
        end_time = time.time()
        end_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        end_reserved = torch.cuda.memory_reserved() / 1024**3  # GB
        end_util = get_gpu_utilization()
        
        # è®¡ç®—å¹³å‡GPUåˆ©ç”¨ç‡
        avg_util = (start_util + end_util) / 2
        
        print(f"âœ… å®Œæˆ: {name}")
        print(f"  â±ï¸  è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"  ğŸ“Š ç»“æŸæ˜¾å­˜: {end_memory:.2f}GB (å·²åˆ†é…) / {end_reserved:.2f}GB (å·²ä¿ç•™)")
        print(f"  ğŸ“ˆ æ˜¾å­˜å˜åŒ–: {end_memory - start_memory:+.2f}GB (å·²åˆ†é…) / {end_reserved - start_reserved:+.2f}GB (å·²ä¿ç•™)")
        print(f"  âš¡ ç»“æŸGPUåˆ©ç”¨ç‡: {end_util}%")
        print(f"  ğŸ”¥ å¹³å‡GPUåˆ©ç”¨ç‡: {avg_util:.1f}%")
        print("")

# HuggingFace imports
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed

logger = logging.getLogger(__name__)


class Image3DDataset(Dataset):
    """Dataset for image-to-3D generation tasks."""
    
    def __init__(self, image_dir: str, prompts_file: Optional[str] = None, split: str = "train", image_files: Optional[List[str]] = None):
        """
        Initialize dataset.
        
        Args:
            image_dir: Directory containing input images
            prompts_file: Optional file containing text prompts (one per line)
            split: Dataset split ("train" or "test")
            image_files: Optional list of image file names to use directly
        """
        self.image_dir = Path(image_dir)
        self.split = split
        
        # Use provided image_files if available, otherwise find all
        if image_files:
            self.image_paths = [self.image_dir / f for f in image_files]
        else:
            image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
            self.image_paths = [
                p for p in self.image_dir.rglob("*") 
                if p.suffix.lower() in image_extensions
            ]
        
        # Load prompts
        if prompts_file and os.path.exists(prompts_file):
            with open(prompts_file, 'r') as f:
                self.prompts = [line.strip() for line in f if line.strip()]
        else:
            self.prompts = []
        
        logger.info(f"Loaded {len(self.image_paths)} images and {len(self.prompts)} prompts for {split}")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        """Get a sample from the dataset."""
        image_path = self.image_paths[idx]
        prompt = self.get_prompt(image_path)
        
        # Create metadata
        metadata = {
            "image_path": str(image_path),
            "prompt": prompt,
            "split": self.split
        }
        
        return str(image_path), prompt, metadata
    
    @staticmethod
    def collate_fn(examples):
        """Collate function for DataLoader."""
        image_paths = [example[0] for example in examples]
        prompts = [example[1] for example in examples]
        metadata = [example[2] for example in examples]
        
        return image_paths, prompts, metadata

    def get_prompt(self, image_path: Path) -> str:
        """Generate a prompt for the given image."""
        idx = self.image_paths.index(image_path)
        if idx < len(self.prompts):
            return self.prompts[idx]
        else:
            # Generate prompt from filename
            filename = image_path.stem
            # Simple filename-to-prompt conversion
            if filename.isdigit():
                # For numbered files like 1.png, 2.png
                return f"a 3D model, high quality"
            else:
                # For descriptive filenames like walking_cat.png
                # Convert underscores to spaces and clean up
                prompt = filename.replace('_', ' ').replace('-', ' ')
                return f"a 3D model of {prompt}, high quality"


# ğŸ”§ ç§»é™¤å†…ç½®é…ç½®å‡½æ•°ï¼šæ”¹ç”¨å¤–éƒ¨é…ç½®æ–‡ä»¶ï¼ˆä¸SD3ä¿æŒä¸€è‡´ï¼‰
# def create_config():
#     """Create default configuration."""
#     from types import SimpleNamespace
#     
#     config = SimpleNamespace()
#     
#     # Basic settings
#     config.data_dir = "data/3d_training"
#     config.save_dir = "checkpoints/hunyuan3d_grpo"
#     config.resume_from = None
#     config.num_epochs = 100
#     config.mixed_precision = "fp16"
#     config.seed = 42
#     config.use_lora = False
#     config.eval_freq = 10
#     config.save_freq = 10
#     config.per_prompt_stat_tracking = True
#     config.deterministic = False  # ğŸ”§ é»˜è®¤ä½¿ç”¨SDEæ¨¡å¼
#     
#     # Sample configuration
#     config.sample = SimpleNamespace()
#     config.sample.input_batch_size = 2           # ğŸ”§ æ–°å¢ï¼šæ¯æ¬¡å¤„ç†å¤šå°‘å¼ ä¸åŒå›¾åƒ
#     config.sample.num_meshes_per_image = 2       # ğŸ”§ æ–°å¢ï¼šæ¯å¼ å›¾åƒç”Ÿæˆå¤šå°‘ä¸ªmeshå€™é€‰
#     config.sample.num_batches_per_epoch = 2      # æ¯ä¸ªepoché‡‡æ ·å¤šå°‘æ¬¡
#     config.sample.num_steps = 20                 # æ‰©æ•£æ­¥æ•°
#     config.sample.guidance_scale = 5.0
#     config.sample.kl_reward = 0.1
#     config.sample.test_batch_size = 4
#     config.sample.global_std = 0.5
#     
#     # Training config
#     config.train = SimpleNamespace()
#     config.train.batch_size = 2                  # ğŸ”§ ä¿®æ”¹ï¼šå‡å°‘åˆ°2é¿å…CUDAé”™è¯¯
#     config.train.gradient_accumulation_steps = 2
#     config.train.num_inner_epochs = 1
#     config.train.learning_rate = 1e-5
#     config.train.beta = 0.01  # KL coefficient
#     config.train.clip_range = 0.2
#     config.train.adv_clip_max = 5.0
#     config.train.max_grad_norm = 1.0
#     config.train.cfg = False  # ğŸ”§ ä¿®å¤ï¼šç¦ç”¨è®­ç»ƒæ—¶çš„CFGï¼Œå› ä¸ºé‡‡æ ·æ—¶å·²ç»ç”Ÿæˆäº†CFGæ ¼å¼çš„æ¡ä»¶
#     config.train.ema = True
#     config.train.ema_decay = 0.999
#     
#     return config


def unwrap_model(model, accelerator):
    """Unwrap model from accelerator."""
    model = accelerator.unwrap_model(model)
    return model._orig_mod if hasattr(model, "_orig_mod") else model


def save_checkpoint(save_dir, pipeline, global_step, accelerator, ema, config):
    """Save training checkpoint."""
    save_path = os.path.join(save_dir, f"checkpoint_{global_step}")
    
    # Save accelerator state
    accelerator.save_state(save_path)
    
    # Save additional info
    save_info = {
        "global_step": global_step,
        "config": config,
    }
    
    if ema is not None:
        save_info["ema"] = ema.state_dict()
    
    torch.save(save_info, os.path.join(save_path, "training_info.pt"))
    logger.info(f"Saved checkpoint to {save_path}")


def evaluate_3d(
    trainer: Hunyuan3DGRPOTrainer,
    test_dataloader: DataLoader,
    config,
    accelerator: Accelerator,
    global_step: int,
    executor: futures.ThreadPoolExecutor,
):
    """Evaluate 3D generation quality."""
    logger.info("Starting 3D evaluation...")
    
    trainer.pipeline.model.eval()  # ä½¿ç”¨trainer.pipeline.modelè€Œä¸æ˜¯trainer.pipeline.pipeline
    
    eval_results = []
    eval_meshes = []
    eval_rewards = []
    
    with torch.no_grad():
        for batch_idx, (image_paths, prompts, metadata) in enumerate(test_dataloader):
            if batch_idx >= 3:  # Limit evaluation batches
                break
            
            # Generate meshes
            results = trainer.sample_meshes_with_rewards(
                images=image_paths,
                input_batch_size=len(image_paths),  # ğŸ”§ é€‚é…è¯„ä¼°æ¨¡å¼
                num_meshes_per_image=1,  # ğŸ”§ è¯„ä¼°æ—¶æ¯ä¸ªå›¾åƒåªç”Ÿæˆä¸€ä¸ªmesh
                num_inference_steps=config.sample.num_steps,
                guidance_scale=config.sample.guidance_scale,
                deterministic=True,  # Use deterministic for evaluation
                kl_reward=0.0,  # No KL reward during evaluation
                # ğŸ”§ æ–°å¢ï¼šä¼ é€’ mesh é…ç½®å‚æ•°
                octree_resolution=config.mesh.octree_resolution,
                mc_level=config.mesh.mc_level,
                mc_algo=config.mesh.mc_algo,
                box_v=config.mesh.box_v,
                num_chunks=config.mesh.num_chunks,
                executor=executor,
            )
            
            # Wait for rewards
            if hasattr(results["rewards"], 'result'):
                rewards, reward_metadata = results["rewards"].result()
            else:
                rewards = results["rewards"]
                reward_metadata = {}
            
            eval_meshes.extend(results["meshes"])
            eval_rewards.append(rewards)
            
            # Store results
            for i, image_path in enumerate(image_paths):
                eval_results.append({
                    "image_path": image_path,
                    "geometric_score": rewards["geometric"][i] if "geometric" in rewards else 0.0,
                    "semantic_score": rewards["uni3d"][i] if "uni3d" in rewards else 0.0,
                    "avg_score": rewards["avg"][i],
                })
    
    # Aggregate results
    if eval_rewards:
        all_geometric = np.concatenate([r.get("geometric", [0.0] * len(r["avg"])) for r in eval_rewards])
        all_semantic = np.concatenate([r.get("uni3d", [0.0] * len(r["avg"])) for r in eval_rewards])
        all_avg = np.concatenate([r["avg"] for r in eval_rewards])
        
        eval_metrics = {
            "eval_geometric_mean": all_geometric.mean(),
            "eval_semantic_mean": all_semantic.mean(),
            "eval_avg_mean": all_avg.mean(),
            "eval_geometric_std": all_geometric.std(),
            "eval_semantic_std": all_semantic.std(),
            "eval_avg_std": all_avg.std(),
            "eval_num_samples": len(all_avg),
        }
        
        # Log to accelerator
        accelerator.log(eval_metrics, step=global_step)
        
        logger.info(f"Evaluation results: avg_score={all_avg.mean():.3f}, "
                   f"geometric={all_geometric.mean():.3f}, semantic={all_semantic.mean():.3f}")
        
        # Save some example meshes
        if accelerator.is_main_process and eval_meshes:
            eval_dir = os.path.join(config.save_dir, f"eval_{global_step}")
            os.makedirs(eval_dir, exist_ok=True)
            
            # Save a few example meshes
            num_to_save = min(5, len(eval_meshes))
            for i in range(num_to_save):
                mesh_path = os.path.join(eval_dir, f"eval_mesh_{i}.glb")
                eval_meshes[i].export(mesh_path)
            
            logger.info(f"Saved {num_to_save} evaluation meshes to {eval_dir}")


def train_step_with_sub_batching(trainer, all_samples, config, optimizer, accelerator):
    """è®­ç»ƒæ­¥éª¤ï¼Œæ”¯æŒå­æ‰¹æ¬¡å¤„ç†"""
    
    total_batch_size = all_samples["timesteps"].shape[0]
    train_batch_size = config.train.batch_size
    
    # ğŸ”§ éªŒè¯çº¦æŸ
    assert total_batch_size % train_batch_size == 0, \
        f"total_batch_size ({total_batch_size}) must be divisible by train_batch_size ({train_batch_size})"
    
    num_sub_batches = total_batch_size // train_batch_size
    
    train_metrics = {
        "policy_loss": 0.0,
        "approx_kl": 0.0,
        "clipfrac": 0.0,
        "train_batch_size": train_batch_size,
        "num_sub_batches": num_sub_batches,
    }
    
    print(f"ğŸ”§ å­æ‰¹æ¬¡è®­ç»ƒï¼š{total_batch_size} æ ·æœ¬åˆ†ä¸º {num_sub_batches} ä¸ªå­æ‰¹æ¬¡ï¼Œæ¯æ‰¹ {train_batch_size} æ ·æœ¬")
    
    # åˆ†æ‰¹è®­ç»ƒ
    for sub_batch_idx in range(num_sub_batches):
        start_idx = sub_batch_idx * train_batch_size
        end_idx = start_idx + train_batch_size
        
        print(f"  å­æ‰¹æ¬¡ {sub_batch_idx+1}/{num_sub_batches}: æ ·æœ¬ {start_idx}:{end_idx}")
        
        # åˆ‡ç‰‡å­æ‰¹æ¬¡
        sub_batch_samples = {}
        for key, value in all_samples.items():
            if isinstance(value, torch.Tensor):
                sub_batch_samples[key] = value[start_idx:end_idx]
            elif isinstance(value, dict):
                sub_batch_samples[key] = {}
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, torch.Tensor):
                        sub_batch_samples[key][sub_key] = sub_value[start_idx:end_idx]
                    else:
                        sub_batch_samples[key][sub_key] = sub_value
            else:
                sub_batch_samples[key] = value
        
        # è®­ç»ƒå­æ‰¹æ¬¡
        sub_metrics = trainer.train_step(
            samples=sub_batch_samples,
            pipeline=trainer.pipeline.core_pipeline,
            optimizer=optimizer,
            config=config,
            accelerator=accelerator,
        )
        
        # ç´¯ç§¯æŒ‡æ ‡
        for key, value in sub_metrics.items():
            if key in train_metrics:
                train_metrics[key] += value / num_sub_batches
    
    return train_metrics


def main(argv):
    """Main training function."""
    # ğŸ”§ ç»Ÿä¸€é…ç½®ç³»ç»Ÿï¼šä½¿ç”¨ä¸SD3ç›¸åŒçš„é…ç½®æ ‡å¿—
    # åˆ é™¤æœªä½¿ç”¨çš„argvå‚æ•°è­¦å‘Š
    del argv
    
    config = _CONFIG.value
    
    with gpu_timer("ğŸš€ å®Œæ•´è®­ç»ƒåˆå§‹åŒ–"):
        # ğŸ”§ æ·»åŠ deterministicé…ç½®
        if hasattr(config, 'deterministic') and config.deterministic:
            logger.info("ğŸ¯ ä½¿ç”¨ç¡®å®šæ€§æ¨¡å¼ (ODE) è¿›è¡Œrolloutå’Œè®­ç»ƒ")
        else:
            logger.info("ğŸ² ä½¿ç”¨éšæœºæ¨¡å¼ (SDE) è¿›è¡Œrolloutå’Œè®­ç»ƒ")
        
        # Initialize accelerator
        accelerator = Accelerator(
            mixed_precision=config.mixed_precision,
            gradient_accumulation_steps=config.train.gradient_accumulation_steps,
            log_with="wandb" if "WANDB_PROJECT" in os.environ else None,
            project_dir=config.save_dir,
        )
        
        # Setup logging
        logging.basicConfig(
            format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
            datefmt="%m/%d/%Y %H:%M:%S",
            level=logging.INFO,
        )
        logger.info(accelerator.state)
        
        # Set seed
        if hasattr(config, 'seed') and config.seed is not None:
            from accelerate.utils import set_seed
            set_seed(config.seed)
        
        # Create save directory
        os.makedirs(config.save_dir, exist_ok=True)
        
        # ğŸ”§ éªŒè¯çº¦æŸæ¡ä»¶
        total_meshes = config.sample.input_batch_size * config.sample.num_meshes_per_image
        assert config.train.batch_size <= total_meshes, \
            f"train.batch_size ({config.train.batch_size}) must be <= total_meshes ({total_meshes})"
        assert total_meshes % config.train.batch_size == 0, \
            f"total_meshes ({total_meshes}) must be divisible by train.batch_size ({config.train.batch_size})"
        
        print(f"ğŸ”§ Batch sizeé…ç½®:")
        print(f"  input_batch_size: {config.sample.input_batch_size}")
        print(f"  num_meshes_per_image: {config.sample.num_meshes_per_image}")
        print(f"  total_meshes: {total_meshes}")
        print(f"  train.batch_size: {config.train.batch_size}")
        
        # Initialize pipeline and models
        logger.info("Loading Hunyuan3D pipeline...")
        with gpu_timer("Hunyuan3Dæ¨¡å‹åŠ è½½"):
            # ğŸ¯ ä½¿ç”¨åŒ…è£…å™¨ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
            pipeline_wrapper = Hunyuan3DPipeline()
            
            # ğŸ”§ å§‹ç»ˆä½¿ç”¨æ ‡å‡†Volume Decodingï¼ˆç¡®ä¿ç¨³å®šæ€§ï¼‰
            logger.info("ğŸ”§ ä½¿ç”¨æ ‡å‡† Volume Decodingï¼ˆæ¨èç”¨äºç¨³å®šæ€§ï¼‰")
            logger.info("âœ… æ ‡å‡† Volume Decoding å·²å¯ç”¨")
            
            # ç§»åŠ¨æ ¸å¿ƒpipelineåˆ°æŒ‡å®šè®¾å¤‡
            pipeline_wrapper.core_pipeline.to(accelerator.device)
        
        # Initialize reward models
        logger.info("Setting up reward configuration...")
        with gpu_timer("å¥–åŠ±å‡½æ•°åˆå§‹åŒ–"):
            reward_config = {
                "geometric_quality": 0.3,
                "uni3d": 0.7
            }
            
            # Create trainer - æ˜ç¡®ï¼šåªä¼ é€’åŒ…è£…ç±»ï¼Œå¯ç”¨SD3å¼batchå¤„ç†
            trainer = Hunyuan3DGRPOTrainer(
                pipeline=pipeline_wrapper,  # ä¼ é€’åŒ…è£…ç±»ï¼Œä¸æ˜¯å†…éƒ¨pipeline
                reward_config=reward_config,
                device=accelerator.device,
                sample_batch_size=config.sample.input_batch_size,  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ input_batch_size
                train_batch_size=config.train.batch_size,         # ğŸ”§ æ–°å¢ï¼šè®­ç»ƒé˜¶æ®µbatch size
            )
    
    # Create reward function
    # reward_fn = create_3d_reward_function(reward_config, accelerator.device)  # åˆ é™¤é‡å¤åˆ›å»º
    
    # Setup datasets
    logger.info(f"Loading datasets from {config.data_dir}")
    
    # Check if we have train/test structure or just images/ structure
    train_dir = os.path.join(config.data_dir, "train")
    test_dir = os.path.join(config.data_dir, "test")
    images_dir = os.path.join(config.data_dir, "images")
    
    if os.path.exists(train_dir) and os.path.exists(test_dir):
        # Standard train/test structure
        train_dataset = Image3DDataset(
            image_dir=train_dir,
            prompts_file=os.path.join(config.data_dir, "train_prompts.txt"),
            split="train"
        )
        test_dataset = Image3DDataset(
            image_dir=test_dir, 
            prompts_file=os.path.join(config.data_dir, "test_prompts.txt"),
            split="test"
        )
    elif os.path.exists(images_dir):
        # Single images/ folder - split it for train/test
        logger.info("Using images/ folder, splitting for train/test")
        
        # Get all image files
        image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        image_files.sort()  # For reproducible splits
        
        # Split 80/20 for train/test
        split_idx = int(len(image_files) * 0.8)
        train_files = image_files[:split_idx]
        test_files = image_files[split_idx:]
        
        logger.info(f"Split {len(image_files)} images: {len(train_files)} train, {len(test_files)} test")
        
        # Create datasets with image file lists
        train_dataset = Image3DDataset(
            image_dir=images_dir,
            prompts_file=None,  # Will generate prompts from filenames
            split="train",
            image_files=train_files
        )
        test_dataset = Image3DDataset(
            image_dir=images_dir,
            prompts_file=None,
            split="test", 
            image_files=test_files
        )
    else:
        raise FileNotFoundError(f"Neither train/test directories nor images/ directory found in {config.data_dir}")
    
    # Create data loaders
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config.sample.input_batch_size,  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ input_batch_size
        shuffle=True,
        collate_fn=Image3DDataset.collate_fn,
        num_workers=2,
    )
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=config.sample.test_batch_size,
        shuffle=False,
        collate_fn=Image3DDataset.collate_fn,
        num_workers=2,
    )
    
    # Setup model for training - æ˜ç¡®è®¿é—®è·¯å¾„ï¼šé€šè¿‡core_pipeline
    core_pipeline = trainer.pipeline.core_pipeline  # è·å–æ ¸å¿ƒpipeline
    model = core_pipeline.model          # æ ¸å¿ƒæ‰©æ•£æ¨¡å‹
    vae = core_pipeline.vae              # VAEç¼–ç å™¨
    conditioner = core_pipeline.conditioner  # æ¡ä»¶ç¼–ç å™¨
    
    if config.use_lora:
        # Add LoRA adapters
        from peft import LoraConfig, get_peft_model
        # ğŸ”§ Hunyuan3DDiT çš„æ­£ç¡® LoRA é…ç½®ï¼ˆåŸºäºçœŸå®æ¶æ„åˆ†æï¼‰
        lora_config = LoraConfig(
            r=32,  # å¢åŠ rankä»¥è·å¾—æ›´å¥½æ•ˆæœ
            lora_alpha=64,  # å¢åŠ alpha scaling
            target_modules=[
                # DoubleStreamBlock - å›¾åƒæµæ³¨æ„åŠ›å±‚
                "img_attn.qkv", "img_attn.proj",
                # DoubleStreamBlock - å›¾åƒæ¡ä»¶æµæ³¨æ„åŠ›å±‚ï¼ˆè™½ç„¶å«txtï¼Œä½†å¤„ç†çš„æ˜¯å›¾åƒæ¡ä»¶ï¼‰
                "txt_attn.qkv", "txt_attn.proj", 
                # DoubleStreamBlock - MLP å±‚
                "img_mlp.0", "img_mlp.2",
                "txt_mlp.0", "txt_mlp.2",
                # SingleStreamBlock - èåˆå±‚
                "linear1", "linear2",
                # å…³é”®è¾“å…¥/è¾“å‡ºå±‚
                "latent_in", "cond_in",  # è¾“å…¥å±‚
                "final_layer.linear"     # æœ€ç»ˆè¾“å‡ºå±‚
            ],
            lora_dropout=0.1,
            bias="none",
        )
        model = get_peft_model(model, lora_config)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°† LoRA æ¨¡å‹è®¾ç½®å› pipelineï¼Œç¡®ä¿ trainer å¯ä»¥è®¿é—® disable_adapter()
        core_pipeline.model = model
        
        trainable_params = [p for p in model.parameters() if p.requires_grad]
    else:
        trainable_params = model.parameters()
    
    # Move models to device - æ³¨æ„ï¼šVAEå’Œconditionerå·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šäº†
    model = accelerator.prepare(model)
    # vae.to(accelerator.device, dtype=torch.float32)  # åˆ é™¤è¿™è¡Œï¼ŒVAEå·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    # conditioner.to(accelerator.device)  # åˆ é™¤è¿™è¡Œï¼Œconditionerå·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    
    # Setup optimizer
    optimizer = torch.optim.AdamW(
        trainable_params,
        lr=config.train.learning_rate,
        betas=(0.9, 0.999),
        weight_decay=0.01,
        eps=1e-8,
    )
    
    # Setup EMA
    ema = None
    if config.train.ema:
        ema = EMAModuleWrapper(
            trainable_params,
            decay=config.train.ema_decay,
            device=accelerator.device,
        )
    
    # Setup stat tracking
    stat_tracker = None
    if config.per_image_stat_tracking:  # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨ per_image_stat_tracking
        stat_tracker = PerImageStatTracker(config.sample.global_std)
    
    # Prepare for training
    model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, test_dataloader
    )
    
    # Thread executor for async operations
    executor = futures.ThreadPoolExecutor(max_workers=4)
    
    # Training info
    samples_per_epoch = len(train_dataloader) * config.sample.input_batch_size
    total_train_batch_size = (
        config.train.batch_size * 
        accelerator.num_processes * 
        config.train.gradient_accumulation_steps
    )
    
    logger.info("***** Running 3D GRPO Training *****")
    logger.info(f"  Num training samples = {len(train_dataset)}")
    logger.info(f"  Num test samples = {len(test_dataset)}")
    logger.info(f"  Num Epochs = {config.num_epochs}")
    logger.info(f"  Sample batch size per device = {config.sample.input_batch_size}")
    logger.info(f"  Train batch size per device = {config.train.batch_size}")
    logger.info(f"  Gradient Accumulation steps = {config.train.gradient_accumulation_steps}")
    logger.info(f"  Total train batch size = {total_train_batch_size}")
    
    # Initialize training
    first_epoch = 0
    global_step = 0
    
    if config.resume_from:
        logger.info(f"Resuming from {config.resume_from}")
        accelerator.load_state(config.resume_from)
        first_epoch = int(config.resume_from.split("_")[-1]) + 1
    
    # Training loop
    for epoch in range(first_epoch, config.num_epochs):
        logger.info(f"Starting epoch {epoch}")
        
        #################### SAMPLING ####################
        model.eval()  # åªéœ€è¦è®¾ç½®æ ¸å¿ƒæ‰©æ•£æ¨¡å‹ä¸ºevalæ¨¡å¼
        
        epoch_samples = []
        with gpu_timer(f"ğŸ“Š Epoch {epoch} - å®Œæ•´é‡‡æ ·é˜¶æ®µ"):
            for batch_idx, (image_paths, prompts, metadata) in enumerate(tqdm(
                train_dataloader, 
                desc=f"Epoch {epoch}: sampling",
                disable=not accelerator.is_local_main_process
            )):
                if batch_idx >= config.sample.num_batches_per_epoch:
                    break
                
                # Sample meshes with rewards
                with gpu_timer(f"æ ·æœ¬ {batch_idx+1}/{config.sample.num_batches_per_epoch} - é‡‡æ ·+è¯„åˆ†"):
                    results = trainer.sample_meshes_with_rewards(
                        images=image_paths,
                        input_batch_size=config.sample.input_batch_size,        # ğŸ”§ æ–°å¢
                        num_meshes_per_image=config.sample.num_meshes_per_image, # ğŸ”§ æ–°å¢
                        num_inference_steps=config.sample.num_steps,
                        guidance_scale=config.sample.guidance_scale,
                        deterministic=config.deterministic,
                        kl_reward=config.sample.kl_reward,
                        # ğŸ”§ æ–°å¢ï¼šä¼ é€’ mesh é…ç½®å‚æ•°
                        octree_resolution=config.mesh.octree_resolution,
                        mc_level=config.mesh.mc_level,
                        mc_algo=config.mesh.mc_algo,
                        box_v=config.mesh.box_v,
                        num_chunks=config.mesh.num_chunks,
                        executor=executor,
                    )
                
                epoch_samples.append(results)
        
        # Wait for all rewards
        for sample in tqdm(
            epoch_samples,
            desc="Waiting for rewards",
            disable=not accelerator.is_local_main_process,
        ):
            if hasattr(sample["rewards"], 'result'):
                rewards, reward_metadata = sample["rewards"].result()
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ä¸åŒç±»å‹çš„rewardæ•°æ®ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´
                sample["rewards"] = {}
                for key, value in rewards.items():
                    if isinstance(value, (list, tuple)):
                        # åˆ—è¡¨æˆ–å…ƒç»„ï¼Œç›´æ¥è½¬æ¢
                        sample["rewards"][key] = torch.tensor(value, device=accelerator.device, dtype=torch.float32)
                    elif isinstance(value, np.ndarray):
                        # ğŸ”§ å…³é”®ä¿®å¤ï¼šnumpyæ•°ç»„ï¼Œç›´æ¥è½¬æ¢ï¼ˆä¸è¦åµŒå¥—ï¼‰
                        sample["rewards"][key] = torch.tensor(value, device=accelerator.device, dtype=torch.float32)
                    elif isinstance(value, torch.Tensor):
                        # å·²ç»æ˜¯å¼ é‡ï¼Œç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                        sample["rewards"][key] = value.to(device=accelerator.device, dtype=torch.float32)
                    elif isinstance(value, (int, float)):
                        # æ ‡é‡ï¼Œè½¬æ¢ä¸ºå•å…ƒç´ å¼ é‡
                        sample["rewards"][key] = torch.tensor([value], device=accelerator.device, dtype=torch.float32)
                    else:
                        # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢
                        sample["rewards"][key] = torch.tensor(value, device=accelerator.device, dtype=torch.float32)
                    
                    # ğŸ”§ è°ƒè¯•ï¼šæ‰“å°æ¯ä¸ªrewardçš„å½¢çŠ¶
                    print(f"ğŸ” reward {key}: shape={sample['rewards'][key].shape}, dtype={sample['rewards'][key].dtype}, device={sample['rewards'][key].device}")
                
                print(f"ğŸ”§ ä¿®å¤ï¼šrewardså¤„ç†å®Œæˆï¼Œè®¾å¤‡ {accelerator.device}")
        
        # ğŸ”§ è°ƒè¯•ï¼šåœ¨collateä¹‹å‰æ£€æŸ¥æ¯ä¸ªæ ·æœ¬çš„æ•°æ®ç±»å‹
        print(f"ğŸ” æ ·æœ¬æ•°æ®è°ƒè¯• - æ£€æŸ¥æ¯ä¸ªå­—æ®µçš„ç±»å‹:")
        for i, sample in enumerate(epoch_samples):
            print(f"  æ ·æœ¬ {i}:")
            for key, value in sample.items():
                if isinstance(value, torch.Tensor):
                    print(f"    {key}: Tensor, shape={value.shape}, dtype={value.dtype}")
                elif isinstance(value, dict):
                    print(f"    {key}: dict with keys {list(value.keys())}")
                    for sub_key, sub_value in value.items():
                        if isinstance(sub_value, torch.Tensor):
                            print(f"      {sub_key}: Tensor, shape={sub_value.shape}, dtype={sub_value.dtype}")
                        else:
                            print(f"      {sub_key}: {type(sub_value)} = {sub_value}")
                elif isinstance(value, (list, tuple)):
                    print(f"    {key}: {type(value)} with {len(value)} items")
                    if len(value) > 0:
                        print(f"      first item type: {type(value[0])}")
                else:
                    print(f"    {key}: {type(value)} = {value}")
        
        # Collate samples
        all_samples = {
            k: torch.cat([s[k] for s in epoch_samples], dim=0)
            if not isinstance(epoch_samples[0][k], dict)
            else {
                sub_key: torch.cat([s[k][sub_key] for s in epoch_samples], dim=0)
                for sub_key in epoch_samples[0][k]
                if isinstance(epoch_samples[0][k][sub_key], torch.Tensor)  # ğŸ”§ ä¿®å¤ï¼šåªè¿æ¥å¼ é‡
            }
            for k in epoch_samples[0].keys()
            if k not in ["meshes", "images", "prompts", "positive_image_cond", "metadata"]  # ğŸ”§ ä¿®å¤ï¼šè·³è¿‡positive_image_condå’Œmetadata
        }
        
        # ğŸ”§ ä¿®å¤ï¼šå•ç‹¬å¤„ç†positive_image_condï¼Œå› ä¸ºå®ƒæ˜¯å­—å…¸ä¸”åœ¨æ‰€æœ‰æ ·æœ¬ä¸­ç›¸åŒ
        if "positive_image_cond" in epoch_samples[0]:
            all_samples["positive_image_cond"] = epoch_samples[0]["positive_image_cond"]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬çš„positive_image_cond
        
        # ğŸ” Hunyuan3D Train Debug: é‡‡æ ·åçš„æ•°æ®å½¢çŠ¶
        # âš ï¸ é‡è¦å¯¹æ¯”ï¼š
        # SD3: latents (batch_size, num_steps+1, 16, 32, 32)
        # Hunyuan3D: latents (batch_size, num_steps+1, 1024, 64)
        # ç›¸åŒç‚¹ï¼šlog_probs (batch_size, num_steps), kl (batch_size, num_steps), rewards (batch_size,)
        print(f"ğŸ” Hunyuan3D Train Debug - é‡‡æ ·åæ•°æ®:")
        for key, value in all_samples.items():
            if isinstance(value, torch.Tensor):
                if key == "latents":
                    print(f"  {key}.shape: {value.shape} (Hunyuan3D vs SD3)")
                    print(f"    Hunyuan3D: (batch, steps+1, 1024, 64)")
                    print(f"    SD3:       (batch, steps+1, 16, 32, 32)")
                else:
                    print(f"  {key}.shape: {value.shape}")
            elif isinstance(value, dict):
                print(f"  {key}: dict with keys {list(value.keys())}")
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, torch.Tensor):
                        print(f"    {sub_key}.shape: {sub_value.shape}")
        
        # Adjust rewards with KL penalty
        all_samples["rewards"]["ori_avg"] = all_samples["rewards"]["avg"].clone()
        
        # ğŸ”§ ä¿®å¤ï¼šæŒ‰ç…§SD3çš„æ–¹å¼å¤„ç†KL tensor
        rewards_avg = all_samples["rewards"]["avg"]  # shape: (batch_size,)
        kl_tensor = all_samples["kl"]  # shape: (batch_size, num_steps) - å·²ç»é€šè¿‡torch.catåˆå¹¶
        
        # ğŸ”§ è°ƒè¯•ï¼šæ‰“å°tensorå½¢çŠ¶
        print(f"ğŸ” Tensor shapes debug:")
        print(f"  rewards_avg.shape: {rewards_avg.shape}")
        print(f"  kl_tensor.shape: {kl_tensor.shape}")
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿ç»´åº¦åŒ¹é…
        # rewards_avg: (batch_size,) -> (batch_size, 1)
        # kl_tensor: (batch_size, num_steps)
        # ç»“æœ: (batch_size, num_steps)
        all_samples["rewards"]["avg"] = (
            rewards_avg.unsqueeze(-1) -  # (batch_size, 1)
            config.sample.kl_reward * kl_tensor  # (batch_size, num_steps)
        )  # ç»“æœ: (batch_size, num_steps)
        
        # Gather rewards across processes
        gathered_rewards = {
            key: accelerator.gather(value) 
            for key, value in all_samples["rewards"].items()
        }
        # ğŸ”§ ä¼˜åŒ–ï¼šä¿æŒrewardsåœ¨CUDAä¸Šï¼Œåªåœ¨éœ€è¦æ—¥å¿—æ—¶è½¬CPU
        gathered_rewards_for_log = {
            key: value.cpu().numpy() 
            for key, value in gathered_rewards.items()
        }
        
        # Log metrics (ä½¿ç”¨CPUç‰ˆæœ¬)
        accelerator.log({
            "epoch": epoch,
            **{f"reward_{key}": value.mean() for key, value in gathered_rewards_for_log.items()},
            "kl": all_samples["kl"].mean().cpu().numpy(),
        }, step=global_step)
        
        # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥åœ¨CUDAä¸Šè®¡ç®—advantagesï¼Œé¿å…ä¸å¿…è¦çš„è®¾å¤‡è½¬æ¢
        if config.per_image_stat_tracking and stat_tracker:
            # ï¿½ï¿½ ä¿®å¤ï¼šHunyuan3Dä½¿ç”¨å›¾åƒè·¯å¾„è¿›è¡Œç»Ÿè®¡è·Ÿè¸ªï¼Œè€Œä¸æ˜¯æ–‡æœ¬æç¤º
            all_images = []
            for sample in epoch_samples:
                all_images.extend(sample["images"])  # ğŸ”§ å›¾åƒè·¯å¾„åˆ—è¡¨
            
            # ğŸ”§ ä¿®å¤ï¼šåªæœ‰å½“å¤„ç†çš„æ ·æœ¬æ•°ç­‰äºè®­ç»ƒé›†å¤§å°æ—¶æ‰ä½¿ç”¨per-imageè·Ÿè¸ª
            if len(all_images) == len(train_dataset):
                # stat_trackeréœ€è¦CPUæ•°æ®ï¼Œä½†æˆ‘ä»¬ç«‹å³è½¬å›CUDA
                advantages_np = stat_tracker.update(all_images, gathered_rewards['avg'].cpu().numpy())
                # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šåˆ›å»ºtensorï¼Œé¿å…ä¸­é—´è½¬æ¢
                advantages = torch.tensor(advantages_np, device=accelerator.device, dtype=torch.float32)
                print(f"ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨per-image advantagesï¼Œç›´æ¥åœ¨CUDAä¸Šåˆ›å»º")
            else:
                logger.warning(f"Processed {len(all_images)} samples but have {len(train_dataset)} in dataset. Using global advantages.")
                # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥åœ¨CUDAä¸Šè®¡ç®—global advantagesï¼Œæ— éœ€CPUè½¬æ¢
                advantages = gathered_rewards['avg']
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-4)
                print(f"ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨global advantagesï¼Œä¿æŒåœ¨CUDAä¸Šè®¡ç®—")
        else:
            # ğŸ”§ ä¼˜åŒ–ï¼šç›´æ¥åœ¨CUDAä¸Šè®¡ç®—global advantages
            advantages = gathered_rewards['avg']
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-4)
            print(f"ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨global advantagesï¼Œä¿æŒåœ¨CUDAä¸Šè®¡ç®—")
        
        print(f"ğŸ”§ è®¾å¤‡ä¼˜åŒ–ï¼šadvantagesåœ¨è®¾å¤‡ {advantages.device} ä¸Šï¼Œå½¢çŠ¶ {advantages.shape}")
        
        #  ä¿®å¤ï¼šæ­£ç¡®å¤„ç†advantagesçš„ç»´åº¦
        # å…³é”®é—®é¢˜ï¼šadvantagesç°åœ¨æ˜¯(batch_size, num_steps)ï¼Œä½†æˆ‘ä»¬éœ€è¦åœ¨batchç»´åº¦ä¸Šè¿›è¡Œç­›é€‰
        # è§£å†³æ–¹æ¡ˆï¼šè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡advantageï¼Œç”¨äºç­›é€‰æ•´ä¸ªæ ·æœ¬
        print(f"ğŸ” Advantageså¤„ç† - ä¿®å¤å‰:")
        print(f"  advantages.shape: {advantages.shape}")
        print(f"  æœŸæœ›: (batch_size, num_steps) æˆ– (batch_size,)")
        
        if advantages.dim() == 2:
            # å¦‚æœadvantagesæ˜¯2Dçš„ (batch_size, num_steps)ï¼Œè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡advantage
            sample_advantages = advantages.mean(dim=1)  # (batch_size,)
            print(f"  è®¡ç®—æ ·æœ¬å¹³å‡advantages: {sample_advantages.shape}")
        else:
            # å¦‚æœadvantagesæ˜¯1Dçš„ (batch_size,)ï¼Œç›´æ¥ä½¿ç”¨
            sample_advantages = advantages
            print(f"  ç›´æ¥ä½¿ç”¨advantages: {sample_advantages.shape}")
        
        # æŒ‰è¿›ç¨‹åˆ†å‰² - ç°åœ¨åœ¨batchç»´åº¦ä¸Šåˆ†å‰²
        batch_size = sample_advantages.shape[0]
        samples_per_process = batch_size // accelerator.num_processes
        
        # å–å½“å‰è¿›ç¨‹çš„éƒ¨åˆ†
        start_idx = accelerator.process_index * samples_per_process
        end_idx = start_idx + samples_per_process
        if end_idx > batch_size or accelerator.process_index == accelerator.num_processes - 1:
            end_idx = batch_size  # æœ€åä¸€ä¸ªè¿›ç¨‹å¤„ç†å‰©ä½™çš„æ ·æœ¬
        
        print(f"ğŸ” è¿›ç¨‹åˆ†å‰²:")
        print(f"  è¿›ç¨‹ {accelerator.process_index}/{accelerator.num_processes}")
        print(f"  å¤„ç†æ ·æœ¬ {start_idx}:{end_idx} (å…±{batch_size}ä¸ª)")
        
        # ä¸ºæ‰€æœ‰tensoråˆ†é…advantagesï¼Œä¿æŒåŸå§‹å½¢çŠ¶
        if advantages.dim() == 2:
            # å¦‚æœåŸå§‹advantagesæ˜¯2Dçš„ï¼Œä¿æŒ2Då½¢çŠ¶
            # ğŸ”§ ä¼˜åŒ–ï¼šadvantageså·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼Œæ— éœ€.to()æ“ä½œ
            all_samples["advantages"] = advantages[start_idx:end_idx]
            print(f"ğŸ”§ ä¼˜åŒ–ï¼š2D advantagesåˆ‡ç‰‡ï¼Œæ— è®¾å¤‡è½¬æ¢")
        else:
            # å¦‚æœåŸå§‹advantagesæ˜¯1Dçš„ï¼Œä¿æŒ1Då½¢çŠ¶
            # ğŸ”§ ä¼˜åŒ–ï¼šsample_advantageså·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            all_samples["advantages"] = sample_advantages[start_idx:end_idx]
            print(f"ğŸ”§ ä¼˜åŒ–ï¼š1D advantagesåˆ‡ç‰‡ï¼Œæ— è®¾å¤‡è½¬æ¢")
        
        # ğŸ”§ ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§æ£€æŸ¥æ‰€æœ‰tensorçš„è®¾å¤‡ï¼Œå‡å°‘é‡å¤æ£€æŸ¥
        print(f"ğŸ”§ è®¾å¤‡æ£€æŸ¥ï¼šå¼€å§‹ç»Ÿä¸€è®¾å¤‡æ£€æŸ¥...")
        
        # ğŸ”§ ä¼˜åŒ–ï¼šå¼ºåˆ¶è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥ï¼Œç¡®ä¿æ‰€æœ‰tensoréƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        print(f"ğŸ”§ è®¾å¤‡æ£€æŸ¥ï¼šéªŒè¯æ‰€æœ‰tensorè®¾å¤‡ä¸€è‡´æ€§...")
        
        # ğŸ”§ ä¿®å¤ï¼šå¤„ç†cudaå’Œcuda:0çš„è®¾å¤‡è¡¨ç¤ºå·®å¼‚
        def devices_match(tensor_device, target_device):
            """æ£€æŸ¥ä¸¤ä¸ªè®¾å¤‡æ˜¯å¦åŒ¹é…ï¼Œå¤„ç†cudaå’Œcuda:0çš„å·®å¼‚"""
            tensor_str = str(tensor_device)
            target_str = str(target_device)
            
            # å¦‚æœå®Œå…¨ç›¸åŒï¼Œç›´æ¥è¿”å›True
            if tensor_str == target_str:
                return True
            
            # å¤„ç†cudaå’Œcuda:0çš„ç­‰ä»·æ€§
            if (tensor_str == "cuda:0" and target_str == "cuda") or (tensor_str == "cuda" and target_str == "cuda:0"):
                return True
            
            return False
        
        # åŒæ—¶æ›´æ–°æ‰€æœ‰å…¶ä»–tensoråˆ°ç›¸åŒçš„æ ·æœ¬èŒƒå›´
        for key, value in all_samples.items():
            if key != "advantages" and isinstance(value, torch.Tensor):
                all_samples[key] = value[start_idx:end_idx]
                # ğŸ”§ å¼ºåˆ¶æ£€æŸ¥ï¼šç¡®ä¿tensoråœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                assert devices_match(value.device, accelerator.device), f"âŒ {key} åœ¨é”™è¯¯è®¾å¤‡ä¸Š: {value.device}, æœŸæœ›: {accelerator.device}"
            elif key != "advantages" and isinstance(value, dict):
                all_samples[key] = {
                    sub_key: sub_value[start_idx:end_idx] 
                    for sub_key, sub_value in value.items()
                }
                # ğŸ”§ å¼ºåˆ¶æ£€æŸ¥ï¼šç¡®ä¿åµŒå¥—tensoråœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, torch.Tensor):
                        assert devices_match(sub_value.device, accelerator.device), f"âŒ {key}.{sub_key} åœ¨é”™è¯¯è®¾å¤‡ä¸Š: {sub_value.device}, æœŸæœ›: {accelerator.device}"
        
        print(f"âœ… æ‰€æœ‰tensorè®¾å¤‡ä¸€è‡´æ€§éªŒè¯é€šè¿‡: {accelerator.device}")
        
        # Filter out zero-advantage samples - ç°åœ¨åœ¨æ­£ç¡®çš„ç»´åº¦ä¸Šè¿›è¡Œç­›é€‰
        if all_samples["advantages"].dim() == 2:
            # å¦‚æœadvantagesæ˜¯2Dçš„ï¼Œä½¿ç”¨å¹³å‡å€¼æ¥ç­›é€‰
            mask = (all_samples["advantages"].mean(dim=1).abs() > 1e-6)
        else:
            # å¦‚æœadvantagesæ˜¯1Dçš„ï¼Œç›´æ¥ç­›é€‰
            mask = (all_samples["advantages"].abs() > 1e-6)
        
        # ğŸ”§ ä¼˜åŒ–ï¼šmaskå·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼Œæ— éœ€è½¬æ¢
        print(f"ğŸ”§ ä¼˜åŒ–ï¼šmaskåœ¨è®¾å¤‡ {mask.device} ä¸Šï¼Œå½¢çŠ¶ {mask.shape}")
        
        print(f"ğŸ” æ ·æœ¬ç­›é€‰:")
        print(f"  mask.shape: {mask.shape}")
        print(f"  mask.device: {mask.device}")
        print(f"  ç­›é€‰å‰æ ·æœ¬æ•°: {all_samples['advantages'].shape[0]}")
        print(f"  ç­›é€‰åæ ·æœ¬æ•°: {mask.sum().item()}")
        
        # ğŸ”§ ä¼˜åŒ–ï¼šç®€åŒ–è®¾å¤‡æ£€æŸ¥ï¼Œåªåœ¨çœŸæ­£éœ€è¦æ—¶è½¬æ¢
        filtered_samples = {}
        for key, value in all_samples.items():
            if isinstance(value, torch.Tensor):
                # ğŸ”§ ä¼˜åŒ–ï¼šæ‰€æœ‰tensoråº”è¯¥å·²ç»åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼Œç›´æ¥åº”ç”¨mask
                filtered_samples[key] = value[mask]
            elif isinstance(value, dict):
                filtered_samples[key] = {}
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, torch.Tensor):
                        # ğŸ”§ ä¿®å¤ï¼špositive_image_condæ˜¯æŒ‰å›¾åƒæ•°é‡è€Œä¸æ˜¯meshæ•°é‡ï¼Œä¸åº”ç”¨mask
                        if key == "positive_image_cond":
                            filtered_samples[key][sub_key] = sub_value  # ä¸åº”ç”¨mask
                        else:
                            # ğŸ”§ ä¼˜åŒ–ï¼šæ‰€æœ‰åµŒå¥—tensorä¹Ÿåº”è¯¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                            filtered_samples[key][sub_key] = sub_value[mask]
                    else:
                        filtered_samples[key][sub_key] = sub_value
            else:
                filtered_samples[key] = value
        
        all_samples = filtered_samples
        
        logger.info(f"Training on {mask.sum().item()} samples with non-zero advantages")
        
        # ğŸ” ä¿®å¤åçš„tensorå½¢çŠ¶éªŒè¯
        print(f"ğŸ” ä¿®å¤åçš„tensorå½¢çŠ¶éªŒè¯:")
        for key, value in all_samples.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}.shape: {value.shape}")
            elif isinstance(value, dict):
                print(f"  {key}: dict with keys {list(value.keys())}")
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, torch.Tensor):
                        print(f"    {sub_key}.shape: {sub_value.shape}")
        print(f"  æ‰€æœ‰tensorçš„ç¬¬ä¸€ç»´åº”è¯¥ç›¸åŒï¼")
        
        # åœ¨ all_samples å¤„ç†åï¼Œæ·»åŠ SD3å¼çš„æ•°æ®é‡ç»„
        if "latents" in all_samples:
            # ğŸ” SD3å¼æ•°æ®é‡ç»„: å°†latentsåˆ†å‰²ä¸ºcurrentå’ŒnextçŠ¶æ€
            # âš ï¸ é‡è¦ï¼šè™½ç„¶latent shapeä¸åŒï¼Œä½†åˆ†å‰²æ–¹å¼ç›¸åŒ
            # SD3: latents (batch, steps+1, 16, 32, 32) â†’ current/next (batch, steps, 16, 32, 32)
            # Hunyuan3D: latents (batch, steps+1, 1024, 64) â†’ current/next (batch, steps, 1024, 64)
            # é€šç”¨æ–¹å¼: latents[:, :-1] for current, latents[:, 1:] for next
            latents = all_samples["latents"]
            print(f"ğŸ” SD3å¼æ•°æ®é‡ç»„å‰: latents.shape = {latents.shape}")
            print(f"  Hunyuan3D: (batch, steps+1, 1024, 64)")
            print(f"  SD3å¯¹æ¯”:   (batch, steps+1, 16, 32, 32)")

            all_samples["latents"] = latents[:, :-1]  # å½“å‰çŠ¶æ€
            all_samples["next_latents"] = latents[:, 1:]  # ä¸‹ä¸€ä¸ªçŠ¶æ€

            print(f"ğŸ” SD3å¼æ•°æ®é‡ç»„å:")
            print(f"  latents.shape: {all_samples['latents'].shape} (current states)")
            print(f"  next_latents.shape: {all_samples['next_latents'].shape} (next states)")
            print(f"  ä¸¤è€…éƒ½åº”ä¸º: (batch_size, num_steps, ...)")
        
        #################### TRAINING ####################
        # ğŸ”§ GPUå†…å­˜ä¼˜åŒ–ï¼šåœ¨è®­ç»ƒå‰æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        print(f"ğŸ”§ GPUå†…å­˜æ¸…ç†ï¼šè®­ç»ƒå‰é‡Šæ”¾ç¼“å­˜")
        
        for inner_epoch in range(config.train.num_inner_epochs):
            model.train()  # åªéœ€è¦è®¾ç½®æ ¸å¿ƒæ‰©æ•£æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¤„ç†batch sizeä¸ä¸€è‡´é—®é¢˜
            # è·å–ä¸»è¦æ•°æ®çš„batch size
            batch_size = all_samples["timesteps"].shape[0]
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿positive_image_condçš„batch sizeä¸å…¶ä»–æ•°æ®ä¸€è‡´
            if "positive_image_cond" in all_samples and isinstance(all_samples["positive_image_cond"], dict):
                pos_cond = all_samples["positive_image_cond"]
                if "main" in pos_cond and pos_cond["main"].shape[0] != batch_size:
                    print(f"ğŸ”§ ä¿®å¤batch sizeä¸ä¸€è‡´: positive_image_cond.mainä»{pos_cond['main'].shape[0]}æ‰©å±•åˆ°{batch_size}")
                    # é‡å¤æ¡ä»¶ä»¥åŒ¹é…batch size
                    current_size = pos_cond["main"].shape[0]
                    repeat_factor = batch_size // current_size
                    remainder = batch_size % current_size
                    
                    repeated_cond = pos_cond["main"].repeat(repeat_factor, 1, 1)
                    if remainder > 0:
                        repeated_cond = torch.cat([repeated_cond, pos_cond["main"][:remainder]], dim=0)
                    
                    all_samples["positive_image_cond"]["main"] = repeated_cond
                    print(f"ğŸ”§ ä¿®å¤å®Œæˆ: positive_image_cond.main.shape = {all_samples['positive_image_cond']['main'].shape}")
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰rewardsçš„å½¢çŠ¶ä¸€è‡´
            if "rewards" in all_samples and isinstance(all_samples["rewards"], dict):
                for reward_key, reward_value in all_samples["rewards"].items():
                    if isinstance(reward_value, torch.Tensor):
                        if reward_value.ndim == 2 and reward_value.shape[0] == batch_size:
                            # å¦‚æœæ˜¯äºŒç»´ä¸”ç¬¬ä¸€ç»´æ­£ç¡®ï¼Œå–å¹³å‡å€¼è½¬ä¸ºä¸€ç»´
                            if reward_key == "avg":
                                all_samples["rewards"][reward_key] = reward_value.mean(dim=1)
                                print(f"ğŸ”§ ä¿®å¤rewardså½¢çŠ¶: {reward_key} ä» {reward_value.shape} è½¬ä¸º {all_samples['rewards'][reward_key].shape}")
                        elif reward_value.shape[0] != batch_size:
                            print(f"ğŸš¨ è­¦å‘Š: rewards[{reward_key}].shape[0]={reward_value.shape[0]} != batch_size={batch_size}")
            
            # ğŸ”§ éªŒè¯æ‰€æœ‰tensorçš„batch sizeä¸€è‡´æ€§
            print(f"ğŸ” Shuffleå‰æ‰¹æ¬¡å¤§å°éªŒè¯:")
            for k, v in all_samples.items():
                if isinstance(v, torch.Tensor):
                    print(f"  {k}.shape[0]: {v.shape[0]}")
                elif isinstance(v, dict):
                    for sub_k, sub_v in v.items():
                        if isinstance(sub_v, torch.Tensor):
                            print(f"  {k}[{sub_k}].shape[0]: {sub_v.shape[0]}")
            
            # Shuffle samples
            perm = torch.randperm(batch_size, device=accelerator.device)
            print(f"ğŸ”§ ç”Ÿæˆshuffle perm: {perm} (max_index={perm.max()}, batch_size={batch_size})")
            
            shuffled_samples = {}
            for k, v in all_samples.items():
                if isinstance(v, torch.Tensor):
                    shuffled_samples[k] = v[perm]
                elif isinstance(v, dict):
                    shuffled_samples[k] = {}
                    for sub_k, sub_v in v.items():
                        if isinstance(sub_v, torch.Tensor):
                            # ğŸ”§ æ·»åŠ å®‰å…¨æ£€æŸ¥
                            if sub_v.shape[0] != batch_size:
                                print(f"ğŸš¨ é”™è¯¯ï¼š{k}[{sub_k}].shape[0]={sub_v.shape[0]} != batch_size={batch_size}")
                                raise ValueError(f"Tensor {k}[{sub_k}] batch size mismatch")
                            shuffled_samples[k][sub_k] = sub_v[perm]
                        else:
                            shuffled_samples[k][sub_k] = sub_v
                else:
                    shuffled_samples[k] = v
            
            # ğŸ”§ ä½¿ç”¨å­æ‰¹æ¬¡è®­ç»ƒæˆ–ç›´æ¥è®­ç»ƒ
            total_batch_size = shuffled_samples["timesteps"].shape[0]
            if total_batch_size > config.train.batch_size:
                # ä½¿ç”¨å­æ‰¹æ¬¡è®­ç»ƒ
                train_metrics = train_step_with_sub_batching(
                    trainer=trainer,
                    all_samples=shuffled_samples,
                    config=config,
                    optimizer=optimizer,
                    accelerator=accelerator,
                )
            else:
                # ç›´æ¥è®­ç»ƒ
                train_metrics = trainer.train_step(
                    samples=shuffled_samples,
                    pipeline=trainer.pipeline.core_pipeline,
                    optimizer=optimizer,
                    config=config,
                    accelerator=accelerator,
                )
            
            # Log training metrics
            accelerator.log({
                **train_metrics,
                "epoch": epoch,
                "inner_epoch": inner_epoch,
            }, step=global_step)
            
            global_step += 1
            
            # Update EMA
            if ema:
                ema.step(trainable_params, global_step)
        
        # Evaluation
        if epoch % config.eval_freq == 0 and epoch > 0:
            evaluate_3d(
                trainer, test_dataloader, config, accelerator, 
                global_step, executor
            )
        
        # Save checkpoint
        if epoch % config.save_freq == 0 and epoch > 0 and accelerator.is_main_process:
            save_checkpoint(config.save_dir, trainer.pipeline, global_step, accelerator, ema, config)
        
        # Clear stat tracker
        if stat_tracker:
            stat_tracker.clear()
    
    logger.info("Training completed!")
    
    # Final save
    if accelerator.is_main_process:
        save_checkpoint(config.save_dir, trainer.pipeline, global_step, accelerator, ema, config)
    
    # Cleanup
    executor.shutdown(wait=True)


if __name__ == "__main__":
    # ğŸ”§ ç»Ÿä¸€ä¸»å‡½æ•°è°ƒç”¨ï¼šä½¿ç”¨ä¸SD3ç›¸åŒçš„absl.app.run
    app.run(main)
