#!/usr/bin/env python3
"""
Hunyuan3D GRPO Training Script

Based on SD3 architecture with Hunyuan3D-specific modifications.
"""

import sys
import os
import time
import logging
from PIL import Image
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict
from concurrent import futures
import contextlib
import datetime
import tempfile
import random
import numpy as np
import wandb

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, Sampler
import numpy as np
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed, ProjectConfiguration
from accelerate.logging import get_logger

import ml_collections
from absl import app, flags
from ml_collections import config_flags

_CONFIG = config_flags.DEFINE_config_file("config")

from generators.hunyuan3d.pipeline import Hunyuan3DPipeline
from reward_models.rewards_mesh import multi_mesh_score, preload_scorers
from flow_grpo.diffusers_patch.hunyuan3d_pipeline_with_logprob import hunyuan3d_pipeline_with_logprob
from flow_grpo.diffusers_patch.hunyuan3d_sde_with_logprob import hunyuan3d_sde_step_with_logprob
from flow_grpo.ema import EMAModuleWrapper
from flow_grpo.stat_tracking import PerImageStatTracker

logger = get_logger(__name__)

class Image3DDataset(Dataset):
    def __init__(self, image_dir: str, prompts_file: Optional[str] = None, split: str = "train"):
        self.image_dir = Path(image_dir)
        self.prompts_file = prompts_file
        self.split = split
        
        if (self.image_dir / "images").exists():
            self.image_dir = self.image_dir / "images"
        
        self.image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            self.image_files.extend(self.image_dir.glob(ext))
        
        self.image_files = sorted(self.image_files)
        
        if not self.image_files:
            raise ValueError(f"No images found in {self.image_dir}")
        
        logger.info(f"Found {len(self.image_files)} images in {self.image_dir}")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        image_path = str(self.image_files[idx])
        prompt = self.get_prompt(self.image_files[idx])
        
        return {
            "image_path": image_path,
            "prompt": prompt,
            "metadata": {"image_name": self.image_files[idx].name}
        }
    
    @staticmethod
    def collate_fn(examples):
        image_paths = [example["image_path"] for example in examples]
        prompts = [example["prompt"] for example in examples]
        metadata = [example["metadata"] for example in examples]
        return image_paths, prompts, metadata
    
    def get_prompt(self, image_path: Path) -> str:
        return f"Generate a 3D model from this image: {image_path.stem}"

class DistributedImageRepeatSampler(Sampler):
    """
    Hunyuan3Dä¸“ç”¨çš„åˆ†å¸ƒå¼é‡å¤é‡‡æ ·å™¨
    ç¡®ä¿æ¯å¼ å›¾åƒåœ¨æ‰€æœ‰GPUä¸Šç”Ÿæˆå¤šä¸ªmeshï¼Œå®ç°çœŸæ­£çš„groupæ¯”è¾ƒ
    ç±»ä¼¼SD3çš„DistributedKRepeatSamplerä½†é€‚é…å›¾åƒè¾“å…¥
    """
    def __init__(self, dataset, batch_size, k, num_replicas, rank, seed=0):
        self.dataset = dataset
        self.batch_size = batch_size  # æ¯å¡çš„batchå¤§å°
        self.k = k                    # æ¯å¼ å›¾åƒé‡å¤çš„æ¬¡æ•°(num_meshes_per_image)
        self.num_replicas = num_replicas  # æ€»å¡æ•°
        self.rank = rank              # å½“å‰å¡ç¼–å·
        self.seed = seed              # éšæœºç§å­ï¼Œç”¨äºåŒæ­¥
        
        # è®¡ç®—æ¯ä¸ªè¿­ä»£éœ€è¦çš„ä¸åŒå›¾åƒæ•°
        self.total_samples = self.num_replicas * self.batch_size
        
        # ğŸ”§ ä¿®å¤ï¼šå¤„ç†total_samples < kçš„æƒ…å†µï¼ˆå•GPUå°batchåœºæ™¯ï¼‰
        if self.total_samples < self.k:
            logger.warning(f"total_samples({self.total_samples}) < k({self.k}), è°ƒæ•´ä¸ºç®€å•é‡å¤æ¨¡å¼")
            self.m = self.total_samples  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ·æœ¬
            self.simple_repeat_mode = True
        else:
            assert self.total_samples % self.k == 0, f"k can not div n*b, k{k}-num_replicas{num_replicas}-batch_size{batch_size}"
            self.m = self.total_samples // self.k  # ä¸åŒå›¾åƒæ•°
            self.simple_repeat_mode = False
        
        self.epoch = 0

    def __iter__(self):
        while True:
            # ç”Ÿæˆç¡®å®šæ€§çš„éšæœºåºåˆ—ï¼Œç¡®ä¿æ‰€æœ‰å¡åŒæ­¥
            g = torch.Generator()
            g.manual_seed(self.seed + self.epoch)
            
            if self.simple_repeat_mode:
                # ğŸ”§ ç®€å•é‡å¤æ¨¡å¼ï¼šå½“total_samples < kæ—¶
                # éšæœºé€‰æ‹©å›¾åƒå¹¶é‡å¤å¡«æ»¡batch
                available_indices = torch.randperm(len(self.dataset), generator=g).tolist()
                
                # åˆ›å»ºè¶³å¤Ÿçš„æ ·æœ¬æ¥å¡«æ»¡æ‰€æœ‰GPUçš„batch
                repeated_indices = []
                for i in range(self.total_samples):
                    repeated_indices.append(available_indices[i % len(available_indices)])
                
                # å°†æ ·æœ¬åˆ†é…åˆ°å„ä¸ªå¡
                per_card_samples = []
                for i in range(self.num_replicas):
                    start = i * self.batch_size
                    end = start + self.batch_size
                    per_card_samples.append(repeated_indices[start:end])
                
                yield per_card_samples[self.rank]
            else:
                # ğŸ”§ æ ‡å‡†é‡å¤æ¨¡å¼ï¼šå½“total_samples >= kæ—¶
                # éšæœºé€‰æ‹©mä¸ªä¸åŒçš„å›¾åƒ
                indices = torch.randperm(len(self.dataset), generator=g)[:self.m].tolist()
                
                # æ¯å¼ å›¾åƒé‡å¤kæ¬¡ï¼Œç”Ÿæˆæ€»æ ·æœ¬æ•°n*b
                repeated_indices = [idx for idx in indices for _ in range(self.k)]
                
                # æ‰“ä¹±é¡ºåºç¡®ä¿å‡åŒ€åˆ†é…
                shuffled_indices = torch.randperm(len(repeated_indices), generator=g).tolist()
                shuffled_samples = [repeated_indices[i] for i in shuffled_indices]
                
                # å°†æ ·æœ¬åˆ†å‰²åˆ°å„ä¸ªå¡
                per_card_samples = []
                for i in range(self.num_replicas):
                    start = i * self.batch_size
                    end = start + self.batch_size
                    per_card_samples.append(shuffled_samples[start:end])
                
                # è¿”å›å½“å‰å¡çš„æ ·æœ¬ç´¢å¼•
                yield per_card_samples[self.rank]
    
    def set_epoch(self, epoch):
        self.epoch = epoch  # ç”¨äºåŒæ­¥ä¸åŒ epoch çš„éšæœºçŠ¶æ€

def compute_log_prob_3d(pipeline, sample: Dict[str, Any], j: int, image_conds: Dict[str, torch.Tensor], config: Any):
    """
    è®¡ç®—3Dæ‰©æ•£æ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡ - ç»“æ„ä¸SD3çš„compute_log_probå®Œå…¨å¯¹é½
    """
    # è°ƒæ•´1ï¼šåœ¨å‡½æ•°å†…éƒ¨ä»sampleå’Œjä¸­æå–æ•°æ®
    latents = sample["latents"][:, j]
    next_latents = sample["next_latents"][:, j]
    timestep = sample["timesteps"][:, j]
    
    # æ­¥éª¤1: æ¨¡å‹å‰å‘ä¼ æ’­ï¼Œæ ¹æ®CFGé…ç½®å‡†å¤‡è¾“å…¥å¹¶é¢„æµ‹å™ªå£°
    if config.train.cfg:
        # CFGè·¯å¾„: å‡†å¤‡æ‹¼æ¥åçš„è¾“å…¥
        model_latents = torch.cat([latents] * 2)
        model_timestep = torch.cat([timestep] * 2)
        
        # Hunyuanç‰¹æœ‰é¢„å¤„ç†
        timestep_normalized = torch.clamp(model_timestep.float() / 1000.0, min=1e-6, max=1.0 - 1e-6)
        contexts = {k: v.repeat_interleaved(2, dim=0) for k, v in image_conds.items()}
        if torch.isnan(model_latents).any(): model_latents = torch.nan_to_num(model_latents)
            
        # æ¨¡å‹é¢„æµ‹
        with torch.amp.autocast('cuda'):
            noise_pred_combined = pipeline.model(model_latents, timestep_normalized, contexts)
            
        # åº”ç”¨CFG
        noise_pred_neg, noise_pred_pos = noise_pred_combined.chunk(2)
        noise_pred = noise_pred_neg + config.sample.guidance_scale * (noise_pred_pos - noise_pred_neg)
        
    else:
        # éCFGè·¯å¾„: ä½¿ç”¨åŸå§‹è¾“å…¥
        model_latents = latents
        model_timestep = timestep
        
        # Hunyuanç‰¹æœ‰é¢„å¤„ç†
        timestep_normalized = torch.clamp(model_timestep.float() / 1000.0, min=1e-6, max=1.0 - 1e-6)
        contexts = image_conds
        if torch.isnan(model_latents).any(): model_latents = torch.nan_to_num(model_latents)
    
        # æ¨¡å‹é¢„æµ‹
        with torch.amp.autocast('cuda'):
            noise_pred = pipeline.model(model_latents, timestep_normalized, contexts)

    # æ­¥éª¤2: SDEæ­¥éª¤è®¡ç®—log_prob (ä¸SD3çš„æµç¨‹ä¸€è‡´)
    prev_sample, log_prob, prev_sample_mean, std_dev = pipeline.scheduler.hunyuan3d_sde_step_with_logprob(
        model_output=noise_pred,
        timestep=timestep[0],
        sample=latents,
        prev_sample=next_latents,
    )
    
    return prev_sample, log_prob, prev_sample_mean, std_dev

def save_meshes_for_wandb(meshes, image_paths, rewards, epoch, tmpdir, device="cuda"):
    """ä¿å­˜meshå¹¶ç”Ÿæˆé¢„è§ˆå›¾ - åªä¿å­˜.objå’Œ.pngï¼Œä¸ä¿å­˜.mtl"""
    from generators.hunyuan3d.hy3dshape.utils.visualizers.renderer import render_mesh_for_training
    import os
    
    mesh_files = []
    preview_files = []
    
    for idx, (mesh, img_path, reward) in enumerate(zip(meshes, image_paths, rewards)):
        # ä¿å­˜meshæ–‡ä»¶(.obj)ï¼Œä½†ä¸ä¿å­˜æè´¨æ–‡ä»¶(.mtl)
        mesh_path = os.path.join(tmpdir, f"mesh_{idx}.obj")
        mesh.write(mesh_path)

        # ç”Ÿæˆé¢„è§ˆå›¾
        preview_path = os.path.join(tmpdir, f"preview_{idx}.png")
        render_mesh_for_training(mesh_path, preview_path, device=device)
        print(f"ğŸ’¾ æ¸²æŸ“å·²ä¿å­˜: {preview_path}")
        
        mesh_files.append(mesh_path)
        preview_files.append(preview_path)
    
    return mesh_files, preview_files

def save_ckpt_hunyuan3d(model, ema, optimizer, epoch, global_step, save_dir, accelerator):
    """Save checkpoint in SD3 style - LoRA compatible"""
    checkpoint_dir = os.path.join(save_dir, f"checkpoints", f"checkpoint-{global_step}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ğŸ”§ ä¿®å¤ï¼šå¯¹äºLoRAæ¨¡å‹ï¼Œä½¿ç”¨save_pretrainedåªä¿å­˜é€‚é…å™¨æƒé‡
    import os
    
    # æ£€æŸ¥æ¨¡å‹é…ç½®ç¡®å®šä¿å­˜æ–¹å¼
    from config.hunyuan3d import _CONFIG
    config = _CONFIG.value
    
    if config.use_lora:
        # LoRAæ¨¡å¼ï¼šåªä¿å­˜é€‚é…å™¨æƒé‡
        lora_save_dir = os.path.join(checkpoint_dir, "lora")
        os.makedirs(lora_save_dir, exist_ok=True)
        
        unwrapped_model = accelerator.unwrap_model(model)
        if hasattr(unwrapped_model, 'save_pretrained'):
            unwrapped_model.save_pretrained(lora_save_dir)
            logger.info(f"âœ… LoRAé€‚é…å™¨å·²ä¿å­˜åˆ°: {lora_save_dir}")
        else:
            logger.warning("âš ï¸ æ¨¡å‹æ²¡æœ‰save_pretrainedæ–¹æ³•ï¼Œfallbackåˆ°state_dict")
            model_state = unwrapped_model.state_dict()
            model_path = os.path.join(checkpoint_dir, "pytorch_model.bin")
            torch.save(model_state, model_path)
    else:
        # å…¨æ¨¡å‹è®­ç»ƒï¼šä¿å­˜å®Œæ•´æƒé‡
        unwrapped_model = accelerator.unwrap_model(model)
        model_state = unwrapped_model.state_dict()
        model_path = os.path.join(checkpoint_dir, "pytorch_model.bin")
        torch.save(model_state, model_path)
    
    # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
    optimizer_path = os.path.join(checkpoint_dir, "optimizer.bin")
    torch.save(optimizer.state_dict(), optimizer_path)
    
    # ä¿å­˜EMAçŠ¶æ€ (å¦‚æœå­˜åœ¨)
    if ema is not None:
        ema_state = ema.state_dict()
        ema_path = os.path.join(checkpoint_dir, "pytorch_model_ema.bin")
        torch.save(ema_state, ema_path)
    
    # ä¿å­˜è®­ç»ƒå…ƒæ•°æ®
    metadata = {
        "epoch": epoch,
        "global_step": global_step,
        "pytorch_version": torch.__version__,
        "use_lora": config.use_lora,
    }
    metadata_path = os.path.join(checkpoint_dir, "training_metadata.json")
    import json
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)
    
    logger.info(f"âœ… Checkpointå·²ä¿å­˜åˆ°: {checkpoint_dir}")

def calculate_zero_std_ratio_images(image_names, gathered_rewards):
    """
    Calculates the ratio of image groups with zero standard deviation in their rewards.
    This function is now simplified to be more robust.
    """
    if 'avg' not in gathered_rewards or gathered_rewards['avg'].size == 0:
        return 0.0

    rewards_flat = gathered_rewards['avg'].flatten()
    image_names_flat = np.array(image_names)

    if rewards_flat.shape[0] != image_names_flat.shape[0]:
        # Fallback if shapes mismatch, though this shouldn't happen with prior fixes.
        return 0.0

    unique_names = np.unique(image_names_flat)
    if len(unique_names) == 0:
        return 0.0

    std_devs = [np.std(rewards_flat[image_names_flat == name]) for name in unique_names]
    zero_std_count = np.count_nonzero(np.array(std_devs) == 0)
    
    return zero_std_count / len(unique_names)


def main(argv):
    """Main training function - inline architecture similar to SD3"""
    del argv
    config = _CONFIG.value
    
    unique_id = datetime.datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
    if not config.run_name:
        config.run_name = unique_id
    else:
        config.run_name += "_" + unique_id

    accelerator_config = ProjectConfiguration(
        project_dir=os.path.join(config.logdir, config.run_name),
        automatic_checkpoint_naming=True,
        total_limit=config.num_checkpoint_limit,
    )
    
    num_train_timesteps = int(config.sample.num_steps * config.train.timestep_fraction)
    
    accelerator = Accelerator(
        log_with="wandb",
        mixed_precision=config.mixed_precision,
        project_config=accelerator_config,
        gradient_accumulation_steps=config.train.gradient_accumulation_steps * num_train_timesteps,
    )
    
    if accelerator.is_main_process:
        accelerator.init_trackers(
            project_name="flow-grpo-3d",
            config=config.to_dict(),
            init_kwargs={"wandb": {"name": config.run_name}},
        )
    
    logger.info(f"\n{config}")
    
    set_seed(config.seed, device_specific=True)
    
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    os.makedirs(config.save_dir, exist_ok=True)
    
    # Load Hunyuan3D pipeline
    logger.info("Loading Hunyuan3D pipeline...")
    pipeline_wrapper = Hunyuan3DPipeline()
    pipeline = pipeline_wrapper.core_pipeline
    
    # 3. ç§»é™¤â€œçŒ´å­è¡¥ä¸â€ï¼šåœ¨pipelineåˆå§‹åŒ–åï¼Œåªæ‰§è¡Œä¸€æ¬¡SDEå‡½æ•°çš„åŠ¨æ€ç»‘å®š
    if not hasattr(pipeline.scheduler, 'hunyuan3d_sde_step_with_logprob'):
        import types
        pipeline.scheduler.hunyuan3d_sde_step_with_logprob = types.MethodType(
            hunyuan3d_sde_step_with_logprob, pipeline.scheduler
        )

    # Enable FlashVDM if configured
    if config.flashvdm and config.flashvdm.enabled:
        pipeline.enable_flashvdm(
            enabled=config.flashvdm.enabled,
            adaptive_kv_selection=config.flashvdm.adaptive_kv_selection,
            topk_mode=config.flashvdm.topk_mode,
            mc_algo=config.flashvdm.mc_algo,
            replace_vae=config.flashvdm.replace_vae
        )
    
    # Freeze parameters
    pipeline.vae.requires_grad_(False)
    pipeline.conditioner.requires_grad_(False)
    pipeline.model.requires_grad_(not config.use_lora)
    
    # Set precision
    inference_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        inference_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        inference_dtype = torch.bfloat16
    
    # Move to devices
    pipeline.vae.to(accelerator.device, dtype=torch.float32)
    pipeline.conditioner.to(accelerator.device, dtype=inference_dtype)
    
    if config.use_lora:
        pipeline.model.to(accelerator.device)
    else:
        pipeline.model.to(accelerator.device, dtype=inference_dtype)
    
    pipeline.vae.eval()
    pipeline.conditioner.eval()
    pipeline.vae.requires_grad_(False)
    pipeline.conditioner.requires_grad_(False)
    
    # LoRA setup
    if config.use_lora:
        from peft import LoraConfig, get_peft_model
        
        lora_config = LoraConfig(
            r=32,
            lora_alpha=64,
            target_modules=[
                "to_q", "to_k", "to_v", "out_proj",
                "fc1", "fc2",
                "final_layer.linear",
            ],
            lora_dropout=0.1,
            bias="none",
        )
        
        pipeline.model = get_peft_model(pipeline.model, lora_config)
    
    model = pipeline.model
    trainable_params = list(filter(lambda p: p.requires_grad, model.parameters()))
    
    # Optimizer
    if config.train.use_8bit_adam:
        try:
            import bitsandbytes as bnb
        except ImportError:
            raise ImportError("Please install bitsandbytes to use 8-bit Adam")
        optimizer_cls = bnb.optim.AdamW8bit
    else:
        optimizer_cls = torch.optim.AdamW
    
    optimizer = optimizer_cls(
        trainable_params,
        lr=config.train.learning_rate,
        betas=(config.train.adam_beta1, config.train.adam_beta2),
        weight_decay=config.train.adam_weight_decay,
        eps=config.train.adam_epsilon,
    )
    
    model, optimizer = accelerator.prepare(model, optimizer)
    pipeline.model = model
    
    # EMA
    ema = None
    if config.train.ema:
        ema = EMAModuleWrapper(
            trainable_params,
            decay=config.train.ema_decay,
            device=accelerator.device
        )
    
    # Enable TF32
    if config.allow_tf32:
        torch.backends.cuda.matmul.allow_tf32 = True
    
    # Reward function - ğŸ”§ NEW: æ›´æ–°ä¸ºç®€åŒ–çš„å›¾åƒæ¨¡å¼API
    reward_config = config.reward_fn.to_dict()
    
    # ğŸ”¥ é˜¶æ®µä¸€ï¼šä½¿ç”¨ä¸“é—¨çš„å‡½æ•°é¢„åŠ è½½å’Œç¼“å­˜è¯„åˆ†æ¨¡å‹
    if accelerator.is_main_process:
        preload_scorers(reward_config, accelerator.device)
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹åŒæ­¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­éƒ½å¯ç”¨ï¼ˆå³ä½¿åªæœ‰ä¸»è¿›ç¨‹åŠ è½½ï¼‰
    accelerator.wait_for_everyone()

    # åˆ›å»ºé€‚é…å™¨å‡½æ•°ï¼Œä¿æŒä¸åŸæœ‰ä»£ç çš„å…¼å®¹æ€§
    def reward_fn(meshes, images, metadata):
        """å¥–åŠ±å‡½æ•°é€‚é…å™¨ï¼Œè°ƒç”¨ç®€åŒ–çš„å›¾åƒæ¨¡å¼API"""
        return multi_mesh_score(meshes, images, metadata, reward_config)
    
    # Dataset
    logger.info(f"Loading dataset from {config.data_dir}")
    train_dataset = Image3DDataset(config.data_dir, split="train")
    
    # ğŸ”§ ä¿®å¤Groupå¤„ç†ï¼šä½¿ç”¨åˆ†å¸ƒå¼é‡å¤é‡‡æ ·å™¨ï¼ˆç±»ä¼¼SD3ï¼‰
    # Create DistributedImageRepeatSampler for proper group comparison
    train_sampler = DistributedImageRepeatSampler(
        train_dataset,
        config.sample.input_batch_size,
        config.sample.num_meshes_per_image,  # k: æ¯å¼ å›¾åƒé‡å¤æ¬¡æ•°
        accelerator.num_processes,
        accelerator.process_index,
        config.seed,
    )
    
    train_dataloader = DataLoader(
        train_dataset,
        batch_sampler=train_sampler,  # ä½¿ç”¨batch_samplerè€Œä¸æ˜¯sampler
        collate_fn=Image3DDataset.collate_fn,
        num_workers=0,
    )
    
    # Stat tracker
    stat_tracker = None
    if config.per_image_stat_tracking:
        stat_tracker = PerImageStatTracker(
            buffer_size=config.per_image_stat_tracking.buffer_size,
            min_count=config.per_image_stat_tracking.min_count
        )
    
    train_dataloader = accelerator.prepare(train_dataloader)
    
    executor = futures.ThreadPoolExecutor(max_workers=8)
    
    # Training loop
    global_step = 0
    first_epoch = 0
    
    train_iter = iter(train_dataloader)
    
    autocast = contextlib.nullcontext if config.use_lora else accelerator.autocast
    
    for epoch in range(first_epoch, config.num_epochs):
        logger.info(f"Starting epoch {epoch}")
        
        #################### SAMPLING ####################
        model.eval()
        samples = []
        
        for i in tqdm(
            range(config.sample.num_batches_per_epoch),
            desc=f"Epoch {epoch}: sampling",
            disable=not accelerator.is_local_main_process,
            position=0,
        ):
            # ğŸ”§ ä¿®å¤Groupå¤„ç†ï¼šè®¾ç½®epochä»¥åŒæ­¥æ‰€æœ‰GPUçš„éšæœºçŠ¶æ€
            train_sampler.set_epoch(epoch * config.sample.num_batches_per_epoch + i)
            
            try:
                image_paths, prompts, metadata = next(train_iter)
            except StopIteration:
                train_iter = iter(train_dataloader)
                image_paths, prompts, metadata = next(train_iter)
            
            pil_images = [Image.open(path).convert('RGBA') for path in image_paths]
            
            # ğŸ”§ è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å½“å‰batchçš„å›¾åƒä¿¡æ¯
            if accelerator.is_local_main_process:
                logger.info(f"Batch {i}: processing {len(image_paths)} images: {[os.path.basename(p) for p in image_paths]}")
            
            # Encode image conditions
            cond_inputs = pipeline.prepare_image(pil_images)
            image_cond = pipeline.encode_cond(
                image=cond_inputs.pop('image'),
                additional_cond_inputs=cond_inputs,
                do_classifier_free_guidance=True,
                dual_guidance=False,
            )
            
            # ğŸ”§ ä¿®å¤Groupå¤„ç†ï¼šç°åœ¨æ¯å¼ å›¾åƒä¼šåœ¨å¤šä¸ªGPUä¸Šé‡å¤å¤„ç†
            # æ¯ä¸ªGPUå¯¹åŒä¸€å›¾åƒç”Ÿæˆä¸åŒçš„meshæ ·æœ¬ï¼Œå®ç°çœŸæ­£çš„groupæ¯”è¾ƒ
            positive_image_cond = {}
            negative_image_cond = {}
            for key in image_cond.keys():
                batch_size = image_cond[key].shape[0]
                positive_image_cond[key] = image_cond[key][:batch_size//2].repeat_interleave(config.sample.num_meshes_per_image, dim=0)
                negative_image_cond[key] = image_cond[key][batch_size//2:].repeat_interleave(config.sample.num_meshes_per_image, dim=0)

            # Generate meshes
            with torch.no_grad():
                meshes, all_latents, all_log_probs, all_kl = hunyuan3d_pipeline_with_logprob(
                    pipeline,
                    positive_image_cond=positive_image_cond,
                    negative_image_cond=negative_image_cond,
                    num_inference_steps=config.sample.num_steps,
                    guidance_scale=config.sample.guidance_scale,
                    kl_reward=config.sample.kl_reward,
                    octree_resolution=384,
                    mc_level=0.0,
                    mc_algo=None,
                    box_v=1.01,
                    num_chunks=50000,
                )
            
            latents = torch.stack(all_latents, dim=1)
            log_probs = torch.stack(all_log_probs, dim=1)
            kls = torch.stack(all_kl, dim=1)
            kl = kls.detach()
            
            timesteps = pipeline.scheduler.timesteps.repeat(
                config.train.batch_size, 1
            )
            # Fix: timesteps should match current_latents/next_latents (20 steps), not latents (21 steps) 
            timesteps = timesteps[:, :-1]  # Remove last timestep to match SD3 behavior
            
            # Compute rewards asynchronously
            rewards = executor.submit(reward_fn, meshes, image_paths, {})
            time.sleep(0)
            
            current_latents = latents[:, :-1]
            next_latents = latents[:, 1:]



            samples.append({
                "image_paths": image_paths,
                "positive_image_cond": positive_image_cond,
                "negative_image_cond": negative_image_cond,
                "timesteps": timesteps,
                "latents": current_latents,
                "next_latents": next_latents,
                "log_probs": log_probs,
                "kl": kl,
                "rewards": rewards,
            })
            
        # Wait for rewards
        for sample in tqdm(
            samples,
            desc="Waiting for rewards",
            disable=not accelerator.is_local_main_process,
        ):
            rewards, reward_metadata = sample["rewards"].result()
            sample["rewards"] = {
                key: torch.as_tensor(value, device=accelerator.device).float()
                for key, value in rewards.items()
            }
        
        # Collate samples (Re-written for clarity and correctness)
        collated_samples = defaultdict(list)
        for s in samples:
            for k, v in s.items():
                collated_samples[k].append(v)

        final_samples = {}
        for k, v_list in collated_samples.items():
            if k in ["positive_image_cond", "negative_image_cond", "rewards"]:
                # It's a list of dictionaries, need to merge them
                merged_dict = defaultdict(list)
                for d in v_list:
                    for sub_k, sub_v in d.items():
                        merged_dict[sub_k].append(sub_v)
                final_samples[k] = {sub_k: torch.cat(sub_v_list, dim=0) for sub_k, sub_v_list in merged_dict.items()}
            elif k == "image_paths":
                # Flatten the list of lists
                final_samples[k] = [path for sublist in v_list for path in sublist]
            else:
                # Regular tensors
                final_samples[k] = torch.cat(v_list, dim=0)
        samples = final_samples


        samples["rewards"]["ori_avg"] = samples["rewards"]["avg"]
        samples["rewards"]["avg"] = samples["rewards"]["avg"].unsqueeze(-1) - config.sample.kl_reward*samples["kl"]
        # gather rewards across processes
        # ğŸ”„ SD3 Debug: åˆ†å¸ƒå¼Gather - æ”¶é›†æ‰€æœ‰GPUçš„å¥–åŠ±æ•°æ®
        # samples["rewards"]["avg"].shape = (local_batch_size, 1) æ¯ä¸ªGPUçš„æœ¬åœ°å¥–åŠ±
        gathered_rewards = {key: accelerator.gather(value) for key, value in samples["rewards"].items()}
        # gathered_rewards["avg"].shape = (total_batch_size, 1) æ‰€æœ‰GPUçš„å¥–åŠ±æ±‡æ€»
        gathered_rewards = {key: value.cpu().numpy() for key, value in gathered_rewards.items()}

        # ä¿å­˜mesh (æ¯10ä¸ªepoch) - æ·»åŠ é…ç½®æ§åˆ¶
        save_visualizations = getattr(config, 'save_visualizations', False)  # é»˜è®¤ç¦ç”¨
        if epoch % 10 == 0 and accelerator.is_main_process and save_visualizations:
            # åˆ›å»ºæœ¬åœ°ä¿å­˜ç›®å½• (ä»¿ç…§SD3çš„logdiræ¨¡å¼)
            mesh_save_dir = os.path.join(config.logdir, config.run_name, "generated_meshes", f"epoch_{epoch}")
            os.makedirs(mesh_save_dir, exist_ok=True)
            
            # é€‰æ‹©å‰2ä¸ªmeshï¼ˆå¯¹åº”ç¬¬ä¸€å¼ å›¾ç‰‡çš„2ä¸ªç”Ÿæˆç»“æœï¼‰
            num_samples = min(2, len(meshes))
            
            sampled_meshes = meshes[:num_samples]
            sampled_paths = samples["image_paths"][:num_samples]
            sampled_rewards = gathered_rewards['avg'][:num_samples]
            
            # æœ¬åœ°ä¿å­˜å’Œæ¸²æŸ“
            mesh_files, preview_files = save_meshes_for_wandb(
                sampled_meshes, sampled_paths, sampled_rewards, epoch, mesh_save_dir, "cuda"
            )
            
            # ğŸ”§ åªä¸Šä¼ é¢„è§ˆå›¾åˆ°wandbï¼Œä¸ä¸Šä¼ 3Då¯¹è±¡
            accelerator.log({
                "mesh_previews": [
                    wandb.Image(preview_files[i], caption=f"{os.path.basename(sampled_paths[i])}")
                    for i in range(len(preview_files))
                ],
            }, step=global_step)
            
            logger.info(f"âœ… å·²ä¿å­˜ {len(mesh_files)} ä¸ªmeshå¯è§†åŒ–åˆ° {mesh_save_dir}")
        # log rewards
        accelerator.log(
            {
                "epoch": epoch,
                **{f"reward_{key}": value.mean() for key, value in gathered_rewards.items() if '_strict_accuracy' not in key and '_accuracy' not in key},
                "kl": samples["kl"].mean().cpu().numpy(),
                "kl_abs": samples["kl"].abs().mean().cpu().numpy()
            },
            step=global_step,
        )

        # per-image mean/std tracking
        if config.per_image_stat_tracking:
            gathered_rewards_for_tracker = gathered_rewards['avg'].flatten()
            image_names_for_tracker = []
            for i in range(len(gathered_rewards_for_tracker)):
                path_idx = i % len(samples["image_paths"])
                image_names_for_tracker.append(os.path.basename(samples["image_paths"][path_idx]))

            # ğŸ”§ FIX: å°†å­—ç¬¦ä¸²å›¾åƒåè½¬æ¢ä¸ºæ•°å€¼IDï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
            unique_names = list(set(image_names_for_tracker))
            name_to_id = {name: idx for idx, name in enumerate(unique_names)}
            image_ids_for_tracker = np.array([name_to_id[name] for name in image_names_for_tracker])
            
            advantages = stat_tracker.update(image_ids_for_tracker, gathered_rewards_for_tracker)
            if accelerator.is_local_main_process:
                print("len(image_names)", len(image_names_for_tracker))
                print("len unique image_names", len(set(image_names_for_tracker)))

            group_size, trained_image_num = stat_tracker.get_stats()
            zero_std_ratio = calculate_zero_std_ratio_images(image_names_for_tracker, gathered_rewards)

            accelerator.log(
                {
                    "group_size": group_size,
                    "trained_image_num": trained_image_num,
                    "zero_std_ratio": zero_std_ratio,
                },
                step=global_step,
            )
        else:
            logger.warning("ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å¯èƒ½å¯¼è‡´ä¸åŒå›¾åƒé—´çš„ä¸åˆç†æ¯”è¾ƒï¼Œå»ºè®®å¯ç”¨per_image_stat_tracking")
            advantages = (gathered_rewards['avg'].mean(axis=1) - gathered_rewards['avg'].mean(axis=1).mean()) / (gathered_rewards['avg'].mean(axis=1).std() + 1e-4)

        # ungather advantages
        advantages = torch.as_tensor(advantages)
        num_steps = samples["timesteps"].shape[1] # num_steps = config.sample.num_steps (å¦‚20)
        advantages = advantages.unsqueeze(1).expand(-1, num_steps) # advantages.shape = (total_batch_size, num_steps)
        # ğŸ”„ SD3 Debug: åˆ†å¸ƒå¼Ungather - æ¯ä¸ªGPUåªä¿ç•™è‡ªå·±å¯¹åº”çš„æ•°æ®åˆ‡ç‰‡
        samples["advantages"] = (
            advantages.reshape(accelerator.num_processes, -1, advantages.shape[-1])[accelerator.process_index]
            # reshape: (total_batch_size, num_steps) -> (num_processes, local_batch_size, num_steps)
            # [process_index]: é€‰æ‹©å½“å‰GPUå¯¹åº”çš„åˆ‡ç‰‡ -> (local_batch_size, num_steps)
            .to(accelerator.device)
        )
        if accelerator.is_local_main_process:
            # ğŸ”„ SD3 Debug: æ‰“å°æœ¬åœ°GPUçš„ä¼˜åŠ¿å‡½æ•°å’ŒKLæ•£åº¦ç»Ÿè®¡ä¿¡æ¯
            print("advantages: ", samples["advantages"].abs().mean()) # samples["advantages"].shape = (local_batch_size, num_steps)
            print("kl: ", samples["kl"].mean()) # samples["kl"].shape = (local_batch_size, num_steps)

        # ğŸ”„ SD3 Debug: å†…å­˜ä¼˜åŒ– - åˆ é™¤ä¸å†éœ€è¦çš„å¤§æ•°æ®ç»“æ„
        del samples["rewards"] # å·²å®Œæˆä¼˜åŠ¿å‡½æ•°è®¡ç®—ï¼Œå¥–åŠ±æ•°æ®ä¸å†éœ€è¦
        del samples["image_paths"] # å›¾åƒè·¯å¾„åªç”¨äºç»Ÿè®¡è·Ÿè¸ªï¼Œç°åœ¨å¯ä»¥åˆ é™¤

        # ğŸ”„ SD3 Debug: æ•°æ®è¿‡æ»¤ - ç­›é€‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬
        # Get the mask for samples where all advantages are zero across the time dimension (SD3 style)
        mask = (samples["advantages"].abs().sum(dim=1) != 0) # mask.shape = (local_batch_size,)
        
        # If the number of True values in mask is not divisible by config.sample.num_batches_per_epoch,
        # randomly change some False values to True to make it divisible
        num_batches = config.sample.num_batches_per_epoch
        true_count = mask.sum() # æœ‰æ•ˆæ ·æœ¬æ•°é‡
        if true_count % num_batches != 0:
            false_indices = torch.where(~mask)[0]
            num_to_change = num_batches - (true_count % num_batches)
            if len(false_indices) >= num_to_change:
                random_indices = torch.randperm(len(false_indices))[:num_to_change]
                mask[false_indices[random_indices]] = True
        accelerator.log(
            {
                "actual_batch_size": mask.sum().item()//config.sample.num_batches_per_epoch,
            },
            step=global_step,
        )
        # ğŸ”„ SD3 Debug: åº”ç”¨maskè¿‡æ»¤ - ç§»é™¤advantageså…¨ä¸ºé›¶çš„æ— æ•ˆæ ·æœ¬
        # Filter out samples where the entire time dimension of advantages is zero
        # (SD3 logic with Hunyuan3D data structure adaptation)
        filtered_samples = {}
        for k, v in samples.items():
            # ğŸ”§ FIX: Skip filtering for image conditions, as their batch size is different.
            if k in ["positive_image_cond", "negative_image_cond"]:
                filtered_samples[k] = v
                continue

            if isinstance(v, torch.Tensor) and v.shape[0] == mask.shape[0]:
                # Apply mask to tensors with matching batch dimension
                # v.shape = (local_batch_size, ...) -> è¿‡æ»¤å -> (filtered_batch_size, ...)
                filtered_samples[k] = v[mask]
            else:
                # Keep unchanged for dimension mismatches (Hunyuan3D specific)
                filtered_samples[k] = v
        samples = filtered_samples

        # ğŸ”„ SD3 Debug: éªŒè¯è¿‡æ»¤åçš„æ•°æ®ç»´åº¦
        total_batch_size, num_timesteps = samples["timesteps"].shape
        # total_batch_size = filtered_batch_size, num_timesteps = config.sample.num_steps
        assert num_timesteps == config.sample.num_steps  # Now timesteps matches latents/log_probs (20 steps)
        
        #################### TRAINING ####################
        for inner_epoch in range(config.train.num_inner_epochs):
            
            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘                    ğŸ”„ æ•°æ®é‡ç»„é˜¶æ®µ1 - æ²¿batchç»´åº¦éšæœºæ‰“ä¹±                    â•‘
            # â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
            # â•‘ ç›®çš„ï¼šæ‰“ç ´æ•°æ®çš„åŸæœ‰é¡ºåºï¼Œå¢åŠ è®­ç»ƒéšæœºæ€§ï¼Œé¿å…æ¨¡å‹å­¦ä¹ åˆ°æ•°æ®æ’åˆ—çš„åè§        â•‘
            # â•‘ åŸç†ï¼šå¯¹batchä¸­çš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œéšæœºé‡æ’ï¼Œä½†ä¿æŒæ¯ä¸ªæ ·æœ¬å†…éƒ¨çš„æ—¶åºå…³ç³»ä¸å˜      â•‘
            # â•‘ å®ç°ï¼šç”Ÿæˆéšæœºæ’åˆ—ç´¢å¼•ï¼Œæ‰€æœ‰tensoræŒ‰ç›¸åŒé¡ºåºé‡æ’ï¼Œä¿æŒæ ·æœ¬é—´çš„å¯¹åº”å…³ç³»       â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # shuffle samples along batch dimension
            perm = torch.randperm(total_batch_size, device=accelerator.device) # perm.shape = (total_batch_size,)
            # ğŸ” permç¤ºä¾‹: å¦‚æœtotal_batch_size=4ï¼Œå¯èƒ½ç”Ÿæˆ [2, 0, 3, 1]
            # è¡¨ç¤º: æ–°ä½ç½®0å–åŸä½ç½®2çš„æ ·æœ¬ï¼Œæ–°ä½ç½®1å–åŸä½ç½®0çš„æ ·æœ¬ï¼Œä»¥æ­¤ç±»æ¨
            
            # Handle dictionary and tensor shuffles
            for k, v in samples.items():
                if k in ["positive_image_cond", "negative_image_cond", "rewards"]:
                    samples[k] = {sub_k: sub_v[perm] for sub_k, sub_v in v.items()}
                else:
                    samples[k] = v[perm]


            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘                 ğŸ”„ æ•°æ®é‡ç»„é˜¶æ®µ2 - æ²¿æ—¶é—´ç»´åº¦ç‹¬ç«‹æ‰“ä¹±æ¯ä¸ªæ ·æœ¬                 â•‘
            # â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
            # â•‘ ç›®çš„ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„æ—¶é—´æ­¥è¿›è¡Œç‹¬ç«‹é‡æ’ï¼Œå¢åŠ æ—¶åºè®­ç»ƒçš„å¤šæ ·æ€§å’Œé²æ£’æ€§           â•‘
            # â•‘ åŸç†ï¼šGRPOå¯ä»¥åœ¨ä»»æ„æ—¶é—´æ­¥ç»„åˆä¸Šè®­ç»ƒï¼Œä¸éœ€è¦ä¸¥æ ¼æŒ‰æ‰©æ•£è¿‡ç¨‹çš„å›ºå®šé¡ºåº       â•‘
            # â•‘ å®ç°ï¼šä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆç‹¬ç«‹çš„æ—¶é—´æ­¥æ’åˆ—ï¼Œä½†å½“å‰ä¸ºäº†è°ƒè¯•ç¨³å®šæ€§ä½¿ç”¨å›ºå®šé¡ºåº      â•‘
            # â•‘ æ•ˆæœï¼šç ´åæ—¶é—´æ­¥é—´çš„ç›¸å…³æ€§ï¼Œè®©æ¨¡å‹å­¦ä¹ æ›´æ³›åŒ–çš„ç­–ç•¥                         â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # shuffle along time dimension independently for each sample
            perms = torch.stack(
                [
                    # ğŸ”§ å¯é€‰éšæœºåŒ–: torch.randperm(num_timesteps, device=accelerator.device)
                    # ğŸ”§ å½“å‰å›ºå®šé¡ºåº: ä¸ºäº†è°ƒè¯•å’Œå¤ç°æ€§ï¼Œæš‚æ—¶ä½¿ç”¨æ—¶é—´æ­¥çš„è‡ªç„¶é¡ºåº
                    torch.arange(num_timesteps, device=accelerator.device) # å½“å‰ä½¿ç”¨é¡ºåºï¼Œä¸éšæœº
                    for _ in range(total_batch_size)
                ]
            ) # perms.shape = (total_batch_size, num_timesteps)
            
            # ğŸ” permsæ•°æ®ç»“æ„ç¤ºä¾‹: å¦‚æœtotal_batch_size=4, num_timesteps=20
            # perms = tensor([[0,1,2,...,19],    # æ ·æœ¬0çš„æ—¶é—´æ­¥æ’åˆ—: æŒ‰é¡ºåº
            #                 [0,1,2,...,19],    # æ ·æœ¬1çš„æ—¶é—´æ­¥æ’åˆ—: æŒ‰é¡ºåº  
            #                 [0,1,2,...,19],    # æ ·æœ¬2çš„æ—¶é—´æ­¥æ’åˆ—: æŒ‰é¡ºåº
            #                 [0,1,2,...,19]])   # æ ·æœ¬3çš„æ—¶é—´æ­¥æ’åˆ—: æŒ‰é¡ºåº
            # ğŸ”„ å¦‚æœå¯ç”¨éšæœºåŒ–ï¼Œæ¯è¡Œå°†æ˜¯[0-19]çš„ä¸åŒéšæœºæ’åˆ—ï¼Œå®ç°ç‹¬ç«‹çš„æ—¶åºæ‰“ä¹±
            
            # å¯¹æ‰€æœ‰åŒ…å«æ—¶é—´ç»´åº¦çš„tensorè¿›è¡Œé‡æ’
            for key in ["timesteps", "latents", "next_latents", "log_probs"]:
                # ğŸ” é«˜çº§ç´¢å¼•è¯¦è§£:
                # torch.arange(total_batch_size)[:, None] åˆ›å»ºåˆ—å‘é‡: [[0], [1], [2], [3]]
                # perms æ˜¯çŸ©é˜µ: [[perm0], [perm1], [perm2], [perm3]]
                # ç»„åˆç´¢å¼• [batch_indices, time_indices] å®ç°: 
                #   - å¯¹æ ·æœ¬0ï¼Œå– samples[key][0, perm0]
                #   - å¯¹æ ·æœ¬1ï¼Œå– samples[key][1, perm1]  
                #   - å¯¹æ ·æœ¬2ï¼Œå– samples[key][2, perm2]
                #   - å¯¹æ ·æœ¬3ï¼Œå– samples[key][3, perm3]
                # ç»“æœ: æ¯ä¸ªæ ·æœ¬çš„æ—¶é—´ç»´åº¦æŒ‰å…¶ä¸“å±æ’åˆ—é‡æ–°æ’åº
                samples[key] = samples[key][
                    torch.arange(total_batch_size, device=accelerator.device)[:, None],  # (total_batch_size, 1)
                    perms,  # (total_batch_size, num_timesteps)
                ]
                # ğŸ” å˜æ¢è¯´æ˜: samples[key].shapeä¿æŒ (total_batch_size, num_timesteps, ...)
                # ä½†æ¯ä¸ªæ ·æœ¬å†…éƒ¨çš„æ—¶é—´æ­¥é¡ºåºå¯èƒ½å®Œå…¨æ”¹å˜ï¼ˆå½“å‰ä¿æŒåŸåºï¼‰

            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘                    ğŸ”„ æ•°æ®é‡ç»„é˜¶æ®µ3 - Rebatchä¸ºè®­ç»ƒå­æ‰¹æ¬¡                     â•‘
            # â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
            # â•‘ ç›®çš„ï¼šå°†å¤§batché‡ç»„ä¸ºå¤šä¸ªå°batchï¼Œä¾¿äºæ¢¯åº¦ç´¯ç§¯å’Œæ˜¾å­˜ç®¡ç†                   â•‘
            # â•‘ åŸç†ï¼šGRPOéœ€è¦åœ¨å¤šä¸ªå­æ‰¹æ¬¡ä¸Šåˆ†åˆ«è®¡ç®—æ¢¯åº¦ï¼Œæœ€åç´¯ç§¯æ›´æ–°å‚æ•°                 â•‘
            # â•‘ æ•°å­¦ï¼štotal_batch_size -> (num_batches_per_epoch, batch_size_per_batch)      â•‘
            # â•‘ å¥½å¤„ï¼šå¯ä»¥ç”¨å°æ˜¾å­˜è®­ç»ƒå¤§batch_sizeï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§                         â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # --- START: REWRITTEN DATA RESTRUCTURING ---

            # Step 1: Split all tensors and dicts into chunks for each sub-batch
            chunk_size = total_batch_size // config.sample.num_batches_per_epoch
            batched_tensors = {}
            for k, v in samples.items():
                if k == 'image_paths':
                    batched_tensors[k] = [v[i:i + chunk_size] for i in range(0, len(v), chunk_size)]
                elif isinstance(v, dict):
                    # Handle nested dictionaries
                    batched_tensors[k] = [{sub_k: sub_v.chunk(config.sample.num_batches_per_epoch, dim=0)[i] for sub_k, sub_v in v.items()} for i in range(config.sample.num_batches_per_epoch)]
                else:
                    # Handle regular tensors
                    batched_tensors[k] = list(v.chunk(config.sample.num_batches_per_epoch, dim=0))

            # Step 2: Convert to list of dicts for easier iteration
            num_sub_batches = config.sample.num_batches_per_epoch
            samples_batched = [{} for _ in range(num_sub_batches)]
            for k, v_chunks in batched_tensors.items():
                for i in range(num_sub_batches):
                    samples_batched[i][k] = v_chunks[i]

            # --- END: REWRITTEN DATA RESTRUCTURING ---


            # train
            model.train()
            info = defaultdict(list)
            for i, sample in tqdm(
                list(enumerate(samples_batched)),
                desc=f"Epoch {epoch}.{inner_epoch}: training",
                position=0,
                disable=not accelerator.is_local_main_process,
            ):

                # è°ƒæ•´4ï¼šå°†image_condsçš„å‡†å¤‡å·¥ä½œç§»åˆ°å¤–å±‚å¾ªç¯ï¼Œä¸SD3å¯¹é½
                if config.train.cfg:
                    # Concatenate conditions for CFG
                    image_conds = {
                        k: torch.cat([sample["negative_image_cond"][k], sample["positive_image_cond"][k]])
                        for k in sample["positive_image_cond"]
                    }
                else:
                    image_conds = sample["positive_image_cond"]

                train_timesteps = [step_index for step_index in range(num_train_timesteps)]
                for j in tqdm(
                    train_timesteps,
                    desc="Timestep",
                    position=1,
                    leave=False,
                    disable=not accelerator.is_local_main_process,
                ):
                    with accelerator.accumulate(model):
                        with autocast():
                            # è°ƒæ•´2ï¼šç§»é™¤é¢„å…ˆåˆ‡ç‰‡ï¼Œç›´æ¥ä¼ é€’sampleå’Œj
                            prev_sample, log_prob, prev_sample_mean, std_dev = compute_log_prob_3d(
                                pipeline, sample, j, image_conds, config
                            )
                            # log_prob.shape = torch.Size([1, 16, 32, 32])
                            # prev_sample.shape = torch.Size([1, 4096, 64])
                            # std_dev.shape = torch.Size([1, 1, 1])
                            if config.train.beta > 0:
                                with torch.no_grad():
                                    model_for_adapter = model.module if hasattr(model, 'module') else model
                                    with model_for_adapter.disable_adapter():
                                        _, log_prob_ref, prev_sample_mean_ref, std_dev_ref = compute_log_prob_3d(
                                            pipeline, sample, j, image_conds, config
                                        )

                        # grpo logic
                        advantages = torch.clamp(
                            sample["advantages"][:, j],
                            -config.train.adv_clip_max,
                            config.train.adv_clip_max,
                        )
                        ratio = torch.exp(log_prob - sample["log_probs"][:, j])
                        unclipped_loss = -advantages * ratio
                        clipped_loss = -advantages * torch.clamp(
                            ratio,
                            1.0 - config.train.clip_range,
                            1.0 + config.train.clip_range,
                        )
                        policy_loss = torch.mean(torch.maximum(unclipped_loss, clipped_loss))
                        if config.train.beta > 0:
                            kl_loss = ((prev_sample_mean - prev_sample_mean_ref) ** 2).mean(dim=tuple(range(1, prev_sample_mean.ndim))) / (2 * std_dev ** 2)
                            kl_loss = torch.mean(kl_loss)
                            loss = policy_loss + config.train.beta * kl_loss
                        else:
                            loss = policy_loss

                        info["approx_kl"].append(
                            0.5
                            * torch.mean((log_prob - sample["log_probs"][:, j]) ** 2)
                        )
                        info["clipfrac"].append(
                            torch.mean(
                                (
                                    torch.abs(ratio - 1.0) > config.train.clip_range
                                ).float()
                            )
                        )
                        info["policy_loss"].append(policy_loss)
                        if config.train.beta > 0:
                            info["kl_loss"].append(kl_loss)

                        info["loss"].append(loss)

                        # backward pass
                        accelerator.backward(loss)
                        if accelerator.sync_gradients:
                            accelerator.clip_grad_norm_(
                                model.parameters(), config.train.max_grad_norm
                            )
                        optimizer.step()
                        optimizer.zero_grad()

                    # Checks if the accelerator has performed an optimization step behind the scenes
                    if accelerator.sync_gradients:
                        # log training-related stuff
                        info = {k: torch.mean(torch.stack(v)) for k, v in info.items()}
                        info = accelerator.reduce(info, reduction="mean")
                        info.update({"epoch": epoch, "inner_epoch": inner_epoch})
                        accelerator.log(info, step=global_step)
                        global_step += 1
                        info = defaultdict(list)
                if config.train.ema:
                    ema.step(model.parameters(), global_step)
            # make sure we did an optimization step at the end of the inner epoch
        
        # ğŸ”§ NEW: å¢å¼ºé•¿æœŸè®­ç»ƒç¨³å®šæ€§ - æ¯ä¸ªepochç»“æŸåè¿›è¡Œå†…å­˜æ¸…ç†
        if epoch > 0 and epoch % 5 == 0:  # æ¯5ä¸ªepochè¿›è¡Œä¸€æ¬¡æ·±åº¦æ¸…ç†
            if accelerator.is_local_main_process:
                print(f"ğŸ§¹ Epoch {epoch}: æ‰§è¡Œæ·±åº¦å†…å­˜æ¸…ç†ä»¥æå‡é•¿æœŸç¨³å®šæ€§...")
            
            # å¼ºåˆ¶CUDAåŒæ­¥
            torch.cuda.synchronize()
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            
            # Pythonåƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # æ£€æŸ¥GPUå†…å­˜çŠ¶æ€
            if torch.cuda.is_available() and accelerator.is_local_main_process:
                memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
                memory_reserved = torch.cuda.memory_reserved() / 1024**3    # GB
                print(f"ğŸ“Š GPUå†…å­˜çŠ¶æ€: å·²åˆ†é… {memory_allocated:.2f}GB, å·²ä¿ç•™ {memory_reserved:.2f}GB")
            
            if accelerator.is_local_main_process:
                print(f"âœ… æ·±åº¦æ¸…ç†å®Œæˆï¼Œç»§ç»­è®­ç»ƒ...")
        
        # Save checkpoint
        if accelerator.is_main_process and (epoch + 1) % config.save_freq == 0:
            save_ckpt_hunyuan3d(
                model, 
                ema,
                optimizer, 
                epoch, 
                global_step, 
                config.save_dir,
                accelerator
            )

if __name__ == "__main__":
    app.run(main) 