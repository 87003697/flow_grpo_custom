#!/usr/bin/env python3
"""
Hunyuan3D GRPOè®­ç»ƒè„šæœ¬ - å†…è”æ¶æ„ï¼ˆç±»ä¼¼SD3ï¼‰

æ¶æ„æ”¹è¿›ï¼š
1. ç§»é™¤ç‹¬ç«‹trainerç±»ï¼Œæ‰€æœ‰é€»è¾‘å†…è”åˆ°mainå‡½æ•°
2. ç›´æ¥ä½¿ç”¨core_pipelineï¼Œæ— åŒ…è£…å™¨
3. é‡‡æ ·ã€è®­ç»ƒã€è¯„ä¼°åœ¨ç»Ÿä¸€è„šæœ¬ä¸­å¤„ç†
4. ä¸SD3ä¿æŒä¸€è‡´çš„ä»£ç ç»„ç»‡ç»“æ„
"""

import sys
import os
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict
from concurrent import futures

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed
from accelerate.logging import get_logger

# ç»Ÿä¸€é…ç½®ç®¡ç†
import ml_collections
from absl import app, flags
from ml_collections import config_flags

_CONFIG = config_flags.DEFINE_config_file("config")

# æ•°æ®å’Œæ¨¡å‹ç›¸å…³å¯¼å…¥
from generators.hunyuan3d.pipeline import Hunyuan3DPipeline
from reward_models.rewards_mesh import multi_mesh_score
from flow_grpo.diffusers_patch.hunyuan3d_pipeline_with_logprob import hunyuan3d_pipeline_with_logprob
from flow_grpo.diffusers_patch.hunyuan3d_sde_with_logprob import hunyuan3d_sde_step_with_logprob
from flow_grpo.ema import EMAModuleWrapper
from flow_grpo.stat_tracking import PerImageStatTracker

logger = get_logger(__name__)

# ğŸš€ ç®€åŒ–ç‰ˆGPUç›‘æ§ï¼ˆå¯é€‰ï¼‰
def simple_gpu_log(name: str):
    """ç®€å•çš„GPUå†…å­˜æ—¥å¿—ï¼Œä¸é˜»å¡è®­ç»ƒ"""
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1024**3
        logger.info(f"{name}: GPUå†…å­˜ä½¿ç”¨ {memory_used:.2f}GB")

def get_timesteps(pipeline, batch_size: int, num_steps: int, device: str) -> torch.Tensor:
    """ç”Ÿæˆæ ‡å‡†åŒ–çš„æ—¶é—´æ­¥å¼ é‡"""
    scheduler_timesteps = pipeline.scheduler.timesteps
    if len(scheduler_timesteps) < num_steps:
        pipeline.scheduler.set_timesteps(num_steps + 1, device=device)
        scheduler_timesteps = pipeline.scheduler.timesteps
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¯¹äº20ä¸ªæ¨ç†æ­¥éª¤ï¼Œæˆ‘ä»¬æœ‰20å¯¹(current,next)latentsï¼Œéœ€è¦20ä¸ªæ—¶é—´æ­¥
    # ä¸åº”è¯¥å‡1ï¼Œå› ä¸ºæˆ‘ä»¬è¦å¯¹åº”20å¯¹latents
    used_timesteps = scheduler_timesteps[:num_steps]
    return used_timesteps.unsqueeze(0).repeat(batch_size, 1)

def compute_log_prob_3d(pipeline, sample: Dict[str, torch.Tensor], step_index: int, config: Any):
    """è®¡ç®—3Dæ‰©æ•£æ¨¡å‹çš„logæ¦‚ç‡ - ç±»ä¼¼SD3çš„compute_log_prob"""
    # è·å–æ•°æ®
    latents = sample["latents"][:, step_index]
    next_latents = sample["next_latents"][:, step_index]
    timestep = sample["timesteps"][:, step_index]
    
    # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨ç»Ÿä¸€æ ¼å¼çš„tensor
    cond = sample["positive_image_cond"]
    
    # ğŸ”§ ç®€å•å¤„ç†ï¼šç¡®ä¿batch_sizeåŒ¹é…
    if cond.shape[0] != latents.shape[0]:
        cond = cond.repeat_interleaved(latents.shape[0] // cond.shape[0], dim=0)
    
    # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šæ—¶é—´æ­¥æ ‡å‡†åŒ–ä¸è£å‰ª
    timestep_normalized = torch.clamp(
        timestep.float() / pipeline.scheduler.config.num_train_timesteps, 
        min=1e-6, max=1.0 - 1e-6
    )
    
    # ğŸ”§ ç®€å•å¤„ç†ï¼šæ„å»ºcontexts
    contexts = {'main': cond}
    
    # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šæ£€æŸ¥è¾“å…¥
    if torch.isnan(latents).any() or torch.isinf(latents).any():
        logger.warning(f"âš ï¸  è¾“å…¥latentsåŒ…å«NaNæˆ–Infå€¼")
        latents = torch.nan_to_num(latents, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # æ¨¡å‹é¢„æµ‹
    with torch.amp.autocast('cuda'):
        noise_pred = pipeline.model(latents, timestep_normalized, contexts)
    
    # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šæ£€æŸ¥æ¨¡å‹è¾“å‡º
    if torch.isnan(noise_pred).any() or torch.isinf(noise_pred).any():
        logger.warning(f"âš ï¸  æ¨¡å‹è¾“å‡ºåŒ…å«NaNæˆ–Infå€¼")
        noise_pred = torch.nan_to_num(noise_pred, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # è®¡ç®—logæ¦‚ç‡
    try:
        prev_sample, log_prob, prev_sample_mean, std_dev = hunyuan3d_sde_step_with_logprob(
            scheduler=pipeline.scheduler,
            model_output=noise_pred,
            timestep=timestep[0],
            sample=latents,
            prev_sample=next_latents,
            deterministic=getattr(config, 'deterministic', False),
        )
        
        # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šæ£€æŸ¥è¾“å‡ºå¹¶è¿›è¡Œè£å‰ª
        if torch.isnan(log_prob).any() or torch.isinf(log_prob).any():
            logger.warning(f"âš ï¸  log_probåŒ…å«NaNæˆ–Infå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            log_prob = torch.zeros_like(log_prob)
        
        if torch.isnan(prev_sample_mean).any() or torch.isinf(prev_sample_mean).any():
            logger.warning(f"âš ï¸  prev_sample_meanåŒ…å«NaNæˆ–Infå€¼ï¼Œä½¿ç”¨è£å‰ªå€¼")
            prev_sample_mean = torch.nan_to_num(prev_sample_mean, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # ğŸ”§ æ•°å€¼ç¨³å®šæ€§ï¼šstd_devè£å‰ªé˜²æ­¢è¿‡å¤§å€¼
        std_dev = torch.clamp(std_dev, min=1e-6, max=100.0)
        
    except Exception as e:
        logger.warning(f"âš ï¸  SDE stepå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
        prev_sample = next_latents
        log_prob = torch.zeros(latents.shape[0], device=latents.device)
        prev_sample_mean = next_latents
        std_dev = torch.ones(1, device=latents.device)
    
    return prev_sample, log_prob, prev_sample_mean, std_dev

class Image3DDataset(Dataset):
    def __init__(self, image_dir: str, prompts_file: Optional[str] = None, split: str = "train"):
        self.image_dir = Path(image_dir)
        self.prompts_file = prompts_file
        self.split = split
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦åœ¨imageså­ç›®å½•ä¸­
        if (self.image_dir / "images").exists():
            self.image_dir = self.image_dir / "images"
        
        # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
        self.image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            self.image_files.extend(self.image_dir.glob(ext))
        
        self.image_files = sorted(self.image_files)
        
        if not self.image_files:
            raise ValueError(f"No images found in {self.image_dir}")
        
        logger.info(f"Found {len(self.image_files)} images in {self.image_dir}")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        image_path = str(self.image_files[idx])
        prompt = self.get_prompt(self.image_files[idx])
        
        return {
            "image_path": image_path,
            "prompt": prompt,
            "metadata": {"image_name": self.image_files[idx].name}
        }
    
    @staticmethod
    def collate_fn(examples):
        image_paths = [example["image_path"] for example in examples]
        prompts = [example["prompt"] for example in examples]
        metadata = [example["metadata"] for example in examples]
        return image_paths, prompts, metadata
    
    def get_prompt(self, image_path: Path) -> str:
        """æ ¹æ®å›¾åƒè·¯å¾„ç”Ÿæˆæç¤ºè¯"""
        return f"Generate a 3D model from this image: {image_path.stem}"

def main(argv):
    """ä¸»è®­ç»ƒå‡½æ•° - å†…è”æ¶æ„ï¼ˆç±»ä¼¼SD3ï¼‰"""
    del argv
    config = _CONFIG.value
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨acceleratorï¼Œä¸éœ€è¦å¤æ‚çš„è®¾å¤‡æ£€æŸ¥
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.train.gradient_accumulation_steps,
        log_with="wandb" if "WANDB_PROJECT" in os.environ else None,
        project_dir=config.save_dir,
    )
    
    # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šå¯ç”¨PyTorchå†…å­˜ä¼˜åŒ–ç­–ç•¥
    torch.backends.cudnn.benchmark = False  # å‡å°‘å†…å­˜ç¢ç‰‡
    torch.backends.cuda.max_split_size_mb = 128  # é™åˆ¶å†…å­˜åˆ†å‰²å¤§å°
    
    # ğŸ”§ Flash Attentionä¼˜åŒ–ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®
    attention_config = getattr(config, 'attention_optimization', None)
    if attention_config:
        if hasattr(torch.backends.cuda, 'enable_flash_sdp') and attention_config.enable_flash_sdp:
            torch.backends.cuda.enable_flash_sdp(True)
            logger.info("âœ… Flash Attention å·²å¯ç”¨")
        
        if hasattr(torch.backends.cuda, 'enable_mem_efficient_sdp') and attention_config.enable_mem_efficient_sdp:
            torch.backends.cuda.enable_mem_efficient_sdp(True)
            logger.info("âœ… Memory Efficient Attention å·²å¯ç”¨")
        
        if hasattr(torch.backends.cuda, 'enable_math_sdp'):
            torch.backends.cuda.enable_math_sdp(attention_config.enable_math_sdp)
            if not attention_config.enable_math_sdp:
                logger.info("âœ… Math SDPA å·²ç¦ç”¨ï¼ˆä¼˜å…ˆä½¿ç”¨Flash/Memory Efficientï¼‰")
        
        # TF32ä¼˜åŒ–
        if hasattr(torch.backends.cuda, 'allow_tf32') and attention_config.allow_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            logger.info("âœ… TF32åŠ é€Ÿ å·²å¯ç”¨")
    else:
        # ğŸ”§ å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰attention_optimizationé…ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
        if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
            torch.backends.cuda.enable_flash_sdp(True)  # å¯ç”¨Flash Attention
        if hasattr(torch.backends.cuda, 'enable_mem_efficient_sdp'):
            torch.backends.cuda.enable_mem_efficient_sdp(True)  # å¯ç”¨Memory Efficient Attention
        if hasattr(torch.backends.cuda, 'enable_math_sdp'):
            torch.backends.cuda.enable_math_sdp(False)  # ç¦ç”¨æ•°å­¦SDPAï¼Œä¼˜å…ˆä½¿ç”¨Flash/Memory Efficient
        if hasattr(torch.backends.cuda, 'allow_tf32'):
            torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸TF32åŠ é€ŸçŸ©é˜µä¹˜æ³•
            torch.backends.cudnn.allow_tf32 = True
        logger.info("ğŸš€ é»˜è®¤Attentionä¼˜åŒ–å·²å¯ç”¨: Flash Attention + Memory Efficient Attention")
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    
    # è®¾ç½®ç§å­
    if hasattr(config, 'seed') and config.seed is not None:
        set_seed(config.seed)
    
    # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šæ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        logger.info(f"ğŸ§¹ GPUå†…å­˜æ¸…ç†å®Œæˆï¼Œå¯ç”¨å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    os.makedirs(config.save_dir, exist_ok=True)
    
    # ğŸš€ ç›´æ¥åŠ è½½pipelineï¼Œæ— åŒ…è£…å™¨ï¼ˆç±»ä¼¼SD3ï¼‰
    logger.info("Loading Hunyuan3D pipeline...")
    pipeline_wrapper = Hunyuan3DPipeline()
    
    # ğŸš€ è·å–æ ¸å¿ƒpipelineï¼Œç›´æ¥æ“ä½œï¼ˆç±»ä¼¼SD3ç›´æ¥ä½¿ç”¨StableDiffusion3Pipelineï¼‰
    pipeline = pipeline_wrapper.core_pipeline
    
    # ğŸš€ ç®€åŒ–ï¼šç»Ÿä¸€è®¾å¤‡å’Œæ•°æ®ç±»å‹è®¾ç½®ï¼ˆä»¿ç…§SD3ï¼‰
    inference_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        inference_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        inference_dtype = torch.bfloat16
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œä¸éœ€è¦å¤æ‚æ£€æŸ¥ï¼ˆç±»ä¼¼SD3ï¼‰
    pipeline.vae.to(accelerator.device, dtype=torch.float32)
    pipeline.conditioner.to(accelerator.device, dtype=inference_dtype)
    if config.use_lora:
        pipeline.model.to(accelerator.device)
    else:
        pipeline.model.to(accelerator.device, dtype=inference_dtype)
    
    # ğŸš€ å…³é”®ä¿®å¤ï¼šæ˜¾å¼ç¦ç”¨VAEå’Œconditionerçš„æ¢¯åº¦ï¼Œè®¾ç½®evalæ¨¡å¼ï¼ˆç±»ä¼¼SD3ï¼‰
    logger.info("ğŸ”§ è®¾ç½®VAEå’Œconditionerä¸ºæ¨ç†æ¨¡å¼...")
    pipeline.vae.eval()
    pipeline.conditioner.eval()
    pipeline.vae.requires_grad_(False)
    pipeline.conditioner.requires_grad_(False)
    logger.info("âœ… VAEå’Œconditioneræ¢¯åº¦å·²ç¦ç”¨ï¼Œå·²è®¾ç½®ä¸ºevalæ¨¡å¼")
    
    # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šè®­ç»ƒæ—¶å°†VAEç§»åŠ¨åˆ°CPUä»¥èŠ‚çœæ˜¾å­˜
    logger.info("ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šå°†VAEç§»åŠ¨åˆ°CPUä»¥èŠ‚çœè®­ç»ƒæ˜¾å­˜...")
    pipeline.vae.to('cpu')
    logger.info("âœ… VAEå·²ç§»åŠ¨åˆ°CPUï¼Œæ˜¾å­˜èŠ‚çœçº¦8-12GB")
    
    # ğŸš€ LoRAè®¾ç½®ï¼ˆç±»ä¼¼SD3ï¼‰
    if config.use_lora:
        from peft import LoraConfig, get_peft_model
        
        lora_config = LoraConfig(
            r=32,
            lora_alpha=64,
            target_modules=[
                # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®çœŸå®æ¨¡å‹ç»“æ„è®¾ç½®target_modules
                "to_q", "to_k", "to_v", "out_proj",      # æ³¨æ„åŠ›å±‚
                "fc1", "fc2",                             # MLPå±‚
                "final_layer.linear",                     # è¾“å‡ºå±‚
                # ğŸ”§ å¯é€‰ï¼šåŠ å…¥è¾“å…¥embeddingå±‚
                "x_embedder",                             # è¾“å…¥embedding
            ],
            lora_dropout=0.1,
            bias="none",
        )
        
        pipeline.model = get_peft_model(pipeline.model, lora_config)
    
    # ğŸ”§ å…³é”®ï¼šæŒ‰ç…§SD3æ¨¡å¼ï¼Œå…ˆè·å–æ¨¡å‹å¼•ç”¨
    model = pipeline.model
    
    # ğŸ”§ å…³é”®ï¼šè·å–trainableå‚æ•°ï¼ˆSD3æ–¹å¼ï¼‰
    trainable_params = list(filter(lambda p: p.requires_grad, model.parameters()))
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        trainable_params,
        lr=config.train.learning_rate,
        betas=(0.9, 0.999),
        weight_decay=0.01,
        eps=1e-8,
    )
    
    # ğŸ”§ å…³é”®ï¼šæœ€åprepareï¼ˆSD3æ–¹å¼ï¼‰
    model, optimizer = accelerator.prepare(model, optimizer)
    
    # ğŸ”§ å…³é”®ï¼šè®©pipelineä½¿ç”¨preparedçš„æ¨¡å‹
    pipeline.model = model
    
    # ğŸ”§ æŒ‰ç…§SD3æ¨¡å¼ï¼šLoRAè®­ç»ƒæ—¶ä¸ä½¿ç”¨autocast
    import contextlib
    autocast = contextlib.nullcontext if config.use_lora else accelerator.autocast
    
    # è®¾ç½®EMAï¼ˆä»¿ç…§SD3ï¼‰
    ema = None
    if config.train.ema:
        ema = EMAModuleWrapper(
            trainable_params,
            decay=config.train.ema_decay,
            device=accelerator.device
        )
    
    # ğŸš€ åˆå§‹åŒ–å¥–åŠ±å‡½æ•°ï¼ˆå†…è”ï¼Œæ— trainerï¼‰
    reward_config = {"geometric_quality": 1.0, "uni3d": 0.0}  # ğŸš€ æ˜¾å­˜ä¼˜åŒ–ï¼šç¦ç”¨Uni3DèŠ‚çœå¤§é‡æ˜¾å­˜
    reward_fn = multi_mesh_score(accelerator.device, reward_config)
    
    # ğŸš€ ç®€åŒ–ï¼šç›´æ¥åŠ è½½æ•°æ®é›†
    logger.info(f"Loading dataset from {config.data_dir}")
    train_dataset = Image3DDataset(config.data_dir, split="train")
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config.sample.input_batch_size,
        shuffle=True,
        collate_fn=Image3DDataset.collate_fn,
        num_workers=0,
    )
    
    # ç»Ÿè®¡è·Ÿè¸ª
    stat_tracker = None
    if config.per_image_stat_tracking:
        stat_tracker = PerImageStatTracker(
            buffer_size=len(train_dataset),
            min_count=config.stat_tracking.min_count,
        )
    
    # Prepare dataloader
    train_dataloader = accelerator.prepare(train_dataloader)
    
    # executor to perform callbacks asynchronously
    executor = futures.ThreadPoolExecutor(max_workers=8)
    
    # è®­ç»ƒå¾ªç¯ï¼ˆç±»ä¼¼SD3æ¶æ„ï¼‰
    global_step = 0
    first_epoch = 0
    
    # number of timesteps within each trajectory to train on
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæˆ‘ä»¬æœ‰20å¯¹latentsï¼Œæ‰€ä»¥å¯ä»¥è®­ç»ƒ20ä¸ªæ—¶é—´æ­¥
    num_latent_pairs = config.sample.num_steps  # 20å¯¹latents
    num_train_timesteps = min(
        int(num_latent_pairs * config.train.timestep_fraction),
        num_latent_pairs
    )
    
    for epoch in range(first_epoch, config.num_epochs):
        logger.info(f"Starting epoch {epoch}")
        
        #################### SAMPLING ####################
        model.eval()
        epoch_samples = []
        
        simple_gpu_log(f"Epoch {epoch} - å¼€å§‹é‡‡æ ·")
        
        for batch_idx, (image_paths, prompts, metadata) in enumerate(tqdm(
            train_dataloader, 
            desc=f"Epoch {epoch}: sampling",
            disable=not accelerator.is_local_main_process
        )):
            if batch_idx >= config.sample.num_batches_per_epoch:
                break
            
            # ğŸš€ å†…è”é‡‡æ ·é€»è¾‘ï¼ˆåŸtrainer.sample_meshes_with_rewardsï¼‰
            from PIL import Image
            
            # ğŸ”§ å¤šå€™é€‰ç”Ÿæˆï¼šä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆå¤šä¸ªå€™é€‰mesh
            all_pil_images = []
            for img_path in image_paths:
                # ä¸ºå½“å‰å›¾åƒç”Ÿæˆ num_meshes_per_image ä¸ªå€™é€‰
                candidate_images = [img_path] * config.sample.num_meshes_per_image
                pil_candidates = [Image.open(path).convert('RGBA') for path in candidate_images]
                all_pil_images.extend(pil_candidates)
            
            pil_images = all_pil_images
            
            # ç¼–ç å›¾åƒæ¡ä»¶
            cond_inputs = pipeline.prepare_image(pil_images)
            image_tensor = cond_inputs.pop('image')
            
            positive_image_cond = pipeline.encode_cond(
                image=image_tensor,
                additional_cond_inputs=cond_inputs,
                do_classifier_free_guidance=False,
                dual_guidance=False,
            )
            
            # ğŸ”§ å…³é”®ï¼šåœ¨è¿™é‡Œç»Ÿä¸€æ ¼å¼ï¼Œåç»­ä¸å†å¤„ç†
            if not isinstance(positive_image_cond, dict):
                positive_image_cond = {'main': positive_image_cond}
            
            # è°ƒç”¨pipeline
            meshes, all_latents, all_log_probs, all_kl, returned_pos_cond = hunyuan3d_pipeline_with_logprob(
                pipeline,
                image=pil_images,
                num_inference_steps=config.sample.num_steps,
                guidance_scale=config.sample.guidance_scale,
                deterministic=getattr(config, 'deterministic', False),
                kl_reward=config.sample.kl_reward,
                return_image_cond=True,
                positive_image_cond=positive_image_cond,
                octree_resolution=384,
                mc_level=0.0,
                mc_algo=None,
                box_v=1.01,
                num_chunks=50000,
            )
            
            # è®¡ç®—å¥–åŠ±ï¼ˆå¼‚æ­¥ï¼‰
            rewards = executor.submit(reward_fn, meshes, None, {}, image_paths)
            time.sleep(0)  # yield to make sure reward computation starts
            
            # å¤„ç†latentsæ•°æ®
            latents_tensor = torch.stack(all_latents, dim=1)
            current_latents = latents_tensor[:, :-1]  # å‰n-1ä¸ªæ—¶é—´æ­¥
            next_latents = latents_tensor[:, 1:]      # ån-1ä¸ªæ—¶é—´æ­¥
            
            # å¤„ç†log_probså’ŒKL
            log_probs_tensor = torch.stack(all_log_probs, dim=1)
            kl_tensor = torch.stack(all_kl, dim=1)
            
            # å¤„ç†timesteps
            # ğŸ”§ ä¿®å¤ï¼šä¼ å…¥å®Œæ•´çš„num_stepsï¼Œå‡½æ•°å†…éƒ¨ä¼šå¤„ç†-1
            timesteps_tensor = get_timesteps(pipeline, len(all_pil_images), config.sample.num_steps, accelerator.device)
            
            # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨
            returned_pos_cond = returned_pos_cond['main']
            
            epoch_samples.append({
                "latents": current_latents,
                "next_latents": next_latents,
                "log_probs": log_probs_tensor,
                "kl": kl_tensor,
                "rewards": rewards,  # å¼‚æ­¥ç»“æœ
                "timesteps": timesteps_tensor,
                "positive_image_cond": returned_pos_cond,
                "images": image_paths,
                "meshes": meshes,
            })
        
        # ğŸ”§ é‡‡æ ·å®Œæˆï¼Œè®°å½•å†…å­˜çŠ¶æ€
        simple_gpu_log(f"Epoch {epoch} - é‡‡æ ·å®Œæˆ")
        
        # wait for all rewards to be computed
        for sample in tqdm(
            epoch_samples,
            desc="Waiting for rewards",
            disable=not accelerator.is_local_main_process,
        ):
            reward_details, reward_metadata = sample["rewards"].result()
            sample["rewards"] = {
                "avg": torch.tensor(reward_details['avg'], device=accelerator.device, dtype=torch.float32)
            }
        
        # ğŸš€ æ•°æ®å¤„ç†ï¼ˆç±»ä¼¼SD3ï¼‰
        # collate samples into dict where each entry has shape (num_batches_per_epoch * sample.batch_size, ...)
        samples = {}
        for k in epoch_samples[0].keys():
            if k in ["meshes", "images"]:
                continue
            elif k == "rewards":
                # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥å–avgï¼Œç»Ÿä¸€ä¸ºtensoræ ¼å¼
                samples[k] = {
                    "avg": torch.cat([s[k]["avg"] for s in epoch_samples], dim=0)
                }
            elif isinstance(epoch_samples[0][k], torch.Tensor):
                samples[k] = torch.cat([s[k] for s in epoch_samples], dim=0)
        
        # ğŸš€ å¤„ç†å¥–åŠ±å’Œadvantagesï¼ˆç±»ä¼¼SD3ï¼‰
        rewards_avg = samples["rewards"]["avg"]  # ç°åœ¨ç›´æ¥æ˜¯tensor
        kl_tensor = samples["kl"]
        
        # KLè°ƒæ•´åçš„å¥–åŠ±
        samples["rewards"]["ori_avg"] = rewards_avg
        samples["rewards"]["avg"] = rewards_avg.unsqueeze(-1) - config.sample.kl_reward * kl_tensor
        
        # gather rewards across processes
        gathered_rewards = {key: accelerator.gather(value) for key, value in samples["rewards"].items()}
        gathered_rewards_np = {key: value.cpu().numpy() for key, value in gathered_rewards.items()}
        
        # è®¡ç®—advantagesï¼ˆç±»ä¼¼SD3ï¼‰
        if config.per_image_stat_tracking and stat_tracker:
            all_images = [item for s in epoch_samples for item in s["images"]]
            advantages_np = stat_tracker.update(all_images, gathered_rewards_np["avg"].mean(axis=1))
            advantages = torch.tensor(advantages_np, device=accelerator.device)
        else:
            advantages = gathered_rewards["avg"].mean(axis=1)  # å¹³å‡æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰æ—¶é—´æ­¥
            
            # ğŸ”§ æ ‡å‡†åŒ–advantages
            advantages_std = advantages.std()
            if advantages_std > 1e-8:
                advantages = (advantages - advantages.mean()) / (advantages_std + 1e-4)
            else:
                advantages = advantages - advantages.mean()  # åªåšä¸­å¿ƒåŒ–
        
        # æ‰©å±•advantagesåˆ°æ—¶é—´ç»´åº¦
        num_steps = samples["timesteps"].shape[1]
        advantages = advantages.unsqueeze(1).expand(-1, num_steps)
        samples["advantages"] = advantages
        
        # è¿‡æ»¤æ ·æœ¬ï¼ˆç±»ä¼¼SD3ï¼‰
        valid_mask = (advantages.abs().sum(dim=1) > 1e-6)
        if valid_mask.sum().item() == 0:
            logger.warning("âš ï¸  æ‰€æœ‰æ ·æœ¬éƒ½è¢«è¿‡æ»¤æ‰äº†ï¼ä½¿ç”¨æ‰€æœ‰æ ·æœ¬...")
            valid_mask = torch.ones(len(advantages), dtype=torch.bool, device=advantages.device)
        
        # å®‰å…¨çš„æ ·æœ¬è¿‡æ»¤
        for key in samples.keys():
            if isinstance(samples[key], torch.Tensor):
                if samples[key].shape[0] == valid_mask.shape[0]:
                    samples[key] = samples[key][valid_mask]
            elif isinstance(samples[key], dict):
                for sub_key in samples[key]:
                    if isinstance(samples[key][sub_key], torch.Tensor):
                        if samples[key][sub_key].shape[0] == valid_mask.shape[0]:
                            samples[key][sub_key] = samples[key][sub_key][valid_mask]
        
        logger.info(f"Training on {valid_mask.sum().item()} samples")
        
        # ğŸ”§ æ•°æ®åˆ‡åˆ†ä¸ºè®­ç»ƒbatch size
        total_samples = samples["latents"].shape[0]
        train_batch_size = config.train.batch_size
        
        if total_samples > train_batch_size:
            for key in samples.keys():
                if isinstance(samples[key], torch.Tensor):
                    if samples[key].shape[0] == total_samples:
                        samples[key] = samples[key][:train_batch_size]
                elif isinstance(samples[key], dict):
                    for sub_key in samples[key]:
                        if isinstance(samples[key][sub_key], torch.Tensor):
                            if samples[key][sub_key].shape[0] == total_samples:
                                samples[key][sub_key] = samples[key][sub_key][:train_batch_size]
        
        # log rewards
        accelerator.log(
            {
                "epoch": epoch,
                **{f"reward_{key}": value.mean() for key, value in gathered_rewards_np.items()},
                "kl": samples["kl"].mean().cpu().numpy(),
            },
            step=global_step,
        )
        
        # ğŸ”§ æ•°æ®å¤„ç†å®Œæˆï¼Œè®°å½•å†…å­˜çŠ¶æ€
        simple_gpu_log(f"Epoch {epoch} - æ•°æ®å¤„ç†å®Œæˆ")
        
        #################### TRAINING ####################
        # å†…è”è®­ç»ƒé€»è¾‘ï¼ˆåŸtrainer.train_stepï¼‰
        for inner_epoch in range(config.train.num_inner_epochs):
            model.train()
            info = defaultdict(list)
            num_timesteps = samples["timesteps"].shape[1]
            
            # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šè®­ç»ƒå‰æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            simple_gpu_log(f"è®­ç»ƒå‰å†…å­˜æ¸…ç†")
            
            # è®­ç»ƒæ¯ä¸ªæ—¶é—´æ­¥ï¼ˆç±»ä¼¼SD3çš„è®­ç»ƒå¾ªç¯ï¼‰
            train_timesteps = [step_index for step_index in range(num_train_timesteps)]
            for j in tqdm(
                train_timesteps,
                desc="Timestep",
                leave=False,
                disable=not accelerator.is_local_main_process,
            ):
                with accelerator.accumulate(model):
                    with autocast():
                        # è®¡ç®—logæ¦‚ç‡
                        prev_sample, log_prob, prev_sample_mean, std_dev = compute_log_prob_3d(
                            pipeline, samples, j, config
                        )
                        
                        # å‚è€ƒlogæ¦‚ç‡
                        if getattr(config.train, 'beta', 0) > 0:
                            with torch.no_grad():
                                # ğŸ”§ æŒ‰ç…§SD3æ¨¡å¼ï¼šå®‰å…¨è®¿é—®DDPåŒ…è£…åçš„æ¨¡å‹
                                model_for_adapter = model.module if hasattr(model, 'module') else model
                                with model_for_adapter.disable_adapter():
                                    _, log_prob_ref, prev_sample_mean_ref, std_dev_ref = compute_log_prob_3d(
                                        pipeline, samples, j, config
                                    )
                        
                        # è®¡ç®—GRPOæŸå¤±ï¼ˆç±»ä¼¼SD3ï¼‰
                        advantages = torch.clamp(
                            samples["advantages"][:, j],
                            -config.train.adv_clip_max,
                            config.train.adv_clip_max,
                        )
                        
                        # è®¡ç®—æ¯”ç‡
                        ratio = torch.exp(log_prob - samples["log_probs"][:, j])
                        
                        # PPOæŸå¤±
                        unclipped_loss = -advantages * ratio
                        clipped_loss = -advantages * torch.clamp(
                            ratio,
                            1.0 - config.train.clip_range,
                            1.0 + config.train.clip_range,
                        )
                        policy_loss = torch.mean(torch.maximum(unclipped_loss, clipped_loss))
                        
                        # KLæŸå¤±
                        if getattr(config.train, 'beta', 0) > 0:
                            kl_loss = ((prev_sample_mean - prev_sample_mean_ref) ** 2).mean(dim=tuple(range(1, prev_sample_mean.ndim))) / (2 * std_dev ** 2)
                            kl_loss = torch.mean(kl_loss)
                            loss = policy_loss + config.train.beta * kl_loss
                        else:
                            kl_loss = torch.tensor(0.0, device=policy_loss.device)
                            loss = policy_loss
                        
                        info["loss"].append(loss.item())
                        info["policy_loss"].append(policy_loss.item())
                        if getattr(config.train, 'beta', 0) > 0:
                            info["kl_loss"].append(kl_loss.item())
                        info["advantages"].append(advantages.mean().item())
                        info["ratio"].append(ratio.mean().item())
                        
                        # è®¡ç®—clipfracå’Œapprox_klï¼ˆç±»ä¼¼SD3ï¼‰
                        info["approx_kl"].append(
                            0.5 * torch.mean((log_prob - samples["log_probs"][:, j]) ** 2).item()
                        )
                        info["clipfrac"].append(
                            torch.mean(
                                (torch.abs(ratio - 1.0) > config.train.clip_range).float()
                            ).item()
                        )
                    
                    # åå‘ä¼ æ’­
                    accelerator.backward(loss)
                    
                    # æ¢¯åº¦è£å‰ªï¼ˆç±»ä¼¼SD3ï¼‰
                    if accelerator.sync_gradients:
                        accelerator.clip_grad_norm_(
                            model.parameters(), config.train.max_grad_norm
                        )
                    
                    optimizer.step()
                    optimizer.zero_grad()
                
                # è®°å½•è®­ç»ƒä¿¡æ¯ï¼ˆç±»ä¼¼SD3ï¼‰
                if accelerator.sync_gradients:
                    info = {k: np.mean(v) for k, v in info.items()}
                    info.update({"epoch": epoch, "inner_epoch": inner_epoch})
                    accelerator.log(info, step=global_step)
                    global_step += 1
                    info = defaultdict(list)
            
            # æ›´æ–°EMA
            if ema is not None:
                ema.update()
        
        # ğŸ”§ è®­ç»ƒå®Œæˆï¼Œè®°å½•å†…å­˜çŠ¶æ€
        simple_gpu_log(f"Epoch {epoch} - è®­ç»ƒå®Œæˆ")
        
        # ğŸš€ ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆç±»ä¼¼SD3ï¼‰
        if epoch % config.save_freq == 0 and epoch > 0:
            save_dir = os.path.join(config.save_dir, f"checkpoint_{epoch}")
            os.makedirs(save_dir, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            model_to_save = accelerator.unwrap_model(model)
            torch.save(model_to_save.state_dict(), os.path.join(save_dir, "model.pt"))
            
            logger.info(f"Saved checkpoint to {save_dir}")
        
        simple_gpu_log(f"Epoch {epoch} - å®Œæˆ")

if __name__ == "__main__":
    app.run(main) 